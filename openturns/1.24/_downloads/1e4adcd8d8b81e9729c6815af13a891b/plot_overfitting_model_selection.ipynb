{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Over-fitting and model selection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n\nIn this notebook, we present the problem of over-fitting a model to data.\nWe consider noisy observations of the `sine` function.\nWe estimate the coefficients of the univariate polynomial based on linear\nleast squares and show that, when the degree of the polynomial becomes too\nlarge, the overall prediction quality decreases.\n\nThis shows why and how model selection can come into play in order to select\nthe degree of the polynomial: there is a trade-off between fitting the\ndata and preserving the quality of future predictions.\nIn this example, we use cross validation as a model selection method.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\n* Bishop Christopher M., 1995, Neural networks for pattern recognition. Figure 1.4, page 7\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute the data\n\nIn this section, we generate noisy observations from the `sine` function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import openturns as ot\nimport pylab as pl\nimport openturns.viewer as otv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ot.RandomGenerator.SetSeed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define the function that we are going to approximate.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "g = ot.SymbolicFunction([\"x\"], [\"sin(2*pi_*x)\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = ot.Graph(\"Polynomial curve fitting\", \"x\", \"y\", True, \"upper right\")\n# The \"unknown\" function\ncurve = g.draw(0, 1)\ncurve.setLegends(['\"Unknown\" function'])\ngraph.add(curve)\nview = otv.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This seems a smooth function to approximate with polynomials.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def linearSample(xmin, xmax, npoints):\n    \"\"\"Returns a sample created from a regular grid\n    from xmin to xmax with npoints points.\"\"\"\n    step = (xmax - xmin) / (npoints - 1)\n    rg = ot.RegularGrid(xmin, step, npoints)\n    vertices = rg.getVertices()\n    return vertices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We consider 10 observation points in the interval [0,1].\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_train = 10\nx_train = linearSample(0, 1, n_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Assume that the observations are noisy and that the noise follows a Normal distribution with zero mean and small standard deviation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "noise = ot.Normal(0, 0.1)\nnoiseSample = noise.getSample(n_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following computes the observation as the sum of the function value and of the noise.\nThe couple (`x_train` , `y_train`) is the training set: it is used to compute the coefficients of the polynomial model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y_train = g(x_train) + noiseSample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = ot.Graph(\"Polynomial curve fitting\", \"x\", \"y\", True, \"upper right\")\n# The \"unknown\" function\ncurve = g.draw(0, 1)\ncurve.setLegends(['\"Unknown\" function'])\ngraph.add(curve)\n# Training set\ncloud = ot.Cloud(x_train, y_train)\ncloud.setPointStyle(\"circle\")\ncloud.setLegend(\"Observations\")\ngraph.add(cloud)\nview = otv.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute the coefficients of the polynomial decomposition\n\nLet $\\vect{y} \\in \\Rset^n$ be a vector of observations.\nThe polynomial model is\n\n\\begin{align}P(x) = \\beta_0 + \\beta_1 x + ... + \\beta_p x^p,\\end{align}\n\nfor any $x \\in \\Rset$, where $p$ is the polynomial degree and $\\vect{\\beta} \\in \\Rset^{p+1}$ is the vector of the coefficients of the model.\nLet $\\sampleSize$ be the training sample size and let $x_1,...,x_\\sampleSize \\in \\Rset$ be the abscissas of the training set.\nThe design matrix $\\mat{X} \\in \\Rset^{n \\times (p+1)}$ is\n\n\\begin{align}x_{i,j} = x^j_i,\\end{align}\n\nfor $i=1,...,n$ and $j=0,...,p$.\nThe least squares solution is:\n\n\\begin{align}\\beta^\\star = \\argmin_{\\vect{\\beta} \\in \\Rset^{p+1}} \\| \\mat{X} \\vect{\\beta} - \\vect{y}\\|_2^2.\\end{align}\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to approximate the function with polynomials up to degree 4,\nwe create a list of strings containing the associated monomials.\nWe do not include a constant in the polynomial basis, as this constant term\nis automatically included in the model by the :class:`~openturns.LinearLeastSquares` class.\nWe perform the loop from 1 up to `total_degree` (but the `range` function takes `total_degree + 1` as its second input argument).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "total_degree = 4\npolynomialCollection = [\"x^%d\" % (degree) for degree in range(1, total_degree + 1)]\npolynomialCollection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given the list of strings, we create a symbolic function which computes the values of the monomials.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "basis = ot.SymbolicFunction([\"x\"], polynomialCollection)\nbasis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "designMatrix = basis(x_train)\ndesignMatrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "myLeastSquares = ot.LinearLeastSquares(designMatrix, y_train)\nmyLeastSquares.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "responseSurface = myLeastSquares.getMetaModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The test set\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The couple (`x_test` , `y_test`) is the test set: it is used to assess the quality of the polynomial model with points that were not used for training.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_test = 50\nx_test = linearSample(0, 1, n_test)\ny_test = responseSurface(basis(x_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = ot.Graph(\"Polynomial curve fitting\", \"x\", \"y\", True, \"upper right\")\n# The \"unknown\" function\ncurve = g.draw(0, 1)\ncurve.setLegends(['\"Unknown\" function'])\ngraph.add(curve)\n# Training set\ncloud = ot.Cloud(x_train, y_train)\ncloud.setPointStyle(\"circle\")\ncloud.setLegend(\"Observations\")\ngraph.add(cloud)\n# Predictions\ncurve = ot.Curve(x_test, y_test)\ncurve.setLegend(\"Polynomial Degree = %d\" % (total_degree))\ngraph.add(curve)\nview = otv.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute the residuals\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each observation in the training set, the residual is the vertical distance between the model and the observation.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "sphinx_gallery_thumbnail_number = 4\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = ot.Graph(\n    \"Least squares minimizes the sum of the squares of the vertical bars\",\n    \"x\",\n    \"y\",\n    True,\n    \"upper right\",\n)\nresidualsColor = ot.Drawable.BuildDefaultPalette(3)[2]\n# Predictions\ncurve = ot.Curve(x_test, y_test)\ncurve.setLegend(\"Polynomial Degree = %d\" % (total_degree))\ngraph.add(curve)\n# Training set observations\ncloud = ot.Cloud(x_train, y_train)\ncloud.setPointStyle(\"circle\")\ncloud.setLegend(\"Observations\")\ngraph.add(cloud)\n# Errors\nypredicted_train = responseSurface(basis(x_train))\nfor i in range(n_train):\n    curve = ot.Curve([x_train[i], x_train[i]], [y_train[i], ypredicted_train[i]])\n    curve.setColor(residualsColor)\n    curve.setLineWidth(2)\n    if i == 0:\n        curve.setLegend(\"Residual\")\n    graph.add(curve)\nview = otv.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The least squares method minimizes the sum of the squared errors i.e. the sum of the squares of the lengths of the vertical segments.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We gather the previous computation in two different functions. The `myPolynomialDataFitting` function computes\nthe least squares solution and `myPolynomialCurveFittingGraph` plots the results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def myPolynomialDataFitting(total_degree, x_train, y_train):\n    \"\"\"Computes the polynomial curve fitting\n    with given total degree.\n    This is for learning purposes only: please consider a serious metamodel\n    for real applications, e.g. polynomial chaos or kriging.\"\"\"\n    polynomialCollection = [\"x^%d\" % (degree) for degree in range(1, total_degree + 1)]\n    basis = ot.SymbolicFunction([\"x\"], polynomialCollection)\n    designMatrix = basis(x_train)\n    myLeastSquares = ot.LinearLeastSquares(designMatrix, y_train)\n    myLeastSquares.run()\n    responseSurface = myLeastSquares.getMetaModel()\n    return responseSurface, basis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def myPolynomialCurveFittingGraph(total_degree, x_train, y_train):\n    \"\"\"Returns the graphics for a polynomial curve fitting\n    with given total degree\"\"\"\n    responseSurface, basis = myPolynomialDataFitting(total_degree, x_train, y_train)\n    # Graphics\n    n_test = 100\n    x_test = linearSample(0, 1, n_test)\n    ypredicted_test = responseSurface(basis(x_test))\n    # Graphics\n    graph = ot.Graph(\"Polynomial curve fitting\", \"x\", \"y\", True, \"upper right\")\n    # The \"unknown\" function\n    curve = g.draw(0, 1)\n    graph.add(curve)\n    # Training set\n    cloud = ot.Cloud(x_train, y_train)\n    cloud.setPointStyle(\"circle\")\n    cloud.setLegend(\"N=%d\" % (x_train.getSize()))\n    graph.add(cloud)\n    # Predictions\n    curve = ot.Curve(x_test, ypredicted_test)\n    curve.setLegend(\"Degree = %d\" % (total_degree))\n    graph.add(curve)\n    return graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to see the effect of the polynomial degree, we compare the polynomial fit with degrees equal to 0 (constant), 1 (linear), 3 (cubic) and 9.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grid = ot.GridLayout(2, 2)\ngrid.setGraph(0, 0, myPolynomialCurveFittingGraph(0, x_train, y_train))\ngrid.setGraph(0, 1, myPolynomialCurveFittingGraph(1, x_train, y_train))\ngrid.setGraph(1, 0, myPolynomialCurveFittingGraph(3, x_train, y_train))\ngrid.setGraph(1, 1, myPolynomialCurveFittingGraph(9, x_train, y_train))\nview = otv.View(grid, figure_kw={\"figsize\": (8.0, 5.0)})\npl.subplots_adjust(hspace=0.5, wspace=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When the polynomial degree is low, the fit is satisfying.\nThe polynomial is close to the observations, although there is still some residual error.\n\nHowever, when the polynomial degree is high, it produces large oscillations\nwhich significantly deviate from the true function.\nThis is *overfitting*. This is a pity, given the fact that the polynomial\n*exactly* interpolates the observations: the residuals are zeroed.\n\nIf the locations of the x abscissas could be changed, then the oscillations could be made smaller.\nThis is the method used in Gaussian quadrature, where the nodes of interpolation are made closer on the left and right bounds.\nIn our situation, we make the asssumption that these abscissas cannot be changed: the most obvious choice is to limit the degree of the polynomial.\nAnother possibility is to include a regularization into the least squares solution.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Root mean squared error\n\nIn order to assess the quality of the polynomial fit, we create a second dataset, the *test set* and compare the value of the polynomial with the test observations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sqrt = ot.SymbolicFunction([\"x\"], [\"sqrt(x)\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to see how close the model is to the observations, we compute the root mean square error.\n\nFirst, we create a degree 4 polynomial which fits the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "total_degree = 4\nresponseSurface, basis = myPolynomialDataFitting(total_degree, x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we create a test set, with the same method as before.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def createDataset(n):\n    x = linearSample(0, 1, n)\n    noiseSample = noise.getSample(n)\n    y = g(x) + noiseSample\n    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_test = 100\nx_test, y_test = createDataset(n_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On this test set, we evaluate the polynomial.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ypredicted_test = responseSurface(basis(x_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The vector of residuals is the vector of the differences between the observations and the predictions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "residuals = y_test.asPoint() - ypredicted_test.asPoint()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `normSquare` method computes the square of the Euclidian norm (i.e., the 2-norm).\nWe divide this by the test sample size (so as to compare the error for different sample sizes)\nand compute the square root of the result (so that the result has the same unit as `y` ).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "RMSE = sqrt([residuals.normSquare() / n_test])[0]\nRMSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function gathers the RMSE computation to make the experiment easier.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def computeRMSE(responseSurface, basis, x, y):\n    ypredicted = responseSurface(basis(x))\n    residuals = y.asPoint() - ypredicted.asPoint()\n    RMSE = sqrt([residuals.normSquare() / n_test])[0]\n    return RMSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "maximum_degree = 10\nRMSE_train = ot.Sample(maximum_degree, 1)\nRMSE_test = ot.Sample(maximum_degree, 1)\nfor total_degree in range(maximum_degree):\n    responseSurface, basis = myPolynomialDataFitting(total_degree, x_train, y_train)\n    RMSE_train[total_degree, 0] = computeRMSE(responseSurface, basis, x_train, y_train)\n    RMSE_test[total_degree, 0] = computeRMSE(responseSurface, basis, x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "degreeSample = ot.Sample([[i] for i in range(maximum_degree)])\ngraph = ot.Graph(\"Root mean square error\", \"Degree\", \"RMSE\", True, \"upper right\")\n# Train\ncloud = ot.Curve(degreeSample, RMSE_train)\ncloud.setLegend(\"Train\")\ncloud.setPointStyle(\"circle\")\ngraph.add(cloud)\n# Test\ncloud = ot.Curve(degreeSample, RMSE_test)\ncloud.setLegend(\"Test\")\ncloud.setPointStyle(\"circle\")\ngraph.add(cloud)\nview = otv.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the RMSE on the train set continuously decreases, reaching zero\nwhen the polynomial degree is so that the number of coefficients is equal to\nthe train dataset sample size. In this extreme situation, the least squares\nsolution is equivalent to solving a linear system of equations: this leads to a zero residual.\n\nOn the test set however, the RMSE decreases, reaches a flat region,\nthen increases dramatically when the degree is equal to 9.\nHence, limiting the polynomial degree limits overfitting.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Increasing the training set\n\nWe wonder what happens when the training dataset size is increased.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "total_degree = 9\ngrid = ot.GridLayout(1, 2)\nn_train = 11\nx_train, y_train = createDataset(n_train)\ngrid.setGraph(0, 0, myPolynomialCurveFittingGraph(total_degree, x_train, y_train))\nn_train = 100\nx_train, y_train = createDataset(n_train)\ngrid.setGraph(0, 1, myPolynomialCurveFittingGraph(total_degree, x_train, y_train))\nview = otv.View(grid, figure_kw={\"figsize\": (8.0, 4.0)})\npl.subplots_adjust(wspace=0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the polynomial oscillates with a dataset with size 11, but does not with the larger dataset: increasing the training dataset mitigates the oscillations.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}