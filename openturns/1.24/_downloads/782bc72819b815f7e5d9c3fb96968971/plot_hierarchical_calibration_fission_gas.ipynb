{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Bayesian calibration of hierarchical fission gas release models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n\nModels such as the ones described in `use-case-fission-gas` include empirical constants that need to be calibrated so that predictions agree well with measurements.\nDuring this process, the distributions of model parameters are derived,\nwhich can be used in subsequent analyses to generate accurate forecasts with corresponding uncertainties.\nHowever, model inadequacy can disrupt calibration,\nleading to derived uncertainties that fail to cover prediction-measurement discrepancies.\nA Bayesian way to account for this is to assume that the model parameters vary between different sets of experimental conditions, i.e.\nto adopt a hierarchical model and calibrate distribution parameters (means and standard deviations).\n\nUsing the notations defined in `use-case-fission-gas`,\nthe unobserved model inputs $x_{\\mathrm{diff}, i}, i=1...\\sampleSize_{\\mathrm{exp}}$\n(resp. $x_{\\mathrm{crack}, i}, i=1...\\sampleSize_{\\mathrm{exp}}$)\nare i.i.d. random variables which follow a normal distribution with\nmean parameter $\\mu_{\\mathrm{diff}}$ (resp. $\\mu_{\\mathrm{crack}}$)\nand standard deviation parameter $\\sigma_{\\mathrm{diff}}$ (resp. $\\sigma_{\\mathrm{crack}}$).\n\nThe network plot from the page `use-case-fission-gas` can thus be updated:\n\n.. figure:: ../../_static/fission_gas_network_calibration.png\n    :align: center\n    :alt: Fission gas release calibration\n    :width: 50%\n\nIn the network above, full arrows represent deterministic relationships and dashed arrows probabilistic relationships.\nMore precisely, the conditional distribution of the node at the end of two dashed arrows when (only) the starting nodes are known\nis a normal distribution with parameters equal to these starting nodes.\nNote that due to constraints on the input domains of the $\\model_i$ models, for every $1 \\leq i \\leq \\sampleSize_{\\mathrm{exp}}$,\nthe distributions of $x_{\\mathrm{diff}, i}$ and $x_{\\mathrm{crack}, i}$ are truncated to the input domain boundaries.\n\nSuch a hierarchical approach was used in [robertson2024]_, showing how a hierarchical probabilistic model can be sampled using a Metropolis-Hastings-within-Gibbs sampler.\nThe authors calibrated the models against measurements from the International Fuel Performance Experiments (IFPE) database.\nThis example follows the procedure described in their paper.\nPlease note however that we are using simplified models, so the results of this page\nshould not be expected to reproduce those of the paper.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import openturns as ot\nfrom openturns.viewer import View\nimport numpy as np\nimport matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the models\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from openturns.usecases import fission_gas\n\nfgr = fission_gas.FissionGasRelease()\ndesc = fgr.getInputDescription()  # description of the model inputs (diff, crack)\nndim = len(desc)  # dimension of the model inputs: 2\nnexp = fgr.measurement_values.getSize()  # number of sets of experimental conditions\nmodels = fgr.models  # the nexp models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fix the random seed\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ot.RandomGenerator.SetSeed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each experiment $i$ produced one measurement value,\nwhich is used to define the likelihood of the associated model $\\model_i$\nand latent variables $x_{i, \\mathrm{diff}}$ and $x_{i, \\mathrm{crack}}$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "likelihoods = [\n    ot.Normal(v, fgr.measurement_uncertainty(v)) for v in fgr.measurement_values\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Partially conjugate posterior\n\nThe goal of this study is to calibrate the parameters $\\mu_{\\mathrm{diff}}$, $\\sigma_{\\mathrm{diff}}$,\n$\\mu_{\\mathrm{crack}}$ and $\\sigma_{\\mathrm{crack}}$.\nTo perform Bayesian calibration, we set for $\\mu_{\\mathrm{diff}}$ and $\\mu_{\\mathrm{crack}}$ a uniform prior distribution\nand for $\\sigma_{\\mathrm{diff}}$ and $\\sigma_{\\mathrm{crack}}$\nthe limit of a truncated inverse gamma distribution with parameters $(\\lambda, k)$ when $\\lambda \\to \\infty$ and $k \\to 0$.\nThe parameters of the prior distributions are defined later.\n\nThis choice of prior distributions means that the posterior is partially conjugate.\nFor instance, the conditional posterior distribution of $\\mu_{\\mathrm{diff}}$\n(resp. $\\mu_{\\mathrm{crack}}$)\nis a truncated normal distribution with the following parameters\n(for $\\mu_{\\mathrm{crack}}$ simply replace $\\mathrm{diff}$ with $\\mathrm{crack}$ in what follows) :\n\n- The truncation parameters are the bounds of the prior uniform distribution.\n- The mean parameter is $\\frac{1}{\\sampleSize_{\\mathrm{exp}}} \\sum_{i=1}^{\\sampleSize_{\\mathrm{exp}}} x_{\\mathrm{diff}, i}$.\n- The standard deviation parameter is $\\sqrt{\\frac{\\sigma_{\\mathrm{diff}}}{\\sampleSize_{\\mathrm{exp}}}}$.\n\nLet us prepare a random vector to sample the conditional posterior\ndistributions of $\\mu_{\\mathrm{diff}}$ and $\\mu_{\\mathrm{crack}}$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mu_rv = ot.RandomVector(ot.TruncatedNormal())\nmu_desc = [f\"$\\\\mu$_{{{label}}}\" for label in desc]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The conditional posterior distribution of $\\sigma_{\\mathrm{diff}}$\n(resp. $\\sigma_{\\mathrm{crack}}$)\nis a truncated inverse gamma distribution with the following parameters\n(for $\\sigma_{\\mathrm{crack}}$ simply replace $\\mathrm{diff}$ with $\\mathrm{crack}$ in what follows) :\n\n- The truncation parameters are the truncation parameters of the prior distribution.\n- The $\\lambda$ parameter is $\\frac{2}{\\sum_{i=1}^{\\sampleSize_{\\mathrm{exp}}} \\left(x_{\\mathrm{diff}, i} - \\mu_{\\mathrm{diff}} \\right)^2}$.\n- The $k$ parameter is $\\sqrt{\\frac{\\sampleSize_{\\mathrm{exp}}}{2}}$.\n\nLet us prepare a random vector to sample the conditional posterior\ndistribution of $\\sigma_{\\mathrm{diff}}^2$ and $\\sigma_{\\mathrm{crack}}^2$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sigma_square_rv = ot.RandomVector(ot.TruncatedDistribution(ot.InverseGamma(), 0.0, 1.0))\nsigma_square_desc = [f\"$\\\\sigma$_{{{label}}}^2\" for label in desc]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define 3 function templates which produce:\n\n- the parameters of the (truncated normal) conditional posterior distributions of the $\\mu$ parameters\n- the parameters of the (truncated inverse gamma) conditional posterior distributions of the $\\sigma$ parameters\n- the conditional posterior log-PDF of the latent variables.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class PosteriorParametersMu(ot.OpenTURNSPythonFunction):\n    \"\"\"Outputs the parameters of the conditional posterior distribution of one\n    of the mu parameters.\n    This conditional posterior distribution is a TruncatedNormal distribution.\n\n    Parameters\n    ----------\n    state : list of float\n        Current state of the Markov chain.\n        The posterior distribution is conditional to those values.\n\n    Returns\n    -------\n    parameters : list of float\n        Parameters of the conditional posterior distribution.\n        In order: mean, standard deviation, lower bound, upper bound.\n    \"\"\"\n\n    def __init__(self, dim=0, lb=-100, ub=100):\n        self._dim = dim\n\n        # State description: mu values, then sigma values, then for each experiment x values\n        state_length = (1 + 1 + nexp) * ndim\n        super().__init__(state_length, 4)\n        self._xindices = range(state_length)[2 * ndim :][dim::ndim]\n\n        # Get lower and upper bound\n        self._lb = lb\n        self._ub = ub\n\n    def _exec(self, state):\n        # posterior mean of mu = empirical mean of the x values\n        post_mean = np.mean([state[i] for i in self._xindices])\n\n        # posterior std of mu = prior sigma / sqrt(nexp)\n        post_std = np.sqrt(state[ndim + self._dim] / nexp)\n\n        # Hyperparameters of a truncated normal\n        return [post_mean, post_std, self._lb, self._ub]\n\n\nclass PosteriorParametersSigmaSquare(ot.OpenTURNSPythonFunction):\n    \"\"\"Outputs the parameters of the conditional posterior distribution of one\n    of the sigma parameters.\n    This conditional posterior distribution is a Truncated InverseGamma distribution.\n\n    Parameters\n    ----------\n    state : list of float\n        Current state of the Markov chain.\n        The posterior distribution is conditional to those values.\n\n    Returns\n    -------\n    parameters : list of float\n        Parameters of the conditional posterior distribution.\n        In order: k (shape), lambda, lower bound, upper bound.\n    \"\"\"\n\n    def __init__(self, dim=0, lb=1e-4, ub=100):\n        self._dim = dim\n\n        # State description: mu values, then sigma values, then for each experiment x values\n        state_length = (1 + 1 + nexp) * ndim\n        super().__init__(state_length, 4)\n        self._xindices = range(state_length)[2 * ndim :][dim::ndim]\n\n        # Get lower and upper bound\n        self._lb = lb\n        self._ub = ub\n\n    def _exec(self, state):\n        mu = state[self._dim]\n\n        # Get squares of centered xvalues from the state\n        squares = [(state[i] - mu) ** 2 for i in self._xindices]\n\n        post_lambda = 2.0 / np.sum(squares)  # rate lambda =  1 / beta\n        post_k = nexp / 2.0  # shape\n\n        return [post_k, post_lambda, self._lb, self._ub]\n\n\nclass PosteriorLogDensityX(ot.OpenTURNSPythonFunction):\n    \"\"\"Outputs the conditional posterior density (up to an additive constant)\n    of the 2D latent variable x_i = (x_{i, diff}, x_{i, x_{i, crack})\n    corresponding to one experiment i.\n\n    Parameters\n    ----------\n    state : list of float\n        Current state of the Markov chain.\n        The posterior distribution is conditional to those values.\n\n    Returns\n    -------\n    log_density : list of one float\n        PLog-density (up to an additive constant) of the conditional posterior.\n    \"\"\"\n\n    def __init__(self, exp):\n        # State description: mu values, then sigma values, then for each experiment x values\n        state_length = (1 + 1 + nexp) * ndim\n        super().__init__(state_length, 1)\n        self._xindices = range(state_length)[2 * ndim :][exp * ndim : (exp + 1) * ndim]\n\n        # Setup experiment number and associated model and likelihood\n        self._exp = exp\n        self._like = likelihoods[exp]\n        self._model = models[exp]\n\n    def _exec(self, state):\n        # Get the x indices of the experiment\n        x = np.array([state[i] for i in self._xindices])\n\n        # Get mu and sigma in order to normalize x\n        mu = np.array([state[i] for i in range(ndim)])\n        sig = np.sqrt([state[i] for i in range(ndim, 2 * ndim)])\n        normalized = (x - mu) / sig\n\n        # Compute the log-prior density\n        logprior = np.sum([ot.DistFunc.logdNormal(normalized[i]) for i in range(ndim)])\n\n        # Use the model to predict the experiment and compute the log-likelihood\n        pred = self._model(x)\n        loglikelihood = self._like.computeLogPDF(pred)\n\n        # Return the log-posterior, i.e. the sum of the log-prior and the log-likelihood\n        return [logprior + loglikelihood]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metropolis-within-Gibbs algorithm\n\nLower and upper bounds for $\\mu_{\\mathrm{diff}}, \\mu_{\\mathrm{crack}}$\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lbs = [0.1, 1e-4]\nubs = [40.0, 1.0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lower and upper bounds for $\\sigma_{\\mathrm{diff}}^2, \\sigma_{\\mathrm{crack}}^2$\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lbs_sigma_square = np.array([0.1, 0.1]) ** 2\nubs_sigma_square = np.array([40, 10]) ** 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initial state\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "initial_mus = [10.0, 0.3]\ninitial_sigma_squares = [20.0**2, 0.5**2]\ninitial_x = np.repeat([[19.0, 0.4]], repeats=nexp, axis=0).flatten().tolist()\ninitial_state = initial_mus + initial_sigma_squares + initial_x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Support of the prior (and thus posterior) distribution\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "support = ot.Interval(\n    lbs + lbs_sigma_square.tolist() + nexp * lbs,\n    ubs + ubs_sigma_square.tolist() + nexp * ubs,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the list of all samplers in the Gibbs algorithm as outlined in the chart below,\nwhere direct samplers are represented in green and random walk Metropolis-Hastings samplers in blue.\n\n.. figure:: ../../_static/fission_gas_mh.png\n    :align: center\n    :alt: Metropolis-Hastings-within-Gibbs description\n    :width: 100%\n\nWe start with the samplers of $\\mu_{\\mathrm{diff}}, \\mu_{\\mathrm{crack}}$.\nWe are able to directly sample these conditional distributions,\nhence we use the :class:`~openturns.RandomVectorMetropolisHastings` class.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "samplers = [\n    ot.RandomVectorMetropolisHastings(\n        mu_rv,\n        initial_state,\n        [i],\n        ot.Function(PosteriorParametersMu(dim=i, lb=lbs[i], ub=ubs[i])),\n    )\n    for i in range(ndim)\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We continue with the samplers of $\\sigma_{\\mathrm{diff}}^2, \\sigma_{\\mathrm{crack}}^2$.\nWe are also able to directly sample these conditional distributions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "samplers += [\n    ot.RandomVectorMetropolisHastings(\n        sigma_square_rv,\n        initial_state,\n        [ndim + i],\n        ot.Function(\n            PosteriorParametersSigmaSquare(\n                dim=i, lb=lbs_sigma_square[i], ub=ubs_sigma_square[i]\n            )\n        ),\n    )\n    for i in range(ndim)\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We finish with the samplers of the $\\vect{x}_i$, with $1 \\leq i \\leq n_{exp}$.\nEach of these samplers outputs points in a $\\sampleSize_{\\mathrm{exp}}$-dimensional space.\nWe are not able to directly sample these conditional posterior distributions,\nso we resort to random walk Metropolis-Hastings.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for exp in range(nexp):\n    base_index = 2 * ndim + ndim * exp\n\n    samplers += [\n        ot.RandomWalkMetropolisHastings(\n            ot.Function(PosteriorLogDensityX(exp=exp)),\n            support,\n            initial_state,\n            ot.Normal([0.0] * 2, [6.0, 0.15]),\n            [base_index + i for i in range(ndim)],\n        )\n    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Gibbs algorithm combines all these samplers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sampler = ot.Gibbs(samplers)\nx_desc = []\nfor i in range(1, nexp + 1):\n    x_desc += [f\"x_{{{label}, {i}}}\" for label in desc]\nsampler.setDescription(mu_desc + sigma_square_desc + x_desc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run this Metropolis-within-Gibbs algorithm and check the acceptance rates\nfor the Random walk Metropolis-Hastings samplers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "samples = sampler.getSample(2000)\nacceptance = [\n    sampler.getMetropolisHastingsCollection()[i].getAcceptanceRate()\n    for i in range(6, len(samplers))\n]\n\nadaptation_factor = [\n    sampler.getMetropolisHastingsCollection()[i]\n    .getImplementation()\n    .getAdaptationFactor()\n    for i in range(6, len(samplers))\n]\n\nprint(\"Minimum acceptance rate = \", np.min(acceptance))\nprint(\"Maximum acceptance rate for random walk MH = \", np.max(acceptance[2 * ndim :]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the posterior distribution\n\nPlease note that the following plots rely on the MCMC sample.\nAlthough this is not done in the present example,\ndiagnostics should be run on the MCMC sample to assess the convergence of the Markov chain.\n\nWe only represent the $\\mu$ and $\\sigma$ parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reduced_samples = samples[:, 0:4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is possible to quickly draw pair plots.\nHere we tweak the rendering a little.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pair_plots = ot.VisualTest.DrawPairs(reduced_samples)\n\nfor i in range(pair_plots.getNbRows()):\n    for j in range(pair_plots.getNbColumns()):\n        graph = pair_plots.getGraph(i, j)\n        graph.setXTitle(pair_plots.getGraph(pair_plots.getNbRows() - 1, j).getXTitle())\n        graph.setYTitle(pair_plots.getGraph(i, 0).getYTitle())\n        pair_plots.setGraph(i, j, graph)\n\n_ = View(pair_plots)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create an enhanced pair plots grid with histograms of the marginals on the diagonal.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "full_grid = ot.GridLayout(pair_plots.getNbRows() + 1, pair_plots.getNbColumns() + 1)\n\nfor i in range(full_grid.getNbRows()):\n    hist = ot.HistogramFactory().build(reduced_samples.getMarginal(i))\n    pdf = hist.drawPDF()\n    pdf.setLegends([\"\"])\n    pdf.setTitle(\"\")\n    full_grid.setGraph(i, i, pdf)\n\nfor i in range(pair_plots.getNbRows()):\n    for j in range(pair_plots.getNbColumns()):\n        if len(pair_plots.getGraph(i, j).getDrawables()) > 0:\n            full_grid.setGraph(i + 1, j, pair_plots.getGraph(i, j))\n\n_ = View(full_grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally superimpose contour plots of the KDE-estimated 2D marginal PDFs on the pairplots.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ot.ResourceMap.SetAsBool(\"Contour-DefaultIsFilled\", True)\n\n# sphinx_gallery_thumbnail_number = 3\nfor i in range(1, full_grid.getNbRows()):\n    for j in range(i):\n        graph = full_grid.getGraph(i, j)\n        cloud = graph.getDrawable(0).getImplementation()\n        cloud.setPointStyle(\".\")\n        data = cloud.getData()\n        dist = ot.KernelSmoothing().build(data)\n        contour = dist.drawPDF().getDrawable(0).getImplementation()\n        contour.setLevels(np.linspace(0.0, contour.getLevels()[-1], 10))\n        graph.setDrawables([contour, cloud])\n        graph.setBoundingBox(contour.getBoundingBox())\n        full_grid.setGraph(i, j, graph)\n\n_ = View(full_grid, scatter_kw={\"alpha\": 0.1})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post-calibration prediction\n\nRetrieve the $\\mu$ and $\\sigma^2$ columns in the sample.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mu = samples.getMarginal([f\"$\\\\mu$_{{{label}}}\" for label in desc])\nsigma_square = samples.getMarginal([f\"$\\\\sigma$_{{{label}}}^2\" for label in desc])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build the joint distribution of the latent variables $x_{\\mathrm{diff}}, x_{\\mathrm{crack}}$\nobtained when $\\mu_{\\mathrm{diff}}$, $\\sigma_{\\mathrm{diff}}$,\n$\\mu_{\\mathrm{crack}}$ and $\\sigma_{\\mathrm{crack}}$\nfollow their joint posterior distribution.\nIt is estimated as a mixture of truncated $\\sampleSize_{\\mathrm{exp}}$-dimensional normal distributions\ncorresponding to the posterior samples of the $\\mu_{\\mathrm{diff}}$, $\\mu_{\\mathrm{crack}}$,\n$\\sigma_{\\mathrm{diff}}$ and $\\sigma_{\\mathrm{crack}}$ parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "truncation_interval = ot.Interval(lbs, ubs)\nnormal_collection = [\n    ot.TruncatedDistribution(ot.Normal(mean, np.sqrt(var)), truncation_interval)\n    for (mean, var) in zip(mu, sigma_square)\n]\nnormal_mixture = ot.Mixture(normal_collection)\nnormal_mixture.setDescription(desc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build a collection of random vectors such that the distribution\nof each is the push-forward of the marginal distribution of $(x_{\\mathrm{diff}}, x_{\\mathrm{crack}})$\ndefined above through one of the $\\sampleSize_{\\mathrm{exp}}$ models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rv_normal_mixture = ot.RandomVector(normal_mixture)\nrv_models = [ot.CompositeRandomVector(model, rv_normal_mixture) for model in models]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get a Monte-Carlo estimate of the median, 0.05 quantile and 0.95 quantile\nof these push-forward distributions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predictions = [rv.getSample(200) for rv in rv_models]\nprediction_medians = [sam.computeMedian()[0] for sam in predictions]\nprediction_lb = [sam.computeQuantile(0.05)[0] for sam in predictions]\nprediction_ub = [sam.computeQuantile(0.95)[0] for sam in predictions]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These push-forward distributions are the distributions\nof the model predictions when $\\mu_{\\mathrm{diff}}$, $\\mu_{\\mathrm{crack}}$,\n$\\sigma_{\\mathrm{diff}}$ and $\\sigma_{\\mathrm{crack}}$ follow\ntheir joint posterior distribution.\nThey can be compared to the actual measurements to represent predictive accuracy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "yerr = np.abs(np.column_stack([prediction_lb, prediction_ub]).T - prediction_medians)\nplt.errorbar(fgr.measurement_values, prediction_medians, yerr, fmt=\"o\")\nplt.xscale(\"log\")\n\nbisector = np.linspace(0, 0.5)\nplt.plot(bisector, bisector, \"--\", color=\"black\")\n\nplt.xlabel(\"Measurements\")\nplt.ylabel(\"Prediction ranges induced by the posterior\")\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the sake of comparison, we now consider the distributions\nof the model predictions when $\\mu_{\\mathrm{diff}}$, $\\mu_{\\mathrm{crack}}$,\n$\\sigma_{\\mathrm{diff}}$ and $\\sigma_{\\mathrm{crack}}$ follow\ntheir joint prior distribution.\nBecause the actual prior distribution of $\\sigma_{\\mathrm{diff}}$ and $\\sigma_{\\mathrm{crack}}$\ncannot be represented, we approximate them by choosing a very large $\\lambda$ parameter\nand a very small $k$ parameter.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "prior = ot.JointDistribution(\n    [\n        ot.Uniform(lbs[0], ubs[0]),\n        ot.Uniform(lbs[1], ubs[1]),\n        ot.TruncatedDistribution(\n            ot.InverseGamma(0.01, 1e7),\n            lbs_sigma_square[0],\n            float(ubs_sigma_square[0]),\n        ),\n        ot.TruncatedDistribution(\n            ot.InverseGamma(0.01, 1e7),\n            lbs_sigma_square[1],\n            float(ubs_sigma_square[1]),\n        ),\n    ]\n)\nprior_sample = prior.getSample(2000)\n\n# As before, build a mixture of truncated normal distributions from the sample.\nnormal_collection_prior = [\n    ot.TruncatedDistribution(ot.Normal(par[0:2], np.sqrt(par[2:])), truncation_interval)\n    for par in prior_sample\n]\nnormal_mixture_prior = ot.Mixture(normal_collection_prior)\nnormal_mixture_prior.setDescription(desc)\nrv_normal_mixture_prior = ot.RandomVector(normal_mixture_prior)\n\n# Build random vectors sampling from the predictive distributions.\nrv_models_prior = [\n    ot.CompositeRandomVector(model, rv_normal_mixture_prior) for model in models\n]\n\npredictions_prior = [rv.getSample(100) for rv in rv_models_prior]\nprediction_medians_prior = [sam.computeMedian()[0] for sam in predictions_prior]\nprediction_lb_prior = [sam.computeQuantile(0.05)[0] for sam in predictions_prior]\nprediction_ub_prior = [sam.computeQuantile(0.95)[0] for sam in predictions_prior]\n\n# Produce the graph comparing predictive distributions with measurements\nyerr_prior = np.abs(\n    np.column_stack([prediction_lb_prior, prediction_ub_prior]).T\n    - prediction_medians_prior\n)\nplt.errorbar(\n    np.array(fgr.measurement_values), prediction_medians_prior, yerr_prior, fmt=\"o\"\n)\nplt.xscale(\"log\")\nplt.plot(bisector, bisector, \"--\", color=\"black\")\nplt.xlabel(\"Measurements\")\nplt.ylabel(\"Prediction ranges induced by the prior\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To facilitate the comparison, we plot the median value of the predictive distributions only.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plt.scatter(fgr.measurement_values, prediction_medians)\nplt.scatter(fgr.measurement_values, prediction_medians_prior)\nplt.xscale(\"log\")\nplt.plot(bisector, bisector, \"--\", c=\"black\")\nplt.xlabel(\"Measurements\")\nplt.ylabel(\"Prediction medians\")\nplt.legend([\"Posterior\", \"Prior\"])\nplt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ot.ResourceMap.Reload()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}