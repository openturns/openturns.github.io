{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Calibration of the flooding model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example we are interested in the calibration of the `flooding model <use-case-flood-model>`.\nIn this example we calibrate the parameters of a flooding model where only the difference between the downstream and upstream riverbed levels can be calibrated. This example shows how to manage the lack of identifiability in a calibration problem.\n\n## Parameters to calibrate\n\nThe vector of parameters to calibrate is:\n\n\\begin{align}\\theta = (K_s,Z_v,Z_m).\\end{align}\n\n\nThe variables to calibrate are $(K_s,Z_v,Z_m)$ and are set to the following values:\n\n\\begin{align}K_s = 30, \\qquad Z_v = 50, \\qquad Z_m = 55.\\end{align}\n\n\n## Observations\n\nIn this section, we describe the statistical model associated with the $n$ observations.\nThe errors of the water heights are associated with a normal distribution with a zero mean and a standard variation equal to:\n\n\\begin{align}\\sigma=0.1.\\end{align}\n\n\nTherefore, the observed water heights are:\n\n\\begin{align}H_i = G(Q_i,K_s,Z_v,Z_m) + \\epsilon_i\\end{align}\n\n\nfor $i=1,...,n$ where\n\n\\begin{align}\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)\\end{align}\n\n\nand we make the hypothesis that the observation errors are independent.\nWe consider a sample size equal to:\n\n\\begin{align}n=100.\\end{align}\n\n\nThe observations are the couples $\\{(Q_i,H_i)\\}_{i=1,...,n}$, i.e. each observation is a couple made of the flowrate and the corresponding river height.\n\n## Variables\n\n- Q : Input. Observed.\n- Ks, Zv, Zm : Input. Calibrated.\n- H: Output. Observed.\n\n## Analysis\n\nIn the description of the `flooding model<use-case-flood-model>`, we see that only one parameter\ncan be identified.\nHence, calibrating this model requires some regularization.\nWe return to this topic when analyzing the singular values of\nthe Jacobian matrix.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate the observations\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from openturns.usecases import flood_model\nfrom matplotlib import pylab as plt\nimport openturns.viewer as viewer\nimport numpy as np\nimport openturns as ot\not.ResourceMap.SetAsUnsignedInteger('Normal-SmallDimension', 1)\not.Log.Show(ot.Log.NONE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We load the flooding use case :\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fm = flood_model.FloodModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define the model $g$ which has 4 inputs and one output H.\n\nThe nonlinear least squares does not take into account for bounds in the parameters. Therefore, we ensure that the output is computed whatever the inputs. The model fails into two situations:\n\n* if $K_s<0$,\n* if $Z_v-Z_m<0$.\n\nIn these cases, we return an infinite number, so that the optimization algorithm does not get trapped.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def functionFlooding(X):\n    L = 5.0e3\n    B = 300.0\n    Q, K_s, Z_v, Z_m = X\n    alpha = (Z_m - Z_v)/L\n    if alpha < 0.0 or K_s <= 0.0:\n        H = np.inf\n    else:\n        H = (Q/(K_s*B*np.sqrt(alpha)))**(3.0/5.0)\n    return [H]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "g = ot.PythonFunction(4, 1, functionFlooding)\ng = ot.MemoizeFunction(g)\ng.setOutputDescription([\"H (m)\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We load the input distribution for $Q$ :\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Q = fm.Q\nprint(Q)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set the parameters to be calibrated.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "K_s = ot.Dirac(30.0)\nZ_v = ot.Dirac(50.0)\nZ_m = ot.Dirac(55.0)\nK_s.setDescription([\"Ks (m^(1/3)/s)\"])\nZ_v.setDescription([\"Zv (m)\"])\nZ_m.setDescription([\"Zm (m)\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the joint input distribution.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inputRandomVector = ot.ComposedDistribution([Q, K_s, Z_v, Z_m])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a Monte-Carlo sample of the output H.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nbobs = 100\ninputSample = inputRandomVector.getSample(nbobs)\noutputH = g(inputSample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observe the distribution of the output H.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = ot.HistogramFactory().build(outputH).drawPDF()\nview = viewer.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate the observation noise and add it to the output of the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sigmaObservationNoiseH = 0.1  # (m)\nnoiseH = ot.Normal(0., sigmaObservationNoiseH)\nsampleNoiseH = noiseH.getSample(nbobs)\nHobs = outputH + sampleNoiseH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the Y observations versus the X observations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Qobs = inputSample[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = ot.Graph(\"Observations\", \"Q (m3/s)\", \"H (m)\", True)\ncloud = ot.Cloud(Qobs, Hobs)\ngraph.add(cloud)\nview = viewer.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting the calibration parameters\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the value of the reference values of the $\\theta$ parameter. In the bayesian framework, this is called the mean of the *prior* normal distribution. In the data assimilation framework, this is called the *background*.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "KsInitial = 20.\nZvInitial = 49.\nZmInitial = 51.\nthetaPrior = [KsInitial, ZvInitial, ZmInitial]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following statement create the calibrated function from the model. The calibrated parameters $K_s$, $Z_v$, $Z_m$ are at indices 1, 2, 3 in the inputs arguments of the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "calibratedIndices = [1, 2, 3]\nmycf = ot.ParametricFunction(g, calibratedIndices, thetaPrior)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calibration with linear least squares\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `LinearLeastSquaresCalibration` class performs the linear least squares calibration by linearizing the model in the neighbourhood of the reference point.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "algo = ot.LinearLeastSquaresCalibration(mycf, Qobs, Hobs, thetaPrior, \"SVD\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `run` method computes the solution of the problem.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "algo.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "calibrationResult = algo.getResult()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `getParameterMAP` method returns the maximum of the posterior distribution of $\\theta$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "thetaStar = calibrationResult.getParameterMAP()\nprint(thetaStar)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, we see that there seems to be a great distance from the reference value of $\\theta$ to the optimum: the values seem too large in magnitude. The value of the optimum $K_s$ is nonpositive. In fact, there is an identification problem because the Jacobian matrix is rank-degenerate.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diagnostic of the identification issue\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section, we show how to diagnose the identification problem.\n\nThe `getParameterPosterior` method returns the posterior normal distribution of $\\theta$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "distributionPosterior = calibrationResult.getParameterPosterior()\nprint(distributionPosterior)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that there is a large covariance matrix diagonal.\n\nLet us compute a 95% confidence interval for the solution $\\theta^\\star$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(distributionPosterior.computeBilateralConfidenceIntervalWithMarginalProbability(\n    0.95)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The confidence interval is *very* large. In order to clarify the situation, we compute the Jacobian matrix of the model at the candidate point.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mycf.setParameter(thetaPrior)\nthetaDim = len(thetaPrior)\njacobianMatrix = ot.Matrix(nbobs, thetaDim)\nfor i in range(nbobs):\n    jacobianMatrix[i, :] = mycf.parameterGradient(Qobs[i]).transpose()\nprint(jacobianMatrix[0:5, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The rank of the problem can be seen from the singular values of the Jacobian matrix.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(jacobianMatrix.computeSingularValues())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that there are two singular values which are relatively close to zero.\n\nThis explains why the Jacobian matrix is close to being rank-degenerate.\n\nMoreover, this allows one to compute the actual dimensionality of the problem.\nThe algorithm we use computes the singular values in descending order.\nMoreover, by definition, the singular values are nonnegative.\nWe see that the first singular value is close to $10$\nand the others are very close to $0$ in comparison.\nThis implies that the (numerical) rank of the Jacobian matrix is 1,\neven if there are 3 parameters.\n\nHence, only one parameter can be identified, be it $K_s$, $Z_v$ or $Z_m$.\nThe choice of the particular parameter to identify is free.\nHowever, in hydraulic studies, the parameter $K_s$ is classically\ncalibrated while $Z_v$ and $Z_m$ are left constant.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion of the linear least squares calibration\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are several methods to solve the problem.\n\n* Given that the problem is not identifiable, we can use some regularization method. Two methods are provided in the library: the Gaussian linear least squares `GaussianLinearCalibration` and the Gaussian non linear least squares `GaussianNonlinearCalibration`.\n* We can change the problem, replacing it with a problem which is identifiable. In the flooding model, we can view $Z_v$ and $Z_m$ as constants and calibrate $K_s$ only.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calibration with non linear least squares\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `NonLinearLeastSquaresCalibration` class performs the non linear least squares calibration by minimizing the squared euclidian norm between the predictions and the observations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "algo = ot.NonLinearLeastSquaresCalibration(mycf, Qobs, Hobs, thetaPrior)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `run` method computes the solution of the problem.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "algo.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "calibrationResult = algo.getResult()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis of the results\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `getParameterMAP` method returns the maximum of the posterior distribution of $\\theta$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "thetaMAP = calibrationResult.getParameterMAP()\nprint(thetaMAP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can compute a 95% confidence interval of the parameter $\\theta^\\star$.\n\nThis confidence interval is based on bootstrap, based on a sample size equal to 100 (as long as the value of the `ResourceMap` key \"NonLinearLeastSquaresCalibration-BootstrapSize\" is unchanged). This confidence interval reflects the sensitivity of the optimum to the variability in the observations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "thetaPosterior = calibrationResult.getParameterPosterior()\nprint(thetaPosterior.computeBilateralConfidenceIntervalWithMarginalProbability(\n    0.95)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, the value of the parameter $K_s$ is quite accurately computed, but there is a relatively large uncertainty on the values of $Z_v$ and $Z_m$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = calibrationResult.drawObservationsVsInputs()\ngraph.setLegendPosition(\"topleft\")\nview = viewer.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that there is a good fit after calibration, since the predictions after calibration (i.e. the green crosses) are close to the observations (i.e. the blue crosses).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = calibrationResult.drawObservationsVsPredictions()\nview = viewer.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that there is a much better fit after calibration, since the predictions are close to the diagonal of the graphics.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "observationError = calibrationResult.getObservationsError()\nprint(observationError)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the observation error is close to have a zero mean and a standard deviation equal to 0.1.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = calibrationResult.drawResiduals()\ngraph.setLegendPosition(\"topleft\")\nview = viewer.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The analysis of the residuals shows that the distribution is centered on zero and symmetric. This indicates that the calibration performed well.\n\nMoreover, the distribution of the residuals is close to being Gaussian.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = calibrationResult.drawParameterDistributions()\nview = viewer.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gaussian linear calibration\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The standard deviation of the observations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sigmaH = 0.5  # (m^2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the covariance matrix of the output Y of the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "errorCovariance = ot.CovarianceMatrix(1)\nerrorCovariance[0, 0] = sigmaH**2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the covariance matrix of the parameters $\\theta$ to calibrate.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sigmaKs = 5.\nsigmaZv = 1.\nsigmaZm = 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sigma = ot.CovarianceMatrix(3)\nsigma[0, 0] = sigmaKs**2\nsigma[1, 1] = sigmaZv**2\nsigma[2, 2] = sigmaZm**2\nprint(sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `GaussianLinearCalibration` class performs Gaussian linear calibration by linearizing the model in the neighbourhood of the prior.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "algo = ot.GaussianLinearCalibration(\n    mycf, Qobs, Hobs, thetaPrior, sigma, errorCovariance, \"SVD\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `run` method computes the solution of the problem.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "algo.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "calibrationResult = algo.getResult()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis of the results\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `getParameterMAP` method returns the maximum of the posterior distribution of $\\theta$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "thetaStar = calibrationResult.getParameterMAP()\nprint(thetaStar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = calibrationResult.drawObservationsVsInputs()\ngraph.setLegendPosition(\"topleft\")\nview = viewer.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the output of the model after calibration is closer to the observations. However, there is still a distance from the outputs of the model to the observations. This indicates that the calibration cannot be performed with this method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = calibrationResult.drawObservationsVsPredictions()\nview = viewer.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case, the fit is better after calibration, but not perfect. Indeed, the cloud of points after calibration is not centered on the diagonal.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = calibrationResult.drawResiduals()\ngraph.setLegendPosition(\"topleft\")\nview = viewer.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the distribution of the residual is not centered on zero: the mean residual is approximately $-0.5$, which implies that the predictions are, on average, smaller than the observations. This is a proof that the calibration cannot be performed with this method in this particular case.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `getParameterPosterior` method returns the posterior normal distribution of $\\theta$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "distributionPosterior = calibrationResult.getParameterPosterior()\nprint(distributionPosterior)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can compute a 95% confidence interval of the parameter $\\theta^\\star$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(distributionPosterior.computeBilateralConfidenceIntervalWithMarginalProbability(\n    0.95)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that there is a large uncertainty on the value of the parameter $K_s$ which can be as small as $14$ and as large as $34$.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can compare the prior and posterior distributions of the marginals of $\\theta$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = calibrationResult.drawParameterDistributions()\nview = viewer.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The two distributions are different, which shows that the calibration is sensible to the observations (if the observations were not sensible, the two distributions were superimposed). Moreover, the two distributions are quite close, which implies that the prior distribution has played a roled in the calibration (otherwise the two distributions would be completely different, indicating that only the observations were taken into account).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gaussian nonlinear calibration\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `GaussianNonLinearCalibration` class performs Gaussian nonlinear calibration.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "algo = ot.GaussianNonLinearCalibration(\n    mycf, Qobs, Hobs, thetaPrior, sigma, errorCovariance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `run` method computes the solution of the problem.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "algo.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "calibrationResult = algo.getResult()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis of the results\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `getParameterMAP` method returns the maximum of the posterior distribution of $\\theta$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "thetaStar = calibrationResult.getParameterMAP()\nprint(thetaStar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = calibrationResult.drawObservationsVsInputs()\ngraph.setLegendPosition(\"topleft\")\nview = viewer.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the output of the model after calibration is in the middle of the observations: the calibration seems correct.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = calibrationResult.drawObservationsVsPredictions()\nview = viewer.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The fit is excellent after calibration. Indeed, the cloud of points after calibration is on the diagonal.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = calibrationResult.drawResiduals()\nview = viewer.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the histogram of the residual is centered on zero. This is a proof that the calibration did perform correctly.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `getParameterPosterior` method returns the posterior normal distribution of $\\theta$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "distributionPosterior = calibrationResult.getParameterPosterior()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can compute a 95% confidence interval of the parameter $\\theta^\\star$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(distributionPosterior.computeBilateralConfidenceIntervalWithMarginalProbability(\n    0.95)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that there is a small uncertainty on the value of all parameters.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can compare the prior and posterior distributions of the marginals of $\\theta$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = calibrationResult.drawParameterDistributions()\nview = viewer.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The two distributions are very different, with a spiky posterior distribution. This shows that the calibration is very sensible to the observations.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tuning the posterior distribution estimation\n\nThe \"GaussianNonLinearCalibration-BootstrapSize\" key controls the posterior distribution estimation.\n\n* If \"GaussianNonLinearCalibration-BootstrapSize\" > 0 (by default it is equal to 100), then a bootstrap resample algorithm is used to see the dispersion of the MAP estimator. This allows one to see the variability of the estimator with respect to the finite observation sample.\n* If \"GaussianNonLinearCalibration-BootstrapSize\" is zero, then the Gaussian linear calibration estimator is used (i.e. the `GaussianLinearCalibration` class) at the optimum. This is called the Laplace approximation.\n\nWe must configure the key before creating the object (otherwise changing the parameter does not change the result).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ot.ResourceMap.SetAsUnsignedInteger(\n    \"GaussianNonLinearCalibration-BootstrapSize\", 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "algo = ot.GaussianNonLinearCalibration(\n    mycf, Qobs, Hobs, thetaPrior, sigma, errorCovariance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "algo.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "calibrationResult = algo.getResult()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = calibrationResult.drawParameterDistributions()\nviewer = viewer.View(graph)\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, this does not change much the posterior distribution, which remains spiky.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}