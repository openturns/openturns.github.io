{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Bayesian calibration of a computer code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example we are going to compute the parameters of a computer model thanks to Bayesian estimation.\n\nLet us denote $\\underline y = (y_1, \\dots, y_n)$ the observation sample, $\\underline z = (f(x_1|\\underline{\\theta}), \\ldots, f(x_n|\\underline{\\theta}))$ the model prediction, $p(y |z)$ the density function of observation $y$ conditional on model prediction $z$, and $\\underline{\\theta} \\in \\mathbb{R}^p$ the calibration parameters we wish to estimate.\n\n\nThe posterior distribution is given by Bayes theorem:\n\n\\begin{align}\\pi(\\underline{\\theta} | \\underline y) \\quad \\propto \\quad L\\left(\\underline y | \\underline{\\theta}\\right) \\times \\pi(\\underline{\\theta})$`\\end{align}\n\nwhere :math:$\\propto` means \"proportional to\", regarded as a function of $\\underline{\\theta}$. \n\nThe posterior distribution is approximated here by the empirical distribution of the sample $\\underline{\\theta}^1, \\ldots, \\underline{\\theta}^N$ generated by the Metropolis-Hastings algorithm. This means that any quantity characteristic of the posterior distribution (mean, variance, quantile, ...) is approximated by its empirical counterpart.\n\nOur model (i.e. the compute code to calibrate) is a standard normal linear regression, where\n\n\\begin{align}y_i = \\theta_1 + x_i \\theta_2 + x_i^2 \\theta_3 + \\varepsilon_i\\end{align}\n\nwhere $\\varepsilon_i \\stackrel{i.i.d.}{\\sim} \\mathcal N(0, 1)$. \n\nThe \"true\" value of $\\theta$ is:\n\n\\begin{align}\\theta_{true} = (-4.5,4.8,2.2)^T.\\end{align}\n\n\nWe use a normal prior on $\\underline{\\theta}$:\n\n\\begin{align}\\pi(\\underline{\\theta}) = \\mathcal N(\\mu_\\theta, \\Sigma_\\theta)\\end{align}\n\nwhere\n\n\\begin{align}\\mu_\\theta = \n    \\begin{pmatrix}\n     -3 \\\\\n      4 \\\\\n      1\n    \\end{pmatrix}\\end{align}\n\nis the mean of the prior and \n\n\\begin{align}\\Sigma_\\theta = \n   \\begin{pmatrix}\n     \\sigma_{\\theta_1}^2 & 0 & 0 \\\\\n     0 & \\sigma_{\\theta_2}^2 & 0 \\\\\n     0 & 0 & \\sigma_{\\theta_3}^2\n   \\end{pmatrix}\\end{align}\n\nis the prior covariance matrix with \n\n\\begin{align}\\sigma_{\\theta_1} = 2, \\qquad \\sigma_{\\theta_2} = 1, \\qquad \\sigma_{\\theta_3} = 1.5.\\end{align}\n\nThe following objects need to be defined in order to perform Bayesian calibration:\n\n- The conditional density $p(y|z)$ must be defined as a probability distribution\n- The computer model must be implemented thanks to the ParametricFunction class.\n  This takes a value of $\\underline{\\theta}$ as input, and outputs the vector of model predictions $\\underline z$,\n  as defined above (the vector of covariates $\\underline x = (x_1, \\ldots, x_n)$ is treated as a known constant).\n  When doing that, we have to keep in mind that $z$ will be used as the vector of parameters corresponding\n  to the distribution specified for $p(y |z)$. For instance, if $p(y|z)$ is normal,\n  this means that $z$ must be a vector containing the mean and variance of $y$\n- The prior density $\\pi(\\underline{\\theta})$ encoding the set of possible values for the calibration parameters,\n  each value being weighted by its a priori probability, reflecting the beliefs about the possible values\n  of $\\underline{\\theta}$ before consideration of the experimental data.\n  Again, this is implemented as a probability distribution\n- The Metropolis-Hastings algorithm that samples from the posterior distribution of the calibration parameters\n  requires a vector $\\underline{\\theta}_0$ initial values for the calibration parameters,\n  as well as the proposal laws used to update each parameter sequentially.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import openturns as ot\nimport openturns.viewer as viewer\nfrom matplotlib import pylab as plt\not.Log.Show(ot.Log.NONE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dimension of the vector of parameters to calibrate\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "paramDim = 3\n# The number of obesrvations\nobsSize = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Define the observed inputs $x_i$\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "xmin = -2.\nxmax = 3.\nstep = (xmax-xmin)/(obsSize-1)\nrg = ot.RegularGrid(xmin, step, obsSize)\nx_obs = rg.getVertices()\nx_obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Define the parametric model $z = f(x,\\theta)$ that associates each observation $x_i$ and values of the  parameters $\\theta_i$ to the parameters of the distribution of the corresponding observation: here $z=(\\mu, \\sigma)$ where $\\mu$, the first output of the model, is the mean and $\\sigma$, the second output of the model, is the standard deviation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fullModel = ot.SymbolicFunction(\n    ['x1', 'theta1', 'theta2', 'theta3'], ['theta1+theta2*x1+theta3*x1^2','1.0'])\nmodel = ot.ParametricFunction(fullModel, [0], x_obs[0])\nmodel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Define the observation noise $\\varepsilon {\\sim} \\mathcal N(0, 1)$ and create a sample from it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ot.RandomGenerator.SetSeed(0)\nnoiseStandardDeviation = 1.\nnoise = ot.Normal(0,noiseStandardDeviation)\nnoiseSample = noise.getSample(obsSize)\nnoiseSample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Define the vector of observations $y_i$\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this model, we use a constant value of the parameter. The \"true\" value of $\\theta$ is used to compute the model outputs. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "thetaTrue = [-4.5,4.8,2.2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y_obs = ot.Sample(obsSize,1)\nfor i in range(obsSize):\n    model.setParameter(x_obs[i])\n    y_obs[i,0] = model(thetaTrue)[0] + noiseSample[i,0]\ny_obs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Draw the model vs the observations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "functionnalModel = ot.ParametricFunction(fullModel, [1,2,3], thetaTrue)\ngraphModel = functionnalModel.getMarginal(0).draw(xmin,xmax)\nobservations = ot.Cloud(x_obs,y_obs)\nobservations = ot.Cloud(x_obs,y_obs)\nobservations.setColor(\"red\")\ngraphModel.add(observations)\ngraphModel.setLegends([\"Model\",\"Observations\"])\ngraphModel.setLegendPosition(\"topleft\")\nview = viewer.View(graphModel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Define the distribution of observations $\\underline{y} | \\underline{z}$ conditional on model predictions\n\nNote that its parameter dimension is the one of $\\underline{z}$, so the model must be adjusted accordingly\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "conditional = ot.Normal()\nconditional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Define the mean $\\mu_\\theta$, the covariance matrix $\\Sigma_\\theta$, then the prior distribution $\\pi(\\underline{\\theta})$ of the parameter $\\underline{\\theta}$. \n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "thetaPriorMean = [-3.,4.,1.]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sigma0 = [2.,1.,1.5]  # standard deviations\nthetaPriorCovarianceMatrix = ot.CovarianceMatrix(paramDim)\nfor i in range(paramDim):\n    thetaPriorCovarianceMatrix[i, i] = sigma0[i]**2\n\nprior = ot.Normal(thetaPriorMean, thetaPriorCovarianceMatrix)\nprior.setDescription(['theta1', 'theta2', 'theta3'])\nprior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Proposal distribution: uniform.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "proposal = [ot.Uniform(-1., 1.)] * paramDim\nproposal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the MCMC sampler\n\nThe MCMC sampler essentially computes the log-likelihood of the parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mymcmc = ot.MCMC(prior, conditional, model, x_obs, y_obs, thetaPriorMean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mymcmc.computeLogLikelihood(thetaPriorMean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Metropolis-Hastings sampler\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Creation of the Random Walk Metropolis-Hastings (RWMH) sampler.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "initialState = thetaPriorMean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "RWMHsampler = ot.RandomWalkMetropolisHastings(\n    prior, conditional, model, x_obs, y_obs, initialState, proposal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to check our model before simulating it, we compute the log-likelihood.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "RWMHsampler.computeLogLikelihood(initialState)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We observe that, as expected, the previous value is equal to the output of the same method in the MCMC object.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tuning of the RWMH algorithm.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Strategy of calibration for the random walk (trivial example: default).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "strategy = ot.CalibrationStrategyCollection(paramDim)\nRWMHsampler.setCalibrationStrategyPerComponent(strategy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Other parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "RWMHsampler.setVerbose(True)\nRWMHsampler.setThinning(1)\nRWMHsampler.setBurnIn(2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate a sample from the posterior distribution of the parameters theta.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sampleSize = 10000\nsample = RWMHsampler.getSample(sampleSize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Look at the acceptance rate (basic checking of the efficiency of the tuning; value close to 0.2 usually recommended).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "RWMHsampler.getAcceptanceRate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build the distribution of the posterior by kernel smoothing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kernel = ot.KernelSmoothing()\nposterior = kernel.build(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display prior vs posterior for each parameter.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from openturns.viewer import View\nimport pylab as pl\n\nfig = pl.figure(figsize=(12, 4))\n\nfor parameter_index in range(paramDim):\n    graph = posterior.getMarginal(parameter_index).drawPDF()\n    priorGraph = prior.getMarginal(parameter_index).drawPDF()\n    priorGraph.setColors(['blue'])\n    graph.add(priorGraph)\n    graph.setLegends(['Posterior', 'Prior'])\n    ax = fig.add_subplot(1, paramDim, parameter_index+1)\n    _ = ot.viewer.View(graph, figure=fig, axes=[ax])\n\n_ = fig.suptitle(\"Bayesian calibration\")\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}