{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Bayesian calibration of a computer code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example we are going to compute the parameters of a computer model thanks to Bayesian estimation.\n\nLet us denote $(y_1, \\dots, y_n)$ the observation sample,\n$(\\vect z_1, \\ldots, \\vect z_n) = (f(x_1|\\vect\\theta), \\ldots, f(x_n|\\vect\\theta))$ the model prediction,\n$p(y |\\vect z)$ the density function of observation $y$\nconditional on model prediction $\\vect z$,\nand $\\vect\\theta \\in \\mathbb{R}^p$ the calibration parameters we wish to estimate.\n\n\nThe posterior distribution is given by Bayes theorem:\n\n\\begin{align}\\pi(\\vect\\theta | \\vect y) \\quad \\propto \\quad L\\left(\\vect y | \\vect\\theta\\right) \\times \\pi(\\vect\\theta)$`\\end{align}\n\nwhere :math:$\\propto` means \"proportional to\", regarded as a function of $\\vect\\theta$.\n\nThe posterior distribution is approximated here by the empirical distribution of the sample $\\vect\\theta^1, \\ldots, \\vect\\theta^N$ generated by the Metropolis-Hastings algorithm. This means that any quantity characteristic of the posterior distribution (mean, variance, quantile, ...) is approximated by its empirical counterpart.\n\nOur model (i.e. the compute code to calibrate) is a standard normal linear regression, where\n\n\\begin{align}y_i = \\theta_1 + x_i \\theta_2 + x_i^2 \\theta_3 + \\varepsilon_i\\end{align}\n\nwhere $\\varepsilon_i \\stackrel{i.i.d.}{\\sim} \\mathcal N(0, 1)$.\n\nThe \"true\" value of $\\theta$ is:\n\n\\begin{align}\\vect \\theta_{true} = (-4.5,4.8,2.2)^T.\\end{align}\n\n\nWe use a normal prior on $\\vect\\theta$:\n\n\\begin{align}\\pi(\\vect\\theta) = \\mathcal N(\\vect{\\mu}_\\vect{\\theta}, \\mat{\\Sigma}_\\vect{\\theta})\\end{align}\n\nwhere\n\n\\begin{align}\\vect{\\mu}_\\vect{\\theta} =\n    \\begin{pmatrix}\n     -3 \\\\\n      4 \\\\\n      1\n    \\end{pmatrix}\\end{align}\n\nis the mean of the prior and\n\n\\begin{align}\\mat{\\Sigma}_\\vect{\\theta} =\n   \\begin{pmatrix}\n     \\sigma_{\\theta_1}^2 & 0 & 0 \\\\\n     0 & \\sigma_{\\theta_2}^2 & 0 \\\\\n     0 & 0 & \\sigma_{\\theta_3}^2\n   \\end{pmatrix}\\end{align}\n\nis the prior covariance matrix with\n\n\\begin{align}\\sigma_{\\theta_1} = 2, \\qquad \\sigma_{\\theta_2} = 1, \\qquad \\sigma_{\\theta_3} = 1.5.\\end{align}\n\nThe following objects need to be defined in order to perform Bayesian calibration:\n\n- The conditional density $p(y|\\vect z)$ must be defined as a probability distribution.\n- The computer model must be implemented thanks to the ParametricFunction class.\n  This takes a value of $\\vect\\theta$ as input, and outputs the vector of model predictions $\\vect z$,\n  as defined above (the vector of covariates $\\vect x = (x_1, \\ldots, x_n)$ is treated as a known constant).\n  When doing that, we have to keep in mind that $\\vect z$ will be used as the vector of parameters corresponding\n  to the distribution specified for $p(y |\\vect z)$. For instance, if $p(y|\\vect z)$ is normal,\n  this means that $\\vect z$ must be a vector containing the mean and standard deviation of $y$.\n- The prior density $\\pi(\\vect\\theta)$ encoding the set of possible values for the calibration parameters,\n  each value being weighted by its a priori probability, reflecting the beliefs about the possible values\n  of $\\vect\\theta$ before consideration of the experimental data.\n  Again, this is implemented as a probability distribution.\n- Metropolis-Hastings algorithm(s), possibly used in tandem with a Gibbs algorithm\n  in order to sample from the posterior distribution of the calibration parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pylab as pl\nimport openturns as ot\nimport openturns.viewer as viewer\nfrom matplotlib import pylab as plt\not.Log.Show(ot.Log.NONE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dimension of the vector of parameters to calibrate\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "paramDim = 3\n# The number of obesrvations\nobsSize = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the observed inputs $x_i$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "xmin = -2.\nxmax = 3.\nstep = (xmax-xmin)/(obsSize-1)\nrg = ot.RegularGrid(xmin, step, obsSize)\nx_obs = rg.getVertices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the parametric model $\\vect z = f(x,\\vect\\theta)$ that associates each\nobservation $x$ and value of $\\vect \\theta$ to the parameters\nof the distribution of the corresponding observation $y$:\nhere $\\vect z=(\\mu, \\sigma)$ where $\\mu$,\nthe first output of the model, is the mean and $\\sigma$,\nthe second output of the model, is the standard deviation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fullModel = ot.SymbolicFunction(\n    ['x', 'theta1', 'theta2', 'theta3'], ['theta1+theta2*x+theta3*x^2', '1.0'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To differentiate between the two classes of inputs ($x$ and $\\vect\\theta$),\nwe define a :class:`~openturns.ParametricFunction` from `fullModel`\nand make the first input (the observations $x$) its *parameter*:\n$f_x(\\vect \\theta) := f(x, \\vect \\theta)$.\nWe set $x = 1$ as a placeholder,\nbut $x$ will actually take the values $x_i$ of the observations\nwhen we sample $\\vect\\theta$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "linkFunction = ot.ParametricFunction(fullModel, [0], [1.0])\nprint(linkFunction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the observation noise $\\varepsilon {\\sim} \\mathcal N(0, 1)$ and create a sample from it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ot.RandomGenerator.SetSeed(0)\nnoiseStandardDeviation = 1.\nnoise = ot.Normal(0, noiseStandardDeviation)\nnoiseSample = noise.getSample(obsSize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the vector of observations $y_i$,\nhere sampled using the \"true\" value of $\\vect \\theta$: $\\vect \\theta_{true}$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "thetaTrue = [-4.5, 4.8, 2.2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y_obs = ot.Sample(obsSize, 1)\nfor i in range(obsSize):\n    linkFunction.setParameter(x_obs[i])\n    y_obs[i, 0] = linkFunction(thetaTrue)[0] + noiseSample[i, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Draw the model predictions vs the observations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "functionnalModel = ot.ParametricFunction(fullModel, [1, 2, 3], thetaTrue)\ngraphModel = functionnalModel.getMarginal(0).draw(xmin, xmax)\nobservations = ot.Cloud(x_obs, y_obs)\nobservations = ot.Cloud(x_obs, y_obs)\nobservations.setColor(\"red\")\ngraphModel.add(observations)\ngraphModel.setLegends([\"Model\", \"Observations\"])\ngraphModel.setLegendPosition(\"topleft\")\nview = viewer.View(graphModel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the distribution of observations $y | \\vect{z}$ conditional on model predictions.\n\nNote that its parameter dimension is the one of $\\vect{z}$, so the model must be adjusted accordingly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "conditional = ot.Normal()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the mean $\\mu_\\theta$, the covariance matrix $\\Sigma_\\theta$, then the prior distribution $\\pi(\\vect\\theta)$ of the parameter $\\vect\\theta$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "thetaPriorMean = [-3., 4., 1.]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sigma0 = [2., 1., 1.5]  # standard deviations\nthetaPriorCovarianceMatrix = ot.CovarianceMatrix(paramDim)\nfor i in range(paramDim):\n    thetaPriorCovarianceMatrix[i, i] = sigma0[i]**2\n\nprior = ot.Normal(thetaPriorMean, thetaPriorCovarianceMatrix)\nprior.setDescription(['theta1', 'theta2', 'theta3'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The proposed steps for\n$\\theta_1$, $\\theta_2$ and $\\theta_3$\nwill all follow a uniform distribution.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "proposal = ot.Uniform(-1., 1.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Metropolis-Hastings sampler\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creation of a single component random walk Metropolis-Hastings (RWMH) sampler.\nThis involves a combination of the RWMH and the Gibbs algorithms.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "initialState = thetaPriorMean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create a :class:`~openturns.RandomWalkMetropolisHastings` sampler for each component.\nEach sampler must be aware of the joint prior distribution.\nWe also use the same proposal distribution, but this is not mandatory.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mh_coll = [ot.RandomWalkMetropolisHastings(prior, initialState, proposal, [i]) for i in range(paramDim)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each sampler must be made aware of the likelihood.\nOtherwise we would sample from the prior!\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for mh in mh_coll: mh.setLikelihood(conditional, y_obs, linkFunction, x_obs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, the :class:`~openturns.Gibbs` algorithm is constructed from all Metropolis-Hastings samplers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sampler = ot.Gibbs(mh_coll)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tuning of the Gibbs algorithm:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sampler.setThinning(1)\nsampler.setBurnIn(2000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate a sample from the posterior distribution of the parameters $\\vect \\theta$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sampleSize = 10000\nsample = sampler.getSample(sampleSize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Look at the acceptance rate (basic check of the sampling efficiency:\nvalues close to $0.2$ are usually recommended\nfor Normal posterior distributions).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "[mh.getAcceptanceRate() for mh in sampler.getMetropolisHastingsCollection()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build the distribution of the posterior by kernel smoothing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kernel = ot.KernelSmoothing()\nposterior = kernel.build(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display prior vs posterior for each parameter.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig = pl.figure(figsize=(12, 4))\n\nfor parameter_index in range(paramDim):\n    graph = posterior.getMarginal(parameter_index).drawPDF()\n    priorGraph = prior.getMarginal(parameter_index).drawPDF()\n    priorGraph.setColors(['blue'])\n    graph.add(priorGraph)\n    graph.setLegends(['Posterior', 'Prior'])\n    ax = fig.add_subplot(1, paramDim, parameter_index+1)\n    _ = ot.viewer.View(graph, figure=fig, axes=[ax])\n\n_ = fig.suptitle(\"Bayesian calibration\")\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}