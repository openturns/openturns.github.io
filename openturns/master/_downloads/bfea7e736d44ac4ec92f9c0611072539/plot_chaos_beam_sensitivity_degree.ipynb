{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Polynomial chaos is sensitive to the degree\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n\nIn this example, we observe the sensitivity of the polynomial chaos expansion to the total degree of the polynomial.\nMore precisely, we observe how this impacts the $Q^2$ predictivity coefficient.\n\nWe consider the example of the cantilever beam. We create a sparse polynomial chaos with a linear enumeration rule\nand the family of orthogonal polynomials corresponding to each input variable.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import openturns as ot\nimport numpy as np\nimport openturns.viewer\nimport pylab as pl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following parameter value leads to fast simulations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "maxDegree = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For real tests, we suggest using the following parameter value:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "maxDegree = 7\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us define the parameters of the cantilever beam problem.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dist_E = ot.Beta(0.9, 2.2, 2.8e7, 4.8e7)\ndist_E.setDescription([\"E\"])\nF_para = ot.LogNormalMuSigma(3.0e4, 9.0e3, 15.0e3)  # in N\ndist_F = ot.ParametrizedDistribution(F_para)\ndist_F.setDescription([\"F\"])\ndist_L = ot.Uniform(250.0, 260.0)  # in cm\ndist_L.setDescription([\"L\"])\ndist_I = ot.Beta(2.5, 1.5, 310.0, 450.0)  # in cm^4\ndist_I.setDescription([\"I\"])\n\nmyDistribution = ot.JointDistribution([dist_E, dist_F, dist_L, dist_I])\n\ndim_input = 4  # dimension of the input\ndim_output = 1  # dimension of the output\n\n\ndef function_beam(X):\n    E, F, L, II = X\n    Y = F * L**3 / (3 * E * II)\n    return [Y]\n\n\ng = ot.PythonFunction(dim_input, dim_output, function_beam)\ng.setInputDescription(myDistribution.getDescription())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function creates a sparse polynomial chaos with a given total degree.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def ComputeSparseLeastSquaresChaos(\n    inputTrain, outputTrain, multivariateBasis, totalDegree, myDistribution\n):\n    \"\"\"\n    Create a sparse polynomial chaos based on least squares.\n\n    * Uses the enumerate rule in multivariateBasis.\n    * Uses the LeastSquaresStrategy to compute the coefficients based on\n      least squares.\n    * Uses LeastSquaresMetaModelSelectionFactory to use the LARS selection method.\n    * Uses FixedStrategy in order to keep all the coefficients that the\n      LARS method selected.\n\n    Parameters\n    ----------\n    inputTrain : ot.Sample\n        The input design of experiments.\n    outputTrain : ot.Sample\n        The output design of experiments.\n    multivariateBasis : ot.Basis\n        The multivariate chaos basis.\n    totalDegree : int\n        The total degree of the chaos polynomial.\n    myDistribution : ot.Distribution.\n        The distribution of the input variable.\n    Returns\n    -------\n    result : ot.PolynomialChaosResult\n        The estimated polynomial chaos.\n    \"\"\"\n    selectionAlgorithm = ot.LeastSquaresMetaModelSelectionFactory()\n    projectionStrategy = ot.LeastSquaresStrategy(selectionAlgorithm)\n    enumfunc = multivariateBasis.getEnumerateFunction()\n    basisSize = enumfunc.getBasisSizeFromTotalDegree(totalDegree)\n    adaptiveStrategy = ot.FixedStrategy(multivariateBasis, basisSize)\n    chaosalgo = ot.FunctionalChaosAlgorithm(\n        inputTrain, outputTrain, myDistribution, adaptiveStrategy, projectionStrategy\n    )\n    chaosalgo.run()\n    result = chaosalgo.getResult()\n    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function computes the sparsity rate of the polynomial chaos.\nTo do this, we compute the number of coefficients in the decomposition assuming a linear enumeration rule and a fixed truncation.\nThe sparsity rate is the complement of the ratio between the number of coefficients\nselected from LARS and the total number of coefficients in the full polynomial basis.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def computeSparsityRate(multivariateBasis, totalDegree, chaosResult):\n    \"\"\"Compute the sparsity rate, assuming a FixedStrategy.\"\"\"\n    # Get basisSize, the number of candidate coefficients\n    enumfunc = multivariateBasis.getEnumerateFunction()\n    basisSize = enumfunc.getStrataCumulatedCardinal(totalDegree)\n    # Get number of coefficients in the selection\n    indices = chaosResult.getIndices()\n    nbcoeffs = indices.getSize()\n    # Compute rate\n    sparsityRate = 1.0 - nbcoeffs / basisSize\n    return sparsityRate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following functions compute and plot the R2 predictivity coefficients within the validation plot.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def computeR2Chaos(chaosResult, inputTest, outputTest):\n    \"\"\"Compute the R2 of a chaos.\"\"\"\n    metamodel = chaosResult.getMetaModel()\n    metamodelPredictions = metamodel(inputTest)\n    val = ot.MetaModelValidation(outputTest, metamodelPredictions)\n    r2Score = val.computeR2Score()[0]\n    r2Score = max(r2Score, 0.0)  # We are not lucky every day.\n    return r2Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def printChaosStats(multivariateBasis, chaosResult, inputTest, outputTest, totalDegree):\n    \"\"\"Print statistics of a chaos.\"\"\"\n    sparsityRate = computeSparsityRate(multivariateBasis, totalDegree, chaosResult)\n    r2Score = computeR2Chaos(chaosResult, inputTest, outputTest)\n    metamodel = chaosResult.getMetaModel()\n    metamodelPredictions = metamodel(inputTest)\n    val = ot.MetaModelValidation(outputTest, metamodelPredictions)\n    graph = val.drawValidation().getGraph(0, 0)\n    legend1 = \"D=%d, R2=%.2f%%\" % (totalDegree, 100 * r2Score)\n    graph.setLegends([\"\", legend1])\n    graph.setLegendPosition(\"upper left\")\n    print(\n        \"Degree=%d, R2=%.2f%%, Sparsity=%.2f%%\"\n        % (totalDegree, 100 * r2Score, 100 * sparsityRate)\n    )\n    return graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "multivariateBasis = ot.OrthogonalProductPolynomialFactory(\n    [dist_E, dist_F, dist_L, dist_I]\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "N = 20  # size of the train design"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_valid = 1000  # size of the test design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The seed is selected to get *interesting* results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "magicSeed = 43  # 127 is funny too\not.RandomGenerator.SetSeed(magicSeed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inputTrain = myDistribution.getSample(N)\noutputTrain = g(inputTrain)\ninputTest = myDistribution.getSample(n_valid)\noutputTest = g(inputTest)\nfig = pl.figure(figsize=(25, 4))\nfor totalDegree in range(1, maxDegree + 1):\n    chaosResult = ComputeSparseLeastSquaresChaos(\n        inputTrain, outputTrain, multivariateBasis, totalDegree, myDistribution\n    )\n    graph = printChaosStats(\n        multivariateBasis, chaosResult, inputTest, outputTest, totalDegree\n    )\n    ax = fig.add_subplot(1, maxDegree, totalDegree)\n    _ = ot.viewer.View(graph, figure=fig, axes=[ax])\n    pl.suptitle(\"Metamodel validation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that when the degree of the polynomial increases, the R2 coefficient decreases.\nWe also see that the sparsity rate increases: while the basis size grows rapidly with the degree, the algorithm selects a smaller fraction of this basis.\nThis shows that the algorithm performs its task of selecting relevant coefficients.\nHowever, this selection does not seem to be sufficient to mitigate the large number of coefficients.\n\nOf course, this example is designed to make a predictivity decrease gradually.\nWe are going to see that this situation is actually easy to reproduce.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Distribution of the predictivity coefficient\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us repeat the following experiment to see the variability of the R2 coefficient.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def computeSampleR2(N, n_valid, numberAttempts, maxDegree):\n    \"\"\"For a given sample size N, for degree from 1 to maxDegree,\n    repeat the following experiment numberAttempts times:\n    create a sparse least squares chaos and compute the R2\n    using n_valid points.\n    \"\"\"\n    r2Sample = ot.Sample(numberAttempts, maxDegree)\n    for totalDegree in range(1, maxDegree + 1):\n        print(\"Degree = %d\" % (totalDegree))\n        for i in range(numberAttempts):\n            inputTrain = myDistribution.getSample(N)\n            outputTrain = g(inputTrain)\n            inputTest = myDistribution.getSample(n_valid)\n            outputTest = g(inputTest)\n            chaosResult = ComputeSparseLeastSquaresChaos(\n                inputTrain, outputTrain, multivariateBasis, totalDegree, myDistribution\n            )\n            r2Sample[i, totalDegree - 1] = computeR2Chaos(\n                chaosResult, inputTest, outputTest\n            )\n    return r2Sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function uses a boxplot to see the distribution of the R2 coefficients.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plotR2Boxplots(r2Sample, N):\n    data = np.array(r2Sample)\n    pl.figure()\n    pl.boxplot(data)\n    pl.title(\"N=%d\" % (N))\n    pl.xlabel(\"Degree\")\n    pl.ylabel(\"R2 (%)\")\n    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each experiment is repeated several times.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "numberAttempts = 50  # Number of repetitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "N = 20  # size of the train design\nr2Sample = computeSampleR2(N, n_valid, numberAttempts, maxDegree)\nplotR2Boxplots(r2Sample, N)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that when the size of the design of experiments is as small as 20, it is more appropriate to use a very low degree polynomial. Here 1 performs best and 4 is risky.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "N = 30  # size of the train design\nr2Sample = computeSampleR2(N, n_valid, numberAttempts, maxDegree)\nplotR2Boxplots(r2Sample, N)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With a 30-point design set, a polynomial degree of 2 is usually advisable.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "N = 50  # size of the train design\nr2Sample = computeSampleR2(N, n_valid, numberAttempts, maxDegree)\nplotR2Boxplots(r2Sample, N)\n\npl.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When the sample size increases, the R2 computation becomes less sensitive to the polynomial degree.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nWe observe that on the cantilever beam example, to use a polynomial total\ndegree equal to 4, we need a sample size at least equal to 50 to get a\nsatisfactory and reproducible R2.\nWhen the degree is equal to 4, if the sample is small, then depending on the\nparticular sample, the predictivity coefficient can be very low (i.e. less than 0.5).\nWith a sample size as small as 20, a polynomial degree of 1 is safer.\nHowever the limited sample size may have an impact on other statistics that\ncould be derived from a metamodel calculated on such a small training sample.\n\n## References\n\n* \"Metamodel-Based Sensitivity Analysis: Polynomial Chaos Expansions and Gaussian Processes\", Lo\u00efc Le Gratiet, Stefano Marelli, Bruno Sudret,\n  Handbook of Uncertainty Quantification, 2017, Springer International Publishing.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}