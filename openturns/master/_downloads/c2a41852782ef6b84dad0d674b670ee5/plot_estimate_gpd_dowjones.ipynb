{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Estimate a GPD on the Dow Jones Index data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we illustrate various techniques of extreme value modeling applied\nto the 5-year series of daily Dow Jones Index closing prices.\nReaders should refer to [coles2001]_ example 1.8 to get more details.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import math as m\nimport openturns as ot\nimport openturns.experimental as otexp\nimport openturns.viewer as otv\nfrom openturns.usecases import coles\nimport pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we load the Dow Jones dataset and plot it through time.\nWe can see that the process is non-stationary.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "full = pd.read_csv(coles.Coles().dowjones, index_col=0, parse_dates=True)\nprint(full[:10])\ngraph = ot.Graph(\n    \"Daily closing prices of the Dow Jones Index\", \"Day index\", \"Index\", True, \"\"\n)\n# Care: to get the real period range, multiply by a factor to account for non-working days missing values!\nsize = len(full)\ndays = ot.Sample([[i] for i in range(size)])\ndataDowJones = ot.Sample.BuildFromDataFrame(full)\ncurve = ot.Curve(days, dataDowJones)\ncurve.setColor(\"red\")\ngraph.add(curve)\ngraph.setIntegerXTick(True)\nview = otv.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In that example, the time dependence can\nnot be explained by trends or seasonal cycles. Many empirical\nstudies have advised to consider the logarithms of ratios of\nsuccessive observations to get an approximation to stationarity.\nWe apply that transformation:\n\n\\begin{align}\\tilde{X}_i = \\log X_i - \\log X_{i-1}.\\end{align}\n\nThe resulting time series appears to be reasonably close to stationarity.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "transfDataDJ = ot.Sample(\n    [\n        [m.log(dataDowJones[i, 0]) - m.log(dataDowJones[i - 1, 0])]\n        for i in range(1, size)\n    ]\n)\ncurve = ot.Curve(days[:-1], transfDataDJ)\ngraph = ot.Graph(\n    \"Log-daily returns of the Dow Jones Index\", \"Day index\", \"Index\", True, \"\"\n)\ngraph.add(curve)\nview = otv.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For convenience of presentation, we rescale the data:\n\n\\begin{align}\\tilde{X}_i \\rightarrow 100\\tilde{X}_i.\\end{align}\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scalTransfDataDJ = transfDataDJ * 100.0\nsize = len(scalTransfDataDJ)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to select a threshold upon which the GPD model will be fitted, we draw\nthe mean residual life plot with approximate $95\\%$ confidence interval.\nThe curve is initially linear and shows significant curvature for\n$u \\in [-1, 2]$. Then for $u \\geq 2$, the curve is considered as\nreasonably linear when judged to confidence intervals. Hence, we\nchoose the threshold $u=2$. There are 37 exceedances of $u$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "factory = ot.GeneralizedParetoFactory()\ngraph = factory.drawMeanResidualLife(scalTransfDataDJ)\nview = otv.View(graph)\nu = 2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Stationary GPD modeling assuming independence in data**\n\nWe first assume that the dependence between the transformed data\nis negligible, so we first consider the data as\nindependent observations over the observation period.\nWe estimate the parameters of the GPD distribution modeling the excesses\nabove $u=2$ by maximizing the log-likelihood of the excesses.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result_LL = factory.buildMethodOfLikelihoodMaximizationEstimator(scalTransfDataDJ, u)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We get the fitted GPD and the estimated parameters $(\\hat{\\sigma}, \\hat{\\xi})$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fitted_GPD = result_LL.getDistribution()\ndesc = fitted_GPD.getParameterDescription()\nparam = fitted_GPD.getParameter()\nprint(\", \".join([f\"{p}: {value:.3f}\" for p, value in zip(desc, param)]))\nprint(\"log-likelihood = \", result_LL.getLogLikelihood())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We get the asymptotic distribution of the estimator $(\\hat{\\sigma}, \\hat{\\xi})$.\nThe threshold $u$ has not been estimated to ensure the regularity\nof the model and then the asymptotic properties of the maximum likelihood\nestimators. This is the reason why it appears as a Dirac distribution centered on\nthe chosen threshold.\nIn that case, the asymptotic distribution of $(\\hat{\\sigma}, \\hat{\\xi})$\nis normal.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "parameterEstimate = result_LL.getParameterDistribution()\nprint(\"Asymptotic distribution of the estimator : \")\nprint(parameterEstimate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We get the covariance matrix and the standard deviation of\n$(\\hat{\\sigma}, \\hat{\\xi}, u)$ where $u$ is deterministic.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Cov matrix = \\n\", parameterEstimate.getCovariance())\nprint(\"Standard dev = \", parameterEstimate.getStandardDeviation())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We get the marginal confidence intervals of order 0.95 of\n$(\\hat{\\sigma}, \\hat{\\xi})$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "order = 0.95\nfor i in range(2):  # exclude u parameter (fixed)\n    ci = parameterEstimate.getMarginal(i).computeBilateralConfidenceInterval(order)\n    print(desc[i] + \":\", ci)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At last, we can check the quality of the inference thanks to the 4 usual\ndiagnostic plots:\n\n- the probability-probability pot,\n- the quantile-quantile pot,\n- the return level plot,\n- the data histogram and the density of the fitted model.\n\nWe conclude that the goodness-of-fit in the quantile plots seems unconvincing,\neven if the other plots appear to be reasonable. This is due to the fact that\nthe excesses can not be considered as independent: the transformed series\n$\\tilde{X}_i$ has a rich structure of temporal dependence.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "validation = otexp.GeneralizedParetoValidation(result_LL, scalTransfDataDJ)\ngraph = validation.drawDiagnosticPlot()\nview = otv.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Stationary GPD modeling taking into account the dependence in data**\n\nWe illustrate the fact that the excesses of the transformed series happen in\ngroups. Hence we use the declustering method\nwhich filters the dependent observations exceeding a given threshold to obtain a\nset of threshold excesses that can be assumed as independent.\n\nConsecutive exceedances of $u$ belong to the same cluster. Two distinct\nclusters are separated by $r$ consecutive observations under the\nthreshold. Within each cluster, we select the maximum value that will be used to\ninfer the GPD distribution. The cluster maxima are assumed to be independent.\n\nOn the graph, we show the clusters over the threshold $u=2$\nand all the maxima selected within each cluster.\nIt is possible to extract the data belonging to the same cluster and the\ncluster maximum series.\nWe denote by $n_c$ the number of clusters and\nby $n_u$ the number of exceedances above $u$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "part = otexp.SamplePartition(scalTransfDataDJ)\nr = 3\npeaks, clusters = part.getPeakOverThreshold(u, r)\nnc = len(peaks)\nnu = sum([1 if scalTransfDataDJ[i, 0] > u else 0 for i in range(size)])\nprint(f\"nc={nc} nu={u} theta={nc / nu:.3f}\")\ngraph = clusters.draw(u)\ngraph.setTitle(\n    \"Threshold exceedances and clusters by transformed Dow Jones Index series\"\n)\nview = otv.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We estimate a stationary GPD on the clusters maxima which are independent\nwith the $95\\%$ confidence interval of each parameter.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result_LL = factory.buildMethodOfLikelihoodMaximizationEstimator(peaks, u)\nsigma, xi, _ = result_LL.getParameterDistribution().getMean()\nsigma_stddev, xi_stddev, _ = result_LL.getParameterDistribution().getStandardDeviation()\nprint(\n    f\"u={u} r={r} nc={nc} sigma={sigma:.2f} ({sigma_stddev:.2f}) xi={xi:.2f} ({xi_stddev:.2f})\",\n    end=\" \",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We evaluate the $T=100$-year return level which corresponds to the\n$m$-observation return level, where $m = T*n_y$ with $n_y$\nthe number of observations per year. Here, we have daily observations, hence\n$n_y = 365$. To calculate it, we evaluate the extremal index\n$\\theta$ which is the inverse of the mean length of the clusters,\nestimated by the ratio between the number of clusters and the number\nof exceedances of $u$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "theta = nc / nu\nny = 365\nT = 100\nxm_100 = factory.buildReturnLevelEstimator(result_LL, scalTransfDataDJ, T * ny, theta)\nprint(f\"x100={xm_100.getMean()} ({xm_100.getStandardDeviation()}) theta={theta:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We plot the return level for the new fitted model that takes into account\ndependence between excesses.\nWe can see the fitted model works well. However, the large return level confidence intervals\nobtained for extreme return levels makes it difficult to make reliable\npredictions with any degree of certainty.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "validation = otexp.GeneralizedParetoValidation(result_LL, peaks)\ngrid = validation.drawDiagnosticPlot()\nrlPlot = grid.getGraph(1, 0)\nrlPlot.setTitle(rlPlot.getTitle() + f\" (u={u} r={r})\")\nview = otv.View(rlPlot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We plot the whole series of validation graphs of the new fitted model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "view = otv.View(grid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "otv.View.ShowAll()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}