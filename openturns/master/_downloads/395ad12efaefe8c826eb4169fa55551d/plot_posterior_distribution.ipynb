{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Compare frequentist and Bayesian estimation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we want to estimate of the parameter\n$\\vect{\\Theta}$ of the distribution of a random vector $\\inputRV$ from which we have some data.\nWe compare the frequentist and the Bayesian approaches to estimate $\\vect{\\theta}$.\n\nLet $\\inputRV = (X_0, X_1)$ be a random vector following a bivariate normal distribution\nwith zero mean, unit variance and independent components:\n$\\cN_2 \\left(\\vect{\\theta}\\right)$\nwhere the parameter $\\vect{\\theta} = (\\mu_0, \\sigma_0, \\mu_1, \\sigma_1, \\rho) = (0, 1, 0, 1, 0)$.\nWe assume to have a sample generated from $\\vect{X}$ denoted by\n$(\\inputReal_1, \\dots, \\inputReal_\\sampleSize)$ where $\\sampleSize = 25$.\n\nWe assume to know the parameters $(\\mu_0, \\mu_1, \\rho)$ and we want to estimate the parameters $(\\sigma_0, \\sigma_1)$.\nIn the Bayesian approach, we assume that $\\vect{\\Theta} = (0, \\Sigma_0, 0, \\Sigma_1, 0)$ is a random vector and we define\na link function $g : \\Rset^2 \\rightarrow \\Rset^5$ such that:\n\n\\begin{align}\\vect{\\Theta} = g(\\vect{Y})\\end{align}\n\nwhere $\\vect{Y}$ follows the prior distribution denoted by $\\pi_{\\vect{Y}}^0$.\n\nNote that the link function $g$ already contains the information that the two components of $\\inputRV$\nare independent (as $\\rho = 0$) and centered (as $\\mu_i = 0$).\n\nWe consider two interesting link functions:\n\n\\begin{align}g_1(\\vect{y}) & = (0, y_0, 0, y_1, 0) \\\\\n  g_2(\\vect{y}) & = (0,0.5+y_0^2, 0, 0.5+y_1^2, 0)\\end{align}\n\neach one being associated to the respective prior distributions:\n\n\\begin{align}\\pi_{\\vect{Y}}^{0,1} & = \\cT(0,1,2) \\times \\cT(0,1,2) \\\\\n  \\pi_{\\vect{Y}}^{0,2} & = \\cT(-1, 0, 1) \\times \\cT(-1, 0,1)\\end{align}\n\nThe second case is such that the link function $g_2$ is not bijective on the range of $\\pi_{\\vect{Y}}^{0,2}$.\n\nThe Bayesian approach uses the :class:`~openturns.PosteriorDistribution` that estimates the posterior distribution of $\\vect{Y}$ denoted by\n$\\pi_{\\vect{Y}}^\\sampleSize$ maximizing the likelihood of the conditioned model on the sample, weighted by the prior distribution\n$\\pi_{\\vect{Y}}^0$. From the $\\pi_{\\vect{Y}}^\\sampleSize$ distribution, we extract the vector of modes\ndenoted by $\\vect{Y}_n^m$: this point maximizes $\\pi_{\\vect{Y}}^\\sampleSize$.\n\nIt is interesting to note that when $n \\rightarrow +\\infty$, then $\\pi_{\\vect{Y}}^\\sampleSize \\rightarrow \\pi_{\\vect{Y}}^*$\nsuch that $g(\\pi_{\\vect{Y}}^*)$ is a Dirac distribution centered at $\\vect{\\theta}$.\n\nThe frequentist approach estimates the parameter $\\vect{\\theta}$ by maximizing the likelihood of the normal model on the\nsample. To model the same level of information, we consider a centered bivariate normal model with independent components.\nWe denote by $\\vect{\\theta}_n^{MC}$ the parameter obtained.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import openturns as ot\nimport openturns.viewer as otv\n\not.ResourceMap.SetAsUnsignedInteger(\n    \"CompoundDistribution-MarginalIntegrationNodesNumber\", 32\n)\not.ResourceMap.SetAsString(\n    \"CompoundDistribution-ContinuousDiscretizationMethod\", \"GaussProduct\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we define a function that computes the mode of a distribution, which is the point that maximises its PDF.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def computeMode(distribution):\n    def obj_py(X):\n        return distribution.computeLogPDF(X) * (-1.0)\n\n    obj = ot.PythonFunction(distribution.getDimension(), 1, func_sample=obj_py)\n    pb = ot.OptimizationProblem(obj)\n    pb.setBounds(distribution.getRange())\n    algo = ot.Cobyla(pb)\n    algo.setStartingPoint(distribution.getMean())\n    algo.run()\n    return algo.getResult().getOptimalPoint()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the theoretical $\\vect{X}$ distribution which is a normal distribution with zero mean,\nunit variance and independent components.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "conditioned = ot.Normal(2)\nconditioned.setDescription([\"X0\", \"X1\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create the sample that we are going to use to estimate the parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Nsample = 25\nobservations = conditioned.getSample(Nsample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Case 1: Bijective link function\n\nHere, $g_1(\\vect{y}) = (0, y_0, 0, y_1, 0)$ and $\\pi_{\\vect{Y}}^{0,1} = \\cT(0,1,2) \\times \\cT(0,1,2)$.\n\nWe create the link function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "linkFunction = ot.SymbolicFunction([\"u0\", \"u1\"], [\"0.0\", \"u0\", \"0.0\", \"u1\", \"0.0\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create the prior distribution of $\\vect{Y}$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "conditioning = ot.JointDistribution([ot.Triangular(0.0, 1.0, 2.0)] * 2)\nconditioning.setDescription([\"Y0\", \"Y1\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have to decondition the $\\vect{X}|\\vect{\\Theta} = g(\\vect{Y})$ distribution with\nrespect to the  prior distribution $\\pi_{\\vect{Y}}^{0,1}$ in order to get the\nfinal distribution of $\\vect{X}$. To do that, we use the :class:`~openturns.CompoundDistribution`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "compound = ot.CompoundDistribution(conditioned, conditioning, linkFunction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we can create the posterior distribution $\\pi_{\\vect{Y}}^\\sampleSize$ based on the compound distribution of $\\vect{X}$ and\nthe sample.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "posterior_Y = ot.PosteriorDistribution(compound, observations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From  $\\pi_{\\vect{Y}}^\\sampleSize$, we get:\n\n- the mode parameter $\\vect{Y}_n^m$ that maximizes the PDF,\n- the distribution of $\\inputRV$  parameterized by $\\vect{\\theta}_n^m = g(\\vect{Y}_n^m)$,\n- a bilateral condifence interval of level $\\alpha$ of the posterior distribution of $\\vect{Y}$,\n- the volume of this condifence interval,\n- the Kullback-Leibler distance between the Bayesian mode based distribution and the theoretical one:\n  $KL\\left(\\cN_2\\left(g(\\vect{Y}_n^m)\\right), \\cN_2 \\left(\\vect{\\theta}\\right) \\right)$.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "theta_Bay = linkFunction(computeMode(posterior_Y))\nprint(\"Theta Bay =\", theta_Bay)\nmodel_Bay = ot.Distribution(conditioned)\nmodel_Bay.setParameter(theta_Bay)\ndist_estimateur_Bay = posterior_Y\nalpha = 0.95\ninterval_Bay, beta = (\n    dist_estimateur_Bay.computeBilateralConfidenceIntervalWithMarginalProbability(alpha)\n)\nprint(\"Beta =\", beta)\nprint(\"Condifence interval Bay =\\n\", interval_Bay)\nprint(\"Volume =\", interval_Bay.getVolume())\nsample = model_Bay.getSample(10000)\ndist_Bay = (\n    model_Bay.computeLogPDF(sample) - conditioned.computeLogPDF(sample)\n).computeMean()\nprint(\"Kullback-Leibler distance Bay =\", dist_Bay[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The frequentist approach estimates a normal distribution from the sample. We fix the known parameters $(\\mu_0, \\mu_1, \\rho)$\nto their true values.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lh_factory = ot.NormalFactory()\nlh_factory.setKnownParameter([0.0, 0.0, 0.0], [0, 2, 4])\nlh_est = lh_factory.buildEstimator(observations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We extract from the likelihood estimate:\n\n- the asymptotic distribution of the estimator,\n- the parameters estimates,\n- a bilateral condifence interval of level $\\alpha$ of the asymptotic distribution of the estimator,\n- the volume of this condifence interval,\n- the Kullback-Leibler distance between the maximum likelihood based distribution and the theoretical one:\n  $KL\\left(\\cN_2 \\left(\\vect{\\theta}_n^{MV} \\right), \\cN_2 \\left(\\vect{\\theta}\\right) \\right)$.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_ML = lh_est.getDistribution()\ntheta_ML = model_ML.getParameter()\nprint(\"Theta ML = \", theta_Bay)\ndist_estimator_ML = lh_est.getParameterDistribution().getMarginal([1, 3])\ninterval_ML, beta = (\n    dist_estimator_ML.computeBilateralConfidenceIntervalWithMarginalProbability(alpha)\n)\nprint(\"Beta =\", beta)\nprint(\"Condifence interval ML =\\n\", interval_ML)\nprint(\"Volume =\", interval_ML.getVolume())\nsample = model_ML.getSample(10000)\ndist_KL = (\n    model_ML.computeLogPDF(sample) - conditioned.computeLogPDF(sample)\n).computeMean()\nprint(\"Kullback-Leibler distance ML =\", dist_KL[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following figure, we  plot:\n\n- the theoretical distribution of $\\inputRV$:  $\\cN_2 \\left(\\vect{\\theta}\\right)$ (solid lines),\n- its maximum likelihood based distribution: $\\cN_2 \\left(\\vect{\\theta}_n^{MV}\\right)$ (dashed lines),\n- its Bayesian mode based distribution: $\\cN_2(g(\\vect{Y}_n^m))$ (dotted lines).\n\nWe conclude that both approaches lead to the same results.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ot.ResourceMap.SetAsString(\"Contour-DefaultColorMapNorm\", \"rank\")\ng = conditioned.drawPDF()\nlevels = g.getDrawable(0).getLevels()\ndr_ML = model_ML.drawPDF().getDrawable(0).getImplementation()\ndr_ML.setLevels(levels)\ndr_ML.setColorBarPosition(\"\")\ndr_ML.setLineStyle(\"dashed\")\ng.add(dr_ML)\ndr_Bay = model_Bay.drawPDF().getDrawable(0).getImplementation()\ndr_Bay.setLevels(levels)\ndr_Bay.setColorBarPosition(\"\")\ndr_Bay.setLineStyle(\"dotted\")\ng.add(dr_Bay)\ng.add(ot.Cloud(observations))\ng.setLegends([\"Theoretical dist\", \"ML dist\", \"Bay dist\", \"Observations\"])\ng.setXTitle(r\"$X_0$\")\ng.setYTitle(r\"$X_1$\")\ng.setTitle(\"Initial distribution, ML estimated dist and Bayesian estimated dist.\")\nview = otv.View(g, (800, 800), square_axes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following figure, we consider the parameter $\\vect{\\Theta}$ and we plot:\n\n- the asymptotic distribution of the maximum likelihood estimator: $\\vect{\\theta}_n^{MV}$ (left),\n- the Bayesian mode based distribution: $g(\\pi_{\\vect{Y}}^\\sampleSize)$ (right).\n\nOn each figure, we draw the bilateral confidence interval or bilateral credibility interval of\nlevel $\\alpha = 0.95$\ncomputed from the estimator distribution.\n\nFirst the maximum likelihood estimator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "g_ML = dist_estimator_ML.drawPDF([0.5] * 2, [1.5] * 2)\nc = ot.Cloud([theta_ML[[1, 3]]])\nc.setColor(\"red\")\nc.setPointStyle(\"bullet\")\ng_ML.add(c)\nc = ot.Cloud([theta_Bay[[1, 3]]])\nc.setColor(\"red\")\nc.setPointStyle(\"+\")\ng_ML.add(c)\na = interval_ML.getLowerBound()\nb = interval_ML.getUpperBound()\nc = ot.Curve([a, [a[0], b[1]], b, [b[0], a[1]], a])\nc.setColor(\"blue\")\ng_ML.add(c)\na = interval_Bay.getLowerBound()\nb = interval_Bay.getUpperBound()\nc = ot.Curve([a, [a[0], b[1]], b, [b[0], a[1]], a])\nc.setColor(\"blue\")\nc.setLineStyle(\"dashed\")\ng_ML.add(c)\ng_ML.setLegends(\n    [\n        r\"dist of $\\mathbf{\\theta}_n^{MV}$\",\n        r\"$\\mathbf{\\theta}_n^{MV}$\",\n        r\"$g(\\mathbf{Y}_n^m)$\",\n        \"ML CI(\" + str(int(100 * alpha)) + \"%)\",\n        \"Bay CI(\" + str(int(100 * alpha)) + \"%)\",\n    ]\n)\ng_ML.setXTitle(r\"$\\sigma_0$\")\ng_ML.setYTitle(r\"$\\sigma_1$\")\ng_ML.setTitle(\"ML Estimator\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then the Bayesian estimator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "g_Bay = dist_estimateur_Bay.drawPDF([0.5] * 2, [1.5] * 2)\nc = ot.Cloud([theta_Bay[[1, 3]]])\nc.setColor(\"red\")\nc.setPointStyle(\"bullet\")\ng_Bay.add(c)\nc = ot.Cloud([theta_ML[[1, 3]]])\nc.setColor(\"red\")\nc.setPointStyle(\"+\")\ng_Bay.add(c)\na = interval_Bay.getLowerBound()\nb = interval_Bay.getUpperBound()\nc = ot.Curve([a, [a[0], b[1]], b, [b[0], a[1]], a])\nc.setColor(\"blue\")\ng_Bay.add(c)\na = interval_ML.getLowerBound()\nb = interval_ML.getUpperBound()\nc = ot.Curve([a, [a[0], b[1]], b, [b[0], a[1]], a])\nc.setColor(\"blue\")\nc.setLineStyle(\"dashed\")\ng_Bay.add(c)\ng_Bay.setLegends(\n    [\n        r\"dist of $g(\\pi_{n, \\mathbf{Y}})$\",\n        r\"$g(\\mathbf{Y}_n^m)$\",\n        r\"$\\mathbf{\\theta}_n^{MV}$\",\n        \"Bay CI(\" + str(int(100 * alpha)) + \"%)\",\n        \"ML CI(\" + str(int(100 * alpha)) + \"%)\",\n    ]\n)\ng_Bay.setXTitle(r\"$\\sigma_0$\")\ng.setYTitle(r\"$\\sigma_1$\")\ng_Bay.setTitle(\"Bayesian Estimator\")\n\n# Gather both graph in a grid.\ngrid = ot.GridLayout(1, 2)\ngrid.setGraph(0, 0, g_ML)\ngrid.setGraph(0, 1, g_Bay)\n\nview = otv.View(grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, the following table sums up the previous computed quantities.\n\n===========      ========================  ========================  =====================    =====================\nApproach         $\\tilde{\\sigma}_0$  $\\tilde{\\sigma}_1$  IC($\\sigma_0$)     IC($\\sigma_1$)\n===========      ========================  ========================  =====================    =====================\nFrequentist      0.967                     0.951                     $[0.651, 1.30]$    $[0.659, 1.20]$\nBayesian         0.950                     0.934                     $[0.736, 1.38]$    $[0.724, 1.35]$\n===========      ========================  ========================  =====================    =====================\n\n===========      =============   ======================   =================================================\nApproach         $\\beta$   $KL$               $\\cV(IC(\\tilde{\\sigma}_0, \\tilde{\\sigma}))$\n===========      =============   ======================   =================================================\nFrequentist      0.974           $3.46\\, 10^{-3}$   0.352\nBayesian         0.974           $6.87\\, 10^{-3}$   0.381\n===========      =============   ======================   =================================================\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Case 2: we consider the second link function\n\nHere, $g_2(\\vect{y}) = (0,0.5+y_0^2, 0, 0.5+y_1^2, 0)$ and $\\pi_{\\vect{Y}}^{0,2} = \\cT(-1, 0, 1) \\times \\cT(-1, 0,1)$.\n\nWe note that the posterior distribution of $\\vect{Y}$ is quadri-modal, as the link function $g_2$ is no more bijective\non the range of $\\pi_{\\vect{Y}}^{0,2}$.\n\nWe go through the same steps as described previously. The maximum estimator is not changed.\nBut in order to get the posterior distribution of :math:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "linkFunction = ot.SymbolicFunction(\n    [\"u0\", \"u1\"], [\"0.0\", \"0.5+u0^2\", \"0.0\", \"0.5+u1^2\", \"0.0\"]\n)\nconditioning = ot.JointDistribution([ot.Triangular(-1.0, 0.0, 1.0)] * 2)\nconditioning.setDescription([\"Y0\", \"Y1\"])\ncompound = ot.CompoundDistribution(conditioned, conditioning, linkFunction)\nposterior_Y = ot.PosteriorDistribution(compound, observations)\nsample_posterior = linkFunction(posterior_Y.getSample(10000)).getMarginal([1, 3])\ndist_estimateur_Bay = ot.KernelSmoothing().build(sample_posterior)\n\ntheta_Bay = linkFunction(computeMode(posterior_Y))\nprint(\"Theta Bay =\", theta_Bay)\nmodel_Bay = ot.Distribution(conditioned)\nmodel_Bay.setParameter(theta_Bay)\nalpha = 0.95\ninterval_Bay, beta = (\n    dist_estimateur_Bay.computeBilateralConfidenceIntervalWithMarginalProbability(alpha)\n)\nprint(\"Beta =\", beta)\nprint(\"Condifence interval Bay =\\n\", interval_Bay)\nprint(\"Volume =\", interval_Bay.getVolume())\nsample = model_Bay.getSample(10000)\ndist_Bay = (\n    model_Bay.computeLogPDF(sample) - conditioned.computeLogPDF(sample)\n).computeMean()\nprint(\"Kullback-Leibler distance Bay =\", dist_Bay[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "g = conditioned.drawPDF()\nlevels = g.getDrawable(0).getLevels()\ndr_ML = model_ML.drawPDF().getDrawable(0).getImplementation()\ndr_ML.setLevels(levels)\ndr_ML.setColorBarPosition(\"\")\ndr_ML.setLineStyle(\"dashed\")\ng.add(dr_ML)\ndr_Bay = model_Bay.drawPDF().getDrawable(0).getImplementation()\ndr_Bay.setLevels(levels)\ndr_Bay.setColorBarPosition(\"\")\ndr_Bay.setLineStyle(\"dotted\")\ng.add(dr_Bay)\ng.add(ot.Cloud(observations))\ng.setLegends([\"Theoretical dist\", \"ML dist\", \"Bay dist\", \"Observations\"])\ng.setXTitle(r\"$X_0$\")\ng.setYTitle(r\"$X_1$\")\ng.setTitle(\"Initial distribution, ML estimated dist and Bayesian estimated dist.\")\nview = otv.View(g, (800, 800), square_axes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "g_ML = dist_estimator_ML.drawPDF([0.5] * 2, [1.5] * 2)\nc = ot.Cloud([theta_ML[[1, 3]]])\nc.setColor(\"red\")\nc.setPointStyle(\"bullet\")\ng_ML.add(c)\nc = ot.Cloud([theta_Bay[[1, 3]]])\nc.setColor(\"red\")\nc.setPointStyle(\"+\")\ng_ML.add(c)\na = interval_ML.getLowerBound()\nb = interval_ML.getUpperBound()\nc = ot.Curve([a, [a[0], b[1]], b, [b[0], a[1]], a])\nc.setColor(\"blue\")\ng_ML.add(c)\na = interval_Bay.getLowerBound()\nb = interval_Bay.getUpperBound()\nc = ot.Curve([a, [a[0], b[1]], b, [b[0], a[1]], a])\nc.setColor(\"blue\")\nc.setLineStyle(\"dashed\")\ng_ML.add(c)\ng_ML.setLegends(\n    [\n        r\"dist of $\\mathbf{\\theta}_n^{MV}$\",\n        r\"$\\mathbf{\\theta}_n^{MV}$\",\n        r\"$g(\\mathbf{Y}_n^m)$\",\n        \"ML CI(\" + str(int(100 * alpha)) + \"%)\",\n        \"Bay CI(\" + str(int(100 * alpha)) + \"%)\",\n    ]\n)\ng_ML.setXTitle(r\"$\\sigma_0$\")\ng_ML.setYTitle(r\"$\\sigma_1$\")\ng_ML.setTitle(\"ML Estimator\")\n\n\ng_Bay = dist_estimateur_Bay.drawPDF([0.5] * 2, [1.5] * 2)\nc = ot.Cloud([theta_Bay[[1, 3]]])\nc.setColor(\"red\")\nc.setPointStyle(\"bullet\")\ng_Bay.add(c)\nc = ot.Cloud([theta_ML[[1, 3]]])\nc.setColor(\"red\")\nc.setPointStyle(\"+\")\ng_Bay.add(c)\na = interval_Bay.getLowerBound()\nb = interval_Bay.getUpperBound()\nc = ot.Curve([a, [a[0], b[1]], b, [b[0], a[1]], a])\nc.setColor(\"blue\")\ng_Bay.add(c)\na = interval_ML.getLowerBound()\nb = interval_ML.getUpperBound()\nc = ot.Curve([a, [a[0], b[1]], b, [b[0], a[1]], a])\nc.setColor(\"blue\")\nc.setLineStyle(\"dashed\")\ng_Bay.add(c)\ng_Bay.setLegends(\n    [\n        r\"dist of $g(\\pi_{n, \\mathbf{Y}})$\",\n        r\"$g(\\mathbf{Y}_n^m)$\",\n        r\"$\\mathbf{\\theta}_n^{MV}$\",\n        \"Bay CI(\" + str(int(100 * alpha)) + \"%)\",\n        \"ML CI(\" + str(int(100 * alpha)) + \"%)\",\n    ]\n)\ng_Bay.setXTitle(r\"$\\sigma_0$\")\ng.setYTitle(r\"$\\sigma_1$\")\ng_Bay.setTitle(\"Bayesian Estimator\")\n\ngrid = ot.GridLayout(1, 2)\ngrid.setGraph(0, 0, g_ML)\ngrid.setGraph(0, 1, g_Bay)\n\nview = otv.View(grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, the following table sums up the previous computed quantities.\n\n===========      ========================  ========================  =====================    =====================\nApproach         $\\tilde{\\sigma}_0$  $\\tilde{\\sigma}_1$  IC($\\sigma_0$)     IC($\\sigma_1$)\n===========      ========================  ========================  =====================    =====================\nFrequentist      0.967                     0.951                     $[0.651, 1.30]$    $[0.659, 1.20]$\nBayesian         0.929                     0.915                     $[0.706, 1.27]$    $[0.693, 1.25]$\n===========      ========================  ========================  =====================    =====================\n\n===========      =============   ======================   =================================================\nApproach         $\\beta$   $KL$               $\\cV(IC(\\tilde{\\sigma}_0, \\tilde{\\sigma}))$\n===========      =============   ======================   =================================================\nFrequentist      0.974           $3.46\\, 10^{-3}$   0.352\nBayesian         0.974           $1.21\\, 10^{-2}$   0.316\n===========      =============   ======================   =================================================\n\nWe also plot the PDF of the posterior distribution $\\pi_{\\vect{Y}}^\\sampleSize$ of $\\vect{Y}$ which is quadri-modal, with a sample.\nsphinx_gallery_thumbnail_number =  5\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "g_pinY = posterior_Y.drawPDF()\ng_pinY.setXTitle(r\"$Y_0$\")\ng_pinY.setYTitle(r\"$Y_1$\")\ng_pinY.setTitle(r\"Posterior Bayesian Distribution of $\\mathbf{Y}$\")\ng_pinY.add(ot.Cloud(posterior_Y.getSample(100)))\nview = otv.View(g_pinY, (800, 800))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "view.ShowAll()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}