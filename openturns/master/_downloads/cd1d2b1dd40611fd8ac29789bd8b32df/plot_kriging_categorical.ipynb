{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Kriging: metamodel with continuous and categorical variables\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We consider here the surrogate modeling of an analytical function characterized by\ncontinuous and categorical variables\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import openturns as ot\nimport openturns.experimental as otexp\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Seed chosen in order to obtain a visually nice plot\not.RandomGenerator.SetSeed(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first show the advantage of modeling the various levels of a mixed\ncontinuous / categorical function through a single surrogate model\non a simple test-case taken from [pelamatti2020]_, defined below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def illustrativeFunc(inp):\n    x, z = inp\n    y = np.cos(7 * x) + 0.5 * z\n    return [y]\n\n\ndim = 2\nfun = ot.PythonFunction(dim, 1, illustrativeFunc)\nnumberOfZLevels = 2  # Number of categorical levels for z\n# Input distribution\ndist = ot.JointDistribution(\n    [ot.Uniform(0, 1), ot.UserDefined(ot.Sample.BuildFromPoint(range(numberOfZLevels)))]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we compare the performances of the :class:`~openturns.experimental.LatentVariableModel`\nwith a naive approach, which would consist in modeling each combination of categorical\nvariables through a separate and independent Gaussian process.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to deal with mixed continuous / categorical problems we can rely on the\n:class:`~openturns.ProductCovarianceModel` class. We start here by defining the product kernel,\nwhich combines :class:`~openturns.SquaredExponential` kernels for the continuous variables, and\n:class:`~openturns.experimental.LatentVariableModel` for the categorical ones.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "latDim = 1  # Dimension of the latent space\nactiveCoord = 1 + latDim * (\n    numberOfZLevels - 2\n)  # Nb of active coordinates in the latent space\nkx = ot.SquaredExponential(1)\nkz = otexp.LatentVariableModel(numberOfZLevels, latDim)\nkLV = ot.ProductCovarianceModel([kx, kz])\nkLV.setNuggetFactor(1e-6)\n# Bounds for the hyperparameter optimization\nlowerBoundLV = [1e-4] * dim + [-10.0] * activeCoord\nupperBoundLV = [2.0] * dim + [10.0] * activeCoord\nboundsLV = ot.Interval(lowerBoundLV, upperBoundLV)\n# Distribution for the hyperparameters initialization\ninitDistLV = ot.DistributionCollection()\nfor i in range(len(lowerBoundLV)):\n    initDistLV.add(ot.Uniform(lowerBoundLV[i], upperBoundLV[i]))\ninitDistLV = ot.JointDistribution(initDistLV)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As a reference, we consider a purely continuous kernel for independent Gaussian processes.\nOne for each combination of categorical variables levels.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kIndependent = ot.SquaredExponential(1)\nlowerBoundInd = [1e-4]\nupperBoundInd = [20.0]\nboundsInd = ot.Interval(lowerBoundInd, upperBoundInd)\ninitDistInd = ot.DistributionCollection()\nfor i in range(len(lowerBoundInd)):\n    initDistInd.add(ot.Uniform(lowerBoundInd[i], upperBoundInd[i]))\ninitDistInd = ot.JointDistribution(initDistInd)\ninitSampleInd = initDistInd.getSample(10)\noptAlgInd = ot.MultiStart(ot.NLopt(\"LN_COBYLA\"), initSampleInd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate the training data set\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x = dist.getSample(10)\ny = fun(x)\n\n# And the plotting data set\nxPlt = dist.getSample(200)\nxPlt = xPlt.sort()\nyPlt = fun(xPlt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Initialize  and parameterize the optimization algorithm\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "initSampleLV = initDistLV.getSample(30)\noptAlgLV = ot.MultiStart(ot.NLopt(\"LN_COBYLA\"), initSampleLV)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create and train the Gaussian process models\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "basis = ot.ConstantBasisFactory(2).build()\nalgoLV = ot.KrigingAlgorithm(x, y, kLV, basis)\nalgoLV.setOptimizationAlgorithm(optAlgLV)\nalgoLV.setOptimizationBounds(boundsLV)\nalgoLV.run()\nresLV = algoLV.getResult()\n\nalgoIndependentList = []\nfor z in range(2):\n    # Select the training samples corresponding to the correct combination\n    # of categorical levels\n    ind = np.where(np.all(np.array(x[:, 1]) == z, axis=1))[0]\n    xLoc = x[ind][:, 0]\n    yLoc = y[ind]\n\n    # Create and train the Gaussian process models\n    basis = ot.ConstantBasisFactory(1).build()\n    algoIndependent = ot.KrigingAlgorithm(xLoc, yLoc, kIndependent, basis)\n    algoIndependent.setOptimizationAlgorithm(optAlgInd)\n    algoIndependent.setOptimizationBounds(boundsInd)\n    algoIndependent.run()\n    algoIndependentList.append(algoIndependent.getResult())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the prediction of the mixed continuous / categorical GP,\nas well as the one of the two separate continuous GPs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(15, 10))\nfor z in range(numberOfZLevels):\n    # Select the training samples corresponding to the correct combination\n    # of categorical levels\n    ind = np.where(np.all(np.array(x[:, 1]) == z, axis=1))[0]\n    xLoc = x[ind][:, 0]\n    yLoc = y[ind]\n\n    # Compute the models predictive performances on a validation data set.\n    # The predictions are computed independently for each level of z,\n    # i.e., by only considering the values of z corresponding to the\n    # target level.\n    ind = np.where(np.all(np.array(xPlt[:, 1]) == z, axis=1))[0]\n    xPltInd = xPlt[ind]\n    yPltInd = yPlt[ind]\n\n    predMeanLV = resLV.getConditionalMean(xPltInd)\n    predMeanInd = algoIndependentList[z].getConditionalMean(xPltInd[:, 0])\n    predSTDLV = np.sqrt(resLV.getConditionalMarginalVariance(xPltInd))\n    predSTDInd = np.sqrt(\n        algoIndependentList[z].getConditionalMarginalVariance(xPltInd[:, 0])\n    )\n\n    (trainingData,) = ax1.plot(xLoc[:, 0], yLoc, \"r*\")\n    (trueFunction,) = ax1.plot(xPltInd[:, 0], yPltInd, \"k--\")\n    (prediction,) = ax1.plot(xPltInd[:, 0], predMeanLV, \"b-\")\n    stdPred = ax1.fill_between(\n        xPltInd[:, 0].asPoint(),\n        (predMeanLV - predSTDLV).asPoint(),\n        (predMeanLV + predSTDLV).asPoint(),\n        alpha=0.5,\n        color=\"blue\",\n    )\n    ax2.plot(xLoc[:, 0], yLoc, \"r*\")\n    ax2.plot(xPltInd[:, 0], yPltInd, \"k--\")\n    ax2.plot(xPltInd[:, 0], predMeanInd, \"b-\")\n    ax2.fill_between(\n        xPltInd[:, 0].asPoint(),\n        (predMeanInd - predSTDInd).asPoint(),\n        (predMeanInd + predSTDInd).asPoint(),\n        alpha=0.5,\n        color=\"blue\",\n    )\nax1.legend(\n    [trainingData, trueFunction, prediction, stdPred],\n    [\"Training data\", \"True function\", \"Prediction\", \"Prediction standard deviation\"],\n)\nax1.set_title(\"Mixed continuous-categorical modeling\")\nax2.set_title(\"Separate modeling\")\nax2.set_xlabel(\"x\", fontsize=15)\nax1.set_ylabel(\"y\", fontsize=15)\nax2.set_ylabel(\"y\", fontsize=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It can be seen that the joint modeling of categorical and continuous variables\nimproves the overall prediction accuracy, as the Gaussian process model is\nable to exploit the information provided by the entire training data set.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now consider a more complex function which is a modified version of the Goldstein function,\ntaken from [pelamatti2020]_. This function depends on 2 continuous variables and 2 categorical ones.\nEach categorical variable is characterized by 3 levels.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def h(x1, x2, x3, x4):\n    y = (\n        53.3108\n        + 0.184901 * x1\n        - 5.02914 * x1**3 * 1e-6\n        + 7.72522 * x1**4 * 1e-8\n        - 0.0870775 * x2\n        - 0.106959 * x3\n        + 7.98772 * x3**3 * 1e-6\n        + 0.00242482 * x4\n        + 1.32851 * x4**3 * 1e-6 * 0.00146393 * x1 * x2\n        - 0.00301588 * x1 * x3\n        - 0.00272291 * x1 * x4\n        + 0.0017004 * x2 * x3\n        + 0.0038428 * x2 * x4\n        - 0.000198969 * x3 * x4\n        + 1.86025 * x1 * x2 * x3 * 1e-5\n        - 1.88719 * x1 * x2 * x4 * 1e-6\n        + 2.50923 * x1 * x3 * x4 * 1e-5\n        - 5.62199 * x2 * x3 * x4 * 1e-5\n    )\n    return y\n\n\ndef Goldstein(inp):\n    x1, x2, z1, z2 = inp\n    x1 = 100 * x1\n    x2 = 100 * x2\n\n    if z1 == 0:\n        x3 = 80\n    elif z1 == 1:\n        x3 = 20\n    elif z1 == 2:\n        x3 = 50\n    else:\n        print(\"error, no matching category z1\")\n\n    if z2 == 0:\n        x4 = 20\n    elif z2 == 1:\n        x4 = 80\n    elif z2 == 2:\n        x4 = 50\n    else:\n        print(\"error, no matching category z2\")\n\n    return [h(x1, x2, x3, x4)]\n\n\ndim = 4\nfun = ot.PythonFunction(dim, 1, Goldstein)\nnumberOfZLevels1 = 3  # Number of categorical levels for z1\nnumberOfZLevels2 = 3  # Number of categorical levels for z2\n# Input distribution\ndist = ot.JointDistribution(\n    [\n        ot.Uniform(0, 1),\n        ot.Uniform(0, 1),\n        ot.UserDefined(ot.Sample.BuildFromPoint(range(numberOfZLevels1))),\n        ot.UserDefined(ot.Sample.BuildFromPoint(range(numberOfZLevels2))),\n    ]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As in the previous example, we start here by defining the product kernel,\nwhich combines :class:`~openturns.SquaredExponential` kernels for the continuous variables, and\n:class:`~openturns.experimental.LatentVariableModel` for the categorical ones.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "latDim = 2  # Dimension of the latent space\nactiveCoord = (\n    2 + latDim * (numberOfZLevels1 - 2) + latDim * (numberOfZLevels2 - 2)\n)  # Nb ative coordinates in the latent space\nkx1 = ot.SquaredExponential(1)\nkx2 = ot.SquaredExponential(1)\nkz1 = otexp.LatentVariableModel(numberOfZLevels1, latDim)\nkz2 = otexp.LatentVariableModel(numberOfZLevels2, latDim)\nkLV = ot.ProductCovarianceModel([kx1, kx2, kz1, kz2])\nkLV.setNuggetFactor(1e-6)\n# Bounds for the hyperparameter optimization\nlowerBoundLV = [1e-4] * dim + [-10] * activeCoord\nupperBoundLV = [3.0] * dim + [10.0] * activeCoord\nboundsLV = ot.Interval(lowerBoundLV, upperBoundLV)\n# Distribution for the hyperparameters initialization\ninitDistLV = ot.DistributionCollection()\nfor i in range(len(lowerBoundLV)):\n    initDistLV.add(ot.Uniform(lowerBoundLV[i], upperBoundLV[i]))\ninitDistLV = ot.JointDistribution(initDistLV)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively, we consider a purely continuous kernel for independent Gaussian processes.\none for each combination of categorical variables levels.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kIndependent = ot.SquaredExponential(2)\nlowerBoundInd = [1e-4, 1e-4]\nupperBoundInd = [3.0, 3.0]\nboundsInd = ot.Interval(lowerBoundInd, upperBoundInd)\ninitDistInd = ot.DistributionCollection()\nfor i in range(len(lowerBoundInd)):\n    initDistInd.add(ot.Uniform(lowerBoundInd[i], upperBoundInd[i]))\ninitDistInd = ot.JointDistribution(initDistInd)\ninitSampleInd = initDistInd.getSample(10)\noptAlgInd = ot.MultiStart(ot.NLopt(\"LN_COBYLA\"), initSampleInd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to assess their respective robustness with regards to the training data set,\nwe repeat the experiments 10 times with different training of size 72,\nand compute each time the normalized prediction Root Mean Squared Error (RMSE) on a\ntest data set of size 1000.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rmseLVList = []\nrmseIndList = []\nfor rep in range(5):\n    # Generate the normalized training data set\n    x = dist.getSample(72)\n    y = fun(x)\n    yMax = y.getMax()\n    yMin = y.getMin()\n    y = (y - yMin) / (yMin - yMax)\n\n    # Initialize  and parameterize the optimization algorithm\n    initSampleLV = initDistLV.getSample(10)\n    optAlgLV = ot.MultiStart(ot.NLopt(\"LN_COBYLA\"), initSampleLV)\n\n    # Create and train the Gaussian process models\n    basis = ot.ConstantBasisFactory(dim).build()\n    algoLV = ot.KrigingAlgorithm(x, y, kLV, basis)\n    algoLV.setOptimizationAlgorithm(optAlgLV)\n    algoLV.setOptimizationBounds(boundsLV)\n    algoLV.run()\n    resLV = algoLV.getResult()\n\n    # Compute the models predictive performances on a validation data set\n    xVal = dist.getSample(1000)\n    yVal = fun(xVal)\n    yVal = (yVal - yMin) / (yMin - yMax)\n\n    valLV = ot.MetaModelValidation(xVal, yVal, resLV.getMetaModel())\n    rmseLV = valLV.getResidualSample().computeStandardDeviation()[0]\n    rmseLVList.append(rmseLV)\n\n    error = ot.Sample(0, 1)\n    for z1 in range(numberOfZLevels1):\n        for z2 in range(numberOfZLevels2):\n            # Select the training samples corresponding to the correct combination\n            # of categorical levels\n            ind = np.where(np.all(np.array(x[:, 2:]) == [z1, z2], axis=1))[0]\n            xLoc = x[ind][:, :2]\n            yLoc = y[ind]\n\n            # Create and train the Gaussian process models\n            basis = ot.ConstantBasisFactory(2).build()\n            algoIndependent = ot.KrigingAlgorithm(xLoc, yLoc, kIndependent, basis)\n            algoIndependent.setOptimizationAlgorithm(optAlgInd)\n            algoIndependent.setOptimizationBounds(boundsInd)\n            algoIndependent.run()\n            resInd = algoIndependent.getResult()\n\n            # Compute the models predictive performances on a validation data set\n            ind = np.where(np.all(np.array(xVal[:, 2:]) == [z1, z2], axis=1))[0]\n            xValInd = xVal[ind][:, :2]\n            yValInd = yVal[ind]\n            valInd = ot.MetaModelValidation(xValInd, yValInd, resInd.getMetaModel())\n            error.add(valInd.getResidualSample())\n    rmseInd = error.computeStandardDeviation()[0]\n    rmseIndList.append(rmseInd)\n\nplt.figure()\nplt.boxplot([rmseLVList, rmseIndList])\nplt.xticks([1, 2], [\"Mixed continuous-categorical GP\", \"Independent GPs\"])\nplt.ylabel(\"RMSE\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The obtained results show, for this test-case, a better modeling performance\nwhen modeling the function as a mixed categorical/continuous function, rather\nthan relying on multiple purely continuous Gaussian processes.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}