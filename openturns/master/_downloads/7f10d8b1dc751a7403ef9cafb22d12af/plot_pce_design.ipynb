{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Compute leave-one-out error of a polynomial chaos expansion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n\nIn this example, we compute the design matrix of a polynomial chaos\nexpansion using the :class:`~openturns.DesignProxy` class.\nThen we compute the analytical leave-one-out error using the\ndiagonal of the projection matrix.\nTo do this, we use equations from [blatman2009]_ page 85\n(see also [blatman2011]_).\nIn this advanced example, we use the :class:`~openturns.DesignProxy`\nand :class:`~openturns.QRMethod` low level classes.\nA naive implementation of this method is presented in\n:doc:`/auto_meta_modeling/polynomial_chaos_metamodel/plot_chaos_cv`\nusing K-Fold cross-validation.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The design matrix\nIn this section, we analyze why the :class:`~openturns.DesignProxy`\nis linked to the classical linear least squares regression problem.\nLet $n$ be the number of observations and $m$ be the number of coefficients\nof the linear model.\nLet $D \\in \\mathbb{R}^{n \\times m}$ be the design matrix, i.e.\nthe matrix that produces the predictions of the linear regression model from\nthe coefficients:\n\n\\begin{align}\\hat{\\vect{y}} = D \\vect{a}\\end{align}\n\nwhere $\\vect{a} \\in \\mathbb{R}^m$ is the vector of coefficients,\n$\\hat{y} \\in \\mathbb{R}^n$ is the\nvector of predictions.\nThe linear least squares problem is:\n\n\\begin{align}\\operatorname{argmin}_{\\vect{a} \\in \\mathbb{R}^m}\n    \\left\\| D \\vect{a} - \\vect{y} \\right\\|_2^2.\\end{align}\n\nThe solution is given by the normal equations, i.e. the vector of coefficients\nis the solution of the following linear system of equations:\n\n\\begin{align}G \\vect{a} = D^T \\vect{y}\\end{align}\n\nwhere $G \\in \\Rset^{n \\times n}$ is the *Gram* matrix:\n\n\\begin{align}G := D^T D.\\end{align}\n\nThe hat matrix is the projection matrix defined by:\n\n\\begin{align}H := D \\left(D^T D\\right)^{-1} D^T.\\end{align}\n\nThe hat matrix puts a hat to the vector of observations to\nproduce the vector of predictions of the linear model:\n\n\\begin{align}\\hat{\\vect{y}} = H \\vect{y}\\end{align}\n\nTo solve a linear least squares problem, we need to evaluate the\ndesign matrix $D$, which is the primary goal of\nthe :class:`~openturns.DesignProxy` class.\nLet us present some examples of situations where the design matrix\nis required.\n\n- When we use the QR decomposition, we actually do not need to evaluate it in\n  our script: the :class:`~openturns.QRMethod` class knows how to compute the\n  solution without evaluating the Gram matrix $D^T D$.\n- We may need the inverse Gram matrix\n  $\\left(D^T D\\right)^{-1}$ sometimes, for example when we want to create\n  a D-optimal design.\n- Finally, when we want to compute the analytical leave-one-out error,\n  we need to compute the diagonal of the  projection matrix $H$.\n\nFor all these purposes, the `DesignProxy` is *the* tool.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The leave-one-out error\nIn this section, we show that the leave-one-error of a regression problem\ncan be computed using an analytical formula which depends on the hat matrix\n$H$.\nWe consider the physical model:\n\n\\begin{align}y = g(\\vect{x})\\end{align}\n\nwhere $\\vect{x} \\in \\Rset^{n_X}$ is the input and $y \\in \\Rset$ is\nthe output.\nConsider the problem of approximating the physical model $g$ by the\nlinear model:\n\n\\begin{align}\\hat{y} := \\tilde{g}(\\vect{x}) = \\sum_{k = 1}^m a_k \\psi_k(\\vect{x})\\end{align}\n\nfor any $\\vect{x} \\in \\Rset^{n_X}$ where $\\{\\psi_k : \\Rset^{n_X} \\rightarrow \\Rset\\}_{k = 1, \\ldots, m}$ are the basis functions and\n$\\vect{a} \\in \\Rset^m$ is a vector of parameters.\nThe mean squared error is ([blatman2009]_ eq. 4.23 page 83):\n\n\\begin{align}\\operatorname{MSE}\\left(\\tilde{g}\\right)\n    = \\mathbb{E}_{\\vect{X}}\\left[\\left(g\\left(\\vect{X}\\right) - \\tilde{g}\\left(\\vect{X}\\right) \\right)^2 \\right]\\end{align}\n\nThe leave-one-out error is an estimator of the mean squared error.\nLet:\n\n\\begin{align}\\cD = \\{\\vect{x}^{(1)}, \\ldots, \\vect{x}^{(n)} \\in \\Rset^{n_X}\\}\\end{align}\n\nbe independent observations of the input random vector $\\vect{X}$ and\nlet $\\{y^{(1)}, \\ldots, y^{(n)} \\in \\Rset^{n_X}\\}$ be the corresponding\nobservations of the output of the physical model:\n\n\\begin{align}y^{(j)} = g\\left(\\vect{x}^{(j)}\\right)\\end{align}\n\nfor $j = 1, ..., n$.\nLet $\\vect{y} \\in \\Rset^n$ be the vector of observations:\n\n\\begin{align}\\vect{y} = (y^{(1)}, \\ldots, y^{(n)})^T.\\end{align}\n\n\nConsider the following set of inputs, let aside the $j$-th input:\n\n\\begin{align}\\cD^{(-j)} := \\left\\{\\vect{x}^{(1)}, \\ldots, \\vect{x}^{(j - 1)}, \\vect{x}^{(j + 1)}, \\ldots, \\vect{x}^{(n)}\\right\\}\\end{align}\n\nfor $j \\in \\{1, ..., n\\}$.\nLet $\\vect{y}^{(-j)} \\in \\Rset^{n - 1}$ be the vector of\nobservations, let aside the $j$-th observation:\n\n\\begin{align}\\vect{y}^{(-j)} = (y^{(1)}, \\ldots, y^{(j - 1)}, y^{(j + 1)}, \\ldots, y^{(n)})^T\\end{align}\n\nfor $j \\in \\{1, ..., n\\}$.\nLet $\\tilde{g}^{(-j)}$ the metamodel built on the data set $\\left(\\cD^{(-j)}, \\vect{y}^{(-j)}\\right)$.\nThe leave-one-out error is:\n\n\\begin{align}\\widehat{\\operatorname{MSE}}_{LOO}\\left(\\tilde{g}\\right)\n   = \\frac{1}{n} \\sum_{j = 1}^n \\left(g\\left(\\vect{x}^{(j)}\\right) - \\tilde{g}^{(-j)}\\left(\\vect{x}^{(j)}\\right)\\right)^2\\end{align}\n\nThe leave-one-out error is sometimes referred to as *predicted residual sum of\nsquares* (PRESS) or *jacknife error*.\nIn the next section, we show how this estimator can be computed analytically,\nusing the hat matrix.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The analytical leave-one-out error\nOne limitation of the previous equation is that we must train\n$n$ different surrogate models, which can be long in some situations.\nTo overcome this limitation, we can use the following equations.\nLet $\\boldsymbol{\\Psi} \\in \\Rset^{n \\times m}$ design matrix ([blatman2009]_ eq. 4.32 page 85):\n\n\\begin{align}\\boldsymbol{\\Psi}_{ik} = \\psi_k\\left(\\vect{x}^{(j)}\\right)\\end{align}\n\nfor $j = 1, ..., n$ and $k = 1, ..., m$.\nThe matrix $\\boldsymbol{\\Psi}$ is mathematically equal to the\n$D$ matrix presented earlier in the present document.\nLet $H \\in \\Rset^{n \\times n}$ be the projection matrix:\n\n\\begin{align}H = \\boldsymbol{\\Psi} \\left(\\boldsymbol{\\Psi}^T \\boldsymbol{\\Psi}\\right) \\boldsymbol{\\Psi}^T.\\end{align}\n\nIt can be proved that  ([blatman2009]_ eq. 4.33 page 85):\n\n\\begin{align}\\widehat{\\operatorname{MSE}}_{LOO}\\left(\\tilde{g}\\right)\n   = \\frac{1}{n} \\sum_{j = 1}^n \\left(\\frac{g\\left(\\vect{x}^{(j)}\\right) - \\tilde{g}\\left(\\vect{x}^{(j)}\\right)}{1 - h_{jj}}\\right)^2\\end{align}\n\nwhere $h_{jj} \\in \\Rset$ is the diagonal of the hat matrix\nfor $j \\in \\{1, ..., n\\}$.\nThe goal of this example is to show how to implement the previous equation\nusing the :class:`~openturns.DesignProxy` class.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import openturns as ot\nimport openturns.viewer as otv\nimport numpy as np\nfrom openturns.usecases import ishigami_function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the polynomial chaos model\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We load the Ishigami model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "im = ishigami_function.IshigamiModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a training sample.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nTrain = 100\nxTrain = im.distributionX.getSample(nTrain)\nyTrain = im.model(xTrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the chaos.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def ComputeSparseLeastSquaresFunctionalChaos(\n    inputTrain,\n    outputTrain,\n    multivariateBasis,\n    basisSize,\n    distribution,\n    sparse=True,\n):\n    if sparse:\n        selectionAlgorithm = ot.LeastSquaresMetaModelSelectionFactory()\n    else:\n        selectionAlgorithm = ot.PenalizedLeastSquaresAlgorithmFactory()\n    projectionStrategy = ot.LeastSquaresStrategy(\n        inputTrain, outputTrain, selectionAlgorithm\n    )\n    adaptiveStrategy = ot.FixedStrategy(multivariateBasis, basisSize)\n    chaosAlgorithm = ot.FunctionalChaosAlgorithm(\n        inputTrain, outputTrain, distribution, adaptiveStrategy, projectionStrategy\n    )\n    chaosAlgorithm.run()\n    chaosResult = chaosAlgorithm.getResult()\n    return chaosResult"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "multivariateBasis = ot.OrthogonalProductPolynomialFactory([im.X1, im.X2, im.X3])\ntotalDegree = 5\nenumerateFunction = multivariateBasis.getEnumerateFunction()\nbasisSize = enumerateFunction.getBasisSizeFromTotalDegree(totalDegree)\nprint(\"Basis size = \", basisSize)\n\nsparse = False  # For full PCE and comparison with analytical LOO error\nchaosResult = ComputeSparseLeastSquaresFunctionalChaos(\n    xTrain,\n    yTrain,\n    multivariateBasis,\n    basisSize,\n    im.distributionX,\n    sparse,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The DesignProxy\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The :class:`~openturns.DesignProxy` class provides methods used to create the objects necessary to solve\nthe least squares problem.\nMore precisely, it provides the :meth:`~openturns.DesignProxy.computeDesign`\nmethod that we need to evaluate the design matrix.\nIn many cases we do not need that matrix, but the Gram matrix (or its inverse).\nThe :class:`~openturns.DesignProxy` class is needed by a least squares solver,\ne.g. :class:`~openturns.QRMethod` that knows how to actually compute the coefficients.\n\nAnother class is the :class:`~openturns.Basis` class which manages a set of\nfunctions as the functional basis for the decomposition.\nThis basis is required by the constructor of the :class:`~openturns.DesignProxy` because it defines\nthe columns of the matrix.\n\nIn order to create that basis, we use the :meth:`~openturns.FunctionalChaosResult.getReducedBasis` method,\nbecause the model selection (such as :class:`~openturns.LARS` for example)\nmay have selected functions which best predict the output.\nThis may reduce the number of coefficients to estimate and\nimprove their accuracy.\nThis is important here, because it defines the number of\ncolumns in the design matrix.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reducedBasis = chaosResult.getReducedBasis()  # As a result of the model selection\ntransformation = (\n    chaosResult.getTransformation()\n)  # As a result of the input distribution\nzTrain = transformation(\n    xTrain\n)  # Map from the physical input into the transformed input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now create the design.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "designProxy = ot.DesignProxy(zTrain, reducedBasis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To actually evaluate the design matrix, we\ncan specify the columns that we need to evaluate.\nThis can be useful when we perform model selection, because\nnot all columns are always needed.\nThis can lead to CPU and memory savings.\nIn our case, we evaluate all the columns, which corresponds\nto evaluate all the functions in the basis.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reducedBasisSize = reducedBasis.getSize()\nprint(\"Reduced basis size = \", reducedBasisSize)\nallIndices = range(reducedBasisSize)\ndesignMatrix = designProxy.computeDesign(allIndices)\nprint(\"Design matrix : \", designMatrix.getNbRows(), \" x \", designMatrix.getNbColumns())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Solve the least squares problem.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lsqMethod = ot.QRMethod(designProxy, allIndices)\nbetaHat = lsqMethod.solve(yTrain.asPoint())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute the inverse of the Gram matrix.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inverseGram = lsqMethod.getGramInverse()\nprint(\"Inverse Gram : \", inverseGram.getNbRows(), \"x\", inverseGram.getNbColumns())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute the raw leave-one-out error\nIn this section, we show how to compute the raw leave-one-out\nerror using the naive formula.\nTo do this, we could use implement the :class:~openturns.KFoldSplitter` class\nwith `K = N`.\nSince this would complicate the script and obscure its purpose,\nwe implement the leave-one-out method naively.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute leave-one-out error\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predictionsLOO = ot.Sample(nTrain, 1)\nresiduals = ot.Point(nTrain)\nfor j in range(nTrain):\n    indicesLOO = list(range(nTrain))\n    indicesLOO.pop(j)\n    xTrainLOO = xTrain[indicesLOO]\n    yTrainLOO = yTrain[indicesLOO]\n    xj = xTrain[j]\n    yj = yTrain[j]\n\n    chaosResultLOO = ComputeSparseLeastSquaresFunctionalChaos(\n        xTrainLOO,\n        yTrainLOO,\n        multivariateBasis,\n        basisSize,\n        im.distributionX,\n        sparse,\n    )\n    metamodelLOO = chaosResultLOO.getMetaModel()\n    predictionsLOO[j] = metamodelLOO(xj)\n    residuals[j] = (yj - predictionsLOO[j])[0]\nmseLOO = residuals.normSquare() / nTrain\nprint(\"mseLOO = \", mseLOO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each point in the training sample, we plot the predicted leave-one-out\noutput prediction depending on the observed output.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = ot.Graph(\"Leave-one-out validation\", \"Observation\", \"LOO prediction\", True)\ncloud = ot.Cloud(yTrain, predictionsLOO)\ngraph.add(cloud)\ncurve = ot.Curve(yTrain, yTrain)\ngraph.add(curve)\nview = otv.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the previous method, we must pay attention to the fact that\nthe comparison that we are going to make is not necessarily\nvalid if we use the :class:`~openturns.LARS` selection method,\nbecause this may lead to a different active basis for each leave-one-out\nsample.\n\nOne limitation of the previous script is that it can be relatively\nlong when the sample size increases or when the size of the\nfunctional basis increases.\nIn the next section, we use the analytical formula: this can leads\nto significant time savings in some cases.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute the analytical leave-one-out error\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the diagonal of the projection matrix.\nThis is a :class:`~openturns.Point`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "diagonalH = lsqMethod.getHDiag()\nprint(\"diagonalH : \", diagonalH.getDimension())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute the metamodel predictions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "metamodel = chaosResult.getMetaModel()\nyHat = metamodel(xTrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute the residuals.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "residuals = yTrain.asPoint() - yHat.asPoint()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute the analytical leave-one-out error:\nperform elementwise division and exponentiation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "delta = np.array(residuals) / (1.0 - np.array(diagonalH))\nsquaredDelta = delta**2\nleaveOneOutMSE = ot.Sample.BuildFromPoint(squaredDelta).computeMean()[0]\nprint(\"MSE LOO = \", leaveOneOutMSE)\nrelativeLOOError = leaveOneOutMSE / yTrain.computeVariance()[0]\nq2LeaveOneOut = 1.0 - relativeLOOError\nprint(\"Q2 LOO = \", q2LeaveOneOut)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the MSE leave-one-out error is equal to the naive LOO error.\nThe numerical differences between the two values are the consequences\nof the rounding errors in the numerical evaluation of the hat matrix.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "otv.View.ShowAll()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}