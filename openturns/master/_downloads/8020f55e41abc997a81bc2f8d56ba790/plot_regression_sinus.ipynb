{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Compute confidence intervals of a univariate noisy function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n\nIn this example, we compute the pointwise confidence interval of the\nestimator of the conditional expectation given an input.\nWe consider noisy observations of the sine function.\nThen we perform linear least squares regression to fit an order 4\npolynomial.\nFor a given point x, the code computes the confidence interval\nof the prediction y.\nThis is the confidence interval of the conditional expectation\ngiven the input.\nSecondly, we compute the confidence interval of the noisy output observations.\nIn this advanced example, we use the :class:`~openturns.QRMethod` low level class.\nAnother example of this method is presented in\n:doc:`/auto_numerical_methods/general_methods/plot_regression_interval`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import openturns as ot\nimport openturns.viewer as otv\nimport numpy as np\n\n\npalette = ot.Drawable.BuildTableauPalette(5)\n\not.RandomGenerator.SetSeed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute the data\n\nWe generate noisy observations from the sine function.\nWe define the function that we are going to approximate.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "g = ot.SymbolicFunction([\"x\"], [\"sin(2 * pi_ * x)\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We plot the function depending on the input.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plotFunction(g, color, lineStyle=\"dotted\"):\n    curve = g.draw(0.0, 1.0).getDrawable(0)\n    curve.setColor(color)\n    curve.setLineStyle(\"dotted\")\n    curve.setLegend(\"True\")\n    return curve\n\n\ngraph = ot.Graph(\"Polynomial curve fitting\", \"x\", \"y\", True, \"upper right\")\n# The \"unknown\" function\ngraph.add(plotFunction(g, palette[0]))\nview = otv.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a nice, smooth function to approximate with polynomials.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def linearSample(xmin, xmax, npoints):\n    \"\"\"Returns a sample created from a regular grid\n    from xmin to xmax with npoints points.\"\"\"\n    step = (xmax - xmin) / (npoints - 1)\n    rg = ot.RegularGrid(xmin, step, npoints)\n    vertices = rg.getVertices()\n    return vertices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We consider observation points in the interval [0,1].\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nTrain = 100\nxTrain = linearSample(0, 1, nTrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Assume that the observations are noisy and that the noise follows\na Normal distribution with zero mean and small standard deviation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "noise = ot.Normal(0, 0.5)\nnoiseSample = noise.getSample(nTrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code computes the observation as the sum of the\nfunction value and of the noise.\nThe couple `(xTrain,yTrain)` is the training set: it is used\nto compute the coefficients of the polynomial model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "yTrain = g(xTrain) + noiseSample\nprint(yTrain[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we plot the function and the observations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plotData(xTrain, yTrain, color, pointStyle=\"circle\"):\n    cloud = ot.Cloud(xTrain, yTrain)\n    cloud.setPointStyle(pointStyle)\n    cloud.setLegend(\"Observations\")\n    cloud.setColor(palette[1])\n    return cloud\n\n\ngraph = ot.Graph(\"Polynomial curve fitting\", \"x\", \"y\", True, \"upper right\")\n# The \"unknown\" function\ngraph.add(plotFunction(g, palette[0]))\n# Training set\ngraph.add(plotData(xTrain, yTrain, palette[1]))\nview = otv.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the noisy observations of the function are relatively\nlarge compared to the function values.\nIt may not be obvious that a regression model can fit that data well.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute the coefficients of the polynomial decomposition\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to approximate the function with polynomials up to degree 4,\nwe create a list of strings containing the associated monomials.\nWe perform the loop from 0 up to `totalDegree` (but the `range`\nfunction takes `totalDegree + 1` as its second input argument).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "totalDegree = 4\npolynomialCollection = [f\"x^{degree}\" for degree in range(0, totalDegree + 1)]\nprint(polynomialCollection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given the list of strings, we create a symbolic function which computes the\nvalues of the monomials.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "basisFunction = ot.SymbolicFunction([\"x\"], polynomialCollection)\nprint(basisFunction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate the design matrix as the value of the basis functions on the\ntraining sample.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "designSampleTrain = basisFunction(xTrain)\nprint(designSampleTrain[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convert the design sample into a design matrix and create\nan instance of the :class:`~openturns.QRMethod` class.\nThis class has the :meth:`~openturns.QRMethod.getGramInverse` method that\nwe need to compute the confidence interval.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "designMatrixTrain = ot.Matrix(designSampleTrain)\nlsqMethod = ot.QRMethod(designMatrixTrain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Solve the linear least squares problem and get the vector of coefficients.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "betaHat = lsqMethod.solve(yTrain.asPoint())\nprint(betaHat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute residuals and variance\n\nWe need to estimate the variance of the residuals.\nTo do this we evaluate the predictions of the regression model on\nthe training sample and compute the residuals.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "yHatTrain = designMatrixTrain * betaHat\nresiduals = yHatTrain - yTrain.asPoint()\nsampleSize = designMatrixTrain.getNbRows()\nprint(\"sampleSize=\", sampleSize)\nnParameters = designMatrixTrain.getNbColumns()\nprint(\"nParameters = \", nParameters)\nsigma2Hat = residuals.normSquare() / (sampleSize - nParameters)\nprint(\"sigma2Hat = \", sigma2Hat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The couple `(xTest,yHatTest)` is the set where we want to evaluate the\nprediction confidence intervals.\nIn order to evaluate the predictions from the regression model, multiply\nthe design matrix evaluated on the test sample with the vector of coefficients.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nTest = 50\nxTest = linearSample(0, 1, nTest)\ndesignMatrixTest = ot.Matrix(basisFunction(xTest))\nyHatTest = ot.Sample.BuildFromPoint(designMatrixTest * betaHat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we plot the true function, its noisy observations and the least\nsquares model of degree 4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plotPredictions(xTest, yHatTest, totalDegree, color):\n    curve = ot.Curve(xTest, yHatTest)\n    curve.setLegend(f\"L.S. degree {totalDegree}\")\n    curve.setColor(color)\n    return curve\n\n\ngraph = ot.Graph(\"Polynomial curve fitting\", \"x\", \"y\", True, \"upper right\")\n# The \"unknown\" function\ngraph.add(plotFunction(g, palette[0]))\n# Training set\ngraph.add(plotData(xTrain, yTrain, palette[1]))\n# Predictions\ngraph.add(plotPredictions(xTest, yHatTest, totalDegree, palette[2]))\nview = otv.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the least squares polynomial model\nis relatively close to the true function.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute confidence intervals\n\nThe next function will help to compute confidence intervals.\nIt is based on regression analysis.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def computeRegressionConfidenceInterval(\n    lsqMethod,\n    betaHat,\n    sigma2Hat,\n    designSample,\n    alpha=0.95,\n    mean=True,\n    epsilonSigma=1.0e-5,\n):\n    \"\"\"\n    Compute a confidence interval for the estimate of the mean.\n\n    Evaluates this confidence interval at points in the design matrix.\n\n    This code is based on (Rawlings, Pantula & David, 1998)\n    eq. 3.51 and 3.52 page 90.\n\n    Parameters\n    ----------\n    lsqMethod: ot.LeastSquaresMethod\n        The linear least squares method (e.g. QR or Cholesky).\n    betaHat : ot.Point(parameterDimension)\n        The solution of the least squares problem.\n    sigma2Hat : float > 0.0\n        The estimate of the variance.\n    designSample : ot.Sample(size, parameterDimension)\n        The design matrix of the linear model.\n        This is the value of the functional basis depending on the\n        input sample.\n        Each row represents the input where the confidence interval\n        is to be computed.\n    alpha : float, in [0, 1]\n        The width of the confidence interval.\n    mean : bool\n        If True, then computes the confidence interval of the mean.\n        This interval contains yTrue = E[y|x] with probability alpha.\n        Otherwise, computes a confidence interval of the prediction at point x.\n        This interval contains y|x with probability alpha.\n    epsilonSigma : float > 0.0\n        A relatively small number. The minimal value of variance, which\n        avoids a singular Normal distribution.\n\n    Reference\n    ---------\n    - O. Rawlings John, G. Pantula Sastry, and A. Dickey David.\n      Applied regression analysis\u2014a research tool. Springer New York, 1998.\n\n    Returns\n    -------\n    confidenceBounds : ot.Sample(size, 2)\n        The first column contains the lower bound.\n        The second column contains the upper bound.\n    \"\"\"\n\n    inverseGram = lsqMethod.getGramInverse()\n    sampleSize = designSample.getSize()\n    confidenceBounds = ot.Sample(sampleSize, 2)\n    for i in range(sampleSize):\n        x0 = designSample[i, :]\n        meanYHat = x0.dot(betaHat)\n        sigma2YHat = x0.dot(inverseGram * x0) * sigma2Hat\n        if not mean:\n            sigma2YHat += sigma2Hat\n        sigmaYHat = np.sqrt(sigma2YHat)\n        sigmaYHat = max(epsilonSigma, sigmaYHat)  # Prevents a zero s.e.\n        distribution = ot.Normal(meanYHat, sigmaYHat)\n        interval = distribution.computeBilateralConfidenceInterval(alpha)\n        lower = interval.getLowerBound()\n        upper = interval.getUpperBound()\n        confidenceBounds[i, 0] = lower[0]\n        confidenceBounds[i, 1] = upper[0]\n    return confidenceBounds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We evaluate the value of the basis functions on the test sample.\nThis sample is used to compute the confidence interval.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "designSampleTest = basisFunction(xTest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute the confidence interval.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "alpha = 0.95\nconfidenceIntervalMean = computeRegressionConfidenceInterval(\n    lsqMethod, betaHat, sigma2Hat, designSampleTest, alpha=alpha\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On output, the `confidenceIntervalMean` variable is a :class:`~openturns.Sample`\nof size 50 and dimension 2.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(confidenceIntervalMean.getSize())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the confidence interval (C.I.) of the pointwise estimator\nof the conditional expectation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plotConfidenceInterval(\n    xTest, confidenceIntervalSample, color, lineStyle=\"dashed\", label=\"\"\n):\n    graph = ot.Graph()\n    curve = ot.Curve(xTest, confidenceIntervalSample[:, 0])\n    curve.setLegend(label)\n    curve.setColor(color)\n    curve.setLineStyle(lineStyle)\n    graph.add(curve)\n    curve = ot.Curve(xTest, confidenceIntervalSample[:, 1])\n    curve.setLegend(\"\")\n    curve.setColor(color)\n    curve.setLineStyle(lineStyle)\n    graph.add(curve)\n    return graph\n\n\ngraph = ot.Graph(\"Polynomial curve fitting\", \"x\", \"y\", True, \"upper right\")\n# The \"unknown\" function\ngraph.add(plotFunction(g, palette[0]))\n# Training set\ngraph.add(plotData(xTrain, yTrain, palette[1]))\n# Predictions\ngraph.add(plotPredictions(xTest, yHatTest, totalDegree, palette[2]))\n# Confidence interval of the mean\ngraph.add(\n    plotConfidenceInterval(\n        xTest,\n        confidenceIntervalMean,\n        palette[3],\n        label=\"Mean %.0f%%\" % (100.0 * alpha),\n    )\n)\nview = otv.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the pointwise confidence interval contains the true\nmodel for most points.\nFor a small set of points, there are points which are not within\nthe bounds, but are not too far away.\nThe observations, however, are not contained within these bounds.\nThis is the goal of the next cell.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, compute a 95% C.I. of the observations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "alpha = 0.95\nconfidenceIntervalObservations = computeRegressionConfidenceInterval(\n    lsqMethod,\n    betaHat,\n    sigma2Hat,\n    designSampleTest,\n    alpha=alpha,\n    mean=False,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we plot the function, its least squares approximation, the\nC.I. of the mean and the C.I. of the observations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_number = 5\ngraph = ot.Graph(\"Polynomial curve fitting\", \"x\", \"y\", True, \"upper right\")\n# The \"unknown\" function\ngraph.add(plotFunction(g, palette[0]))\n# Training set\ngraph.add(plotData(xTrain, yTrain, palette[1]))\n# Predictions\ngraph.add(plotPredictions(xTest, yHatTest, totalDegree, palette[2]))\n# Confidence interval of the mean\ngraph.add(\n    plotConfidenceInterval(\n        xTest,\n        confidenceIntervalMean,\n        palette[3],\n        label=\"Mean %.0f%%\" % (100.0 * alpha),\n    )\n)\n# Confidence interval of the observations.\ngraph.add(\n    plotConfidenceInterval(\n        xTest,\n        confidenceIntervalObservations,\n        palette[4],\n        label=\"Obs. %.0f%%\" % (100.0 * alpha),\n    )\n)\nview = otv.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the confidence interval of the observations contain\nmost of the observations.\nThe confidence interval of the observations is much larger than the\nC.I. of the mean, as expected from the statistical model.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}