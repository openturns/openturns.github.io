{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Conditional expectation of a polynomial chaos expansion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we compute the conditional expectation of a polynomial\nchaos expansion of the `Ishigami function <use-case-ishigami>` using\nthe :meth:`~openturns.FunctionalChaosResult.getConditionalExpectation`\nmethod.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\nLet $\\inputDim \\in \\Nset$\nbe the dimension of the input random vector.\nLet $\\Expect{\\inputRV} \\in \\Rset^\\inputDim$\nbe the mean of the input random vector $\\inputRV$.\nLet $\\model$ be the physical model:\n\n\\begin{align}\\model : \\Rset^\\inputDim \\rightarrow \\Rset.\\end{align}\n\nGiven $\\vect{u} \\subseteq \\{1, ..., \\inputDim\\}$ a group\nof input variables, we want to create a new function $\\widehat{\\model}$:\n\n\\begin{align}\\widehat{\\model}: \\Rset^{|\\vect{u}|} \\rightarrow \\Rset\\end{align}\n\nwhere $|\\vect{u}| = \\operatorname{card}(\\vect{u})$ is the\nnumber of variables in the group.\n\nIn this example, we experiment two different ways to reduce the\ninput dimension of a polynomial chaos expansion:\n\n- the parametric function,\n- the conditional expectation.\n\nThe goal of this page is to see how we can create these\nfunctions and the difference between them.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parametric function\n\nThe simplest method to reduce the dimension of the input\nis to set some input variables to constant values.\nIn this example, all marginal inputs, except those in\nthe conditioning indices are set to the mean of the input random vector.\n\nLet $\\overline{\\vect{u}}$ be the complementary set of input\nmarginal indices such that $\\vect{u}$ and $\\overline{\\vect{u}}$\nform a disjoint partition of the full set of variable indices:\n\n\\begin{align}\\vect{u} \\; \\dot{\\cup} \\; \\overline{\\vect{u}} = \\{1, ..., \\inputDim\\}.\\end{align}\n\nThe parametric function with reduced dimension is:\n\n\\begin{align}\\widehat{\\model}(\\inputReal_{\\vect{u}})\n  = \\model\\left(\\inputReal_{\\vect{u}},\n           \\inputReal_{\\overline{\\vect{u}}}\n           = \\Expect{\\inputRV_{\\overline{\\vect{u}}}}\\right)\\end{align}\n\nfor any $\\inputReal_{\\vect{u}} \\in \\Rset^{|\\vect{u}|}$.\nThe previous function is a parametric function based on the function $\\model$\nwhere the parameter is $\\Expect{\\inputRV_{\\overline{\\vect{u}}}}$.\nAssuming that the input random vector has an independent copula,\ncomputing $\\Expect{\\inputRV_{\\overline{\\vect{u}}}}$\ncan be done by selecting the corresponding indices in $\\Expect{\\inputRV}$.\nThis function can be created using the :class:`~openturns.ParametricFunction`\nclass.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parametric PCE\n\nIf the physical model is a PCE, then the associated parametric model is also a\nPCE.\nIts coefficients and the associated functional basis can be computed from\nthe original PCE.\nA significant fact, however, is that the coefficients of the parametric\nPCE are *not* the ones of the original PCE: the coefficients of the parametric\nPCE have to be multiplied by factors which depend on the\nvalue of the discarded basis functions on the parameter vector.\nThis feature is not currently available in the library.\nHowever, we present it below as this derivation is interesting\nto understand why the conditional expectation may behave\ndifferently from the corresponding parametric PCE.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let $\\cJ^P \\subseteq \\Nset^{\\inputDim}$ be the set of\nmulti-indices corresponding to the truncated polynomial chaos expansion\nup to the $P$-th coefficient.\nLet $h$ be the PCE in the standard space:\n\n\\begin{align}h(\\standardReal) = \\sum_{\\vect{\\alpha} \\in \\cJ^P}\n        a_{\\vect{\\alpha}} \\psi_{\\vect{\\alpha}}(\\standardReal).\\end{align}\n\nLet $\\vect{u} \\subseteq \\{1, ..., \\inputDim\\}$ be a group of variables,\nlet $\\overline{\\vect{u}}$ be its complementary set such that\n\n\\begin{align}\\vect{u} \\; \\dot{\\cup} \\; \\overline{\\vect{u}} = \\{1, ..., \\inputDim\\}\\end{align}\n\ni.e. the groups $\\vect{u}$ and $\\overline{\\vect{u}}$ create a disjoint partition\nof the set $\\{1, ..., \\inputDim\\}$.\nLet $|\\vect{u}| \\in \\Nset$ be the number of elements\nin the group $\\vect{u}$.\nHence, we have $|\\vect{u}| + |\\overline{\\vect{u}}| = \\inputDim$.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let $\\standardReal_{\\vect{u}}^{(0)} \\in \\Rset^{|\\vect{u}|}$\nbe a given point.\nWe are interested in the function :\n\n\\begin{align}\\widehat{h}(\\standardReal_{\\overline{\\vect{u}}})\n    = h\\left(\\standardReal_{\\overline{\\vect{u}}},\n    \\standardReal_{\\vect{u}}^{(0)}\\right)\\end{align}\n\nfor any $\\standardReal_{\\overline{\\vect{u}}} \\in \\Rset^{|\\overline{\\vect{u}}|}$.\nWe assume that the polynomial basis are defined by the tensor product:\n\n\\begin{align}\\psi_{\\vect{\\alpha}}\\left(\\standardReal\\right)\n    = \\prod_{i = 1}^{\\inputDim}\n    \\pi_{\\alpha_i}^{(i)}\\left(\\standardReal\\right)\\end{align}\n\nfor any $\\standardReal \\in \\standardInputSpace$\nwhere $\\pi_{\\alpha_i}^{(i)}$ is the polynomial of degree\n$\\alpha_i$ of the $i$-th input standard variable.\n\nLet $\\vect{u} = (u_i)_{i = 1, ..., |\\vect{u}|}$ denote the components of the\ngroup $\\vect{u}$ where $|\\vect{u}|$ is the number of elements in the group.\nSimilarly, let $\\overline{\\vect{u}} = (\\overline{u}_i)_{i = 1, ..., |\\overline{\\vect{u}}|}$ denote the\ncomponents of the complementary group $\\overline{\\vect{u}}$.\nThe components of $\\standardReal \\in \\Rset^{\\inputDim}$\nwhich are in the group $\\vect{u}$ are $\\left(z_{u_i}^{(0)}\\right)_{i = 1, ..., |\\vect{u}|}$\nand the complementary components are\n$\\left(z_{\\overline{u}_i}\\right)_{i = 1, ..., |\\overline{\\vect{u}}|}$.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let $\\overline{\\psi}_{\\overline{\\vect{\\alpha}}}$ be the reduced polynomial:\n\n\\begin{align}:label: PCE_CE_1\n\n    \\overline{\\psi}_{\\overline{\\vect{\\alpha}}}(z_{\\overline{\\vect{u}}})\n    = \\left(\\prod_{i = 1}^{|\\overline{\\vect{u}}|}\n       \\pi_{\\alpha_{\\overline{u}_i}}^{(\\overline{u}_i)}\n       \\left(\\standardReal_{\\overline{u}_i}\\right) \\right)\\end{align}\n\nwhere $\\overline{\\vect{\\alpha}} \\in \\Nset^{|\\vect{u}|}$ is the reduced multi-index\ndefined from the multi-index $\\vect{\\alpha}\\in \\Nset^{\\inputDim}$\nby the equation:\n\n\\begin{align}\\overline{\\alpha}_i = \\alpha_{\\overline{u}_i}\\end{align}\n\nfor $i = 1, ..., |\\overline{\\vect{u}}|$.\nThe components of the reduced multi-index $\\overline{\\vect{\\alpha}}$ which corresponds\nto the components of the multi-index given by the complementary group $|\\vect{u}|$.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We must then gather the reduced multi-indices.\nLet $\\overline{\\cJ}^P$ be the set of unique reduced multi-indices:\n\n\\begin{align}:label: PCE_CE_2\n\n    \\overline{\\cJ}^P = \\left\\{\\overline{\\vect{\\alpha}} \\in \\Nset^{|\\vect{u}|}\n    \\; | \\; \\vect{\\alpha} \\in \\cJ^P\\right\\}.\\end{align}\n\nFor any reduced multi-index $\\overline{\\vect{\\alpha}} \\in \\overline{\\cJ}^P$\nof dimension $|\\overline{\\vect{u}}|$,\nwe note $\\cJ_{\\overline{\\vect{\\alpha}}}^P$\nthe set of corresponding (un-reduced) multi-indices of\ndimension $\\inputDim$:\n\n\\begin{align}:label: PCE_CE_3\n\n    \\cJ_{\\overline{\\vect{\\alpha}}}^P\n    = \\left\\{\\vect{\\alpha} \\in \\cJ^P \\; |\\; \\overline{\\alpha}_i\n    = \\alpha_{\\overline{u}_i}, \\; i = 1, ..., |\\overline{\\vect{u}}|\\right\\}.\\end{align}\n\nEach aggregated coefficient $\\overline{a}_{\\overline{\\vect{\\alpha}}} \\in \\Rset$\nis defined by the equation:\n\n\\begin{align}:label: PCE_CE_5\n\n    \\overline{a}_{\\overline{\\vect{\\alpha}}}\n    = \\sum_{\\vect{\\alpha} \\in \\cJ^P_{\\overline{\\vect{\\alpha}}}}\n    a_{\\vect{\\alpha}} \\left(\\prod_{i = 1}^{|\\vect{u}|}\n    \\pi_{\\alpha_{u_i}}^{(u_i)}\\left(\\standardReal_{u_i}^{(0)}\\right) \\right).\\end{align}\n\nFinally:\n\n\\begin{align}:label: PCE_CE_4\n\n    \\widehat{h}(\\standardReal_{\\overline{\\vect{u}}})\n    = \\sum_{\\overline{\\vect{\\alpha}} \\in \\overline{\\cJ}^P}\n    \\overline{a}_{\\overline{\\vect{\\alpha}}}\n    \\overline{\\psi}(z_{\\overline{\\vect{u}}})\\end{align}\n\nfor any $\\standardReal_{\\overline{\\vect{u}}} \\in \\Rset^{|\\overline{\\vect{u}}|}$.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The method is the following.\n\n- Create the reduced polynomial basis from equation :eq:`PCE_CE_1`.\n- Create the list of reduced multi-indices from the equation :eq:`PCE_CE_2`, and, for each\n  reduced multi-index, the list of corresponding multi-indices from the equation :eq:`PCE_CE_3`.\n- Aggregate the coefficients from the equation :eq:`PCE_CE_5`.\n- The parametric PCE is defined by the equation :eq:`PCE_CE_4`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conditional expectation\n\nOne method to reduce the input dimension of a function is to\nconsider its conditional expectation.\nThe conditional expectation function is:\n\n\\begin{align}\\widehat{\\model}(\\inputReal_{\\vect{u}})\n  = \\Expect{\\model(\\inputReal)\n           \\; | \\; \\inputRV_{\\vect{u}}\n           = \\inputReal_{\\vect{u}}}\\end{align}\n\nfor any $\\inputReal_{\\vect{u}} \\in \\Rset^{|\\vect{u}|}$.\nIn general, there is no dedicated method to create such a conditional expectation\nin the library.\nWe can, however, efficiently compute the conditional expectation of a polynomial\nchaos expansion.\nIn turn, this conditional chaos expansion (PCE) is a polynomial chaos expansion\nwhich can be computed using the :meth:`~openturns.FunctionalChaosResult.getConditionalExpectation`\nmethod from the :class:`~openturns.FunctionalChaosResult` class.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the PCE\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import openturns as ot\nimport openturns.viewer as otv\nfrom openturns.usecases import ishigami_function\nimport matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next function creates a parametric PCE based on a\ngiven PCE and a set of indices.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def meanParametricPCE(chaosResult, indices):\n    \"\"\"\n    Return the parametric PCE of Y with given input marginals set to the mean.\n\n    All marginal inputs, except those in the conditioning indices\n    are set to the mean of the input random vector.\n\n    The resulting function is :\n\n    g(xu) = PCE(xu, xnotu = E[Xnotu])\n\n    where xu is the input vector of conditioning indices,\n    xnotu is the input vector fixed indices and\n    E[Xnotu] is the expectation of the random vector of the components\n    not in u.\n\n    Parameters\n    ----------\n    chaosResult: ot.FunctionalChaosResult(inputDimension)\n        The polynomial chaos expansion.\n    indices: ot.Indices()\n        The indices of the input variables which are set to constant values.\n\n    Returns\n    -------\n    parametricPCEFunction : ot.ParametricFunction(reducedInputDimension, outputDimension)\n        The parametric PCE.\n        The reducedInputDimension is equal to inputDimension - indices.getSize().\n    \"\"\"\n    inputDistribution = chaosResult.getDistribution()\n    if not inputDistribution.hasIndependentCopula():\n        raise ValueError(\n            \"The input distribution has a copula\" \"which is not independent\"\n        )\n    # Create the parametric function\n    pceFunction = chaosResult.getMetaModel()\n    xMean = inputDistribution.getMean()\n    referencePoint = xMean[indices]\n    parametricPCEFunction = ot.ParametricFunction(pceFunction, indices, referencePoint)\n    return parametricPCEFunction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next function creates a sparse PCE using least squares.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def computeSparseLeastSquaresFunctionalChaos(\n    inputTrain,\n    outputTrain,\n    multivariateBasis,\n    basisSize,\n    distribution,\n    sparse=True,\n):\n    \"\"\"\n    Create a sparse polynomial chaos based on least squares.\n\n    * Uses the enumerate rule in multivariateBasis.\n    * Uses the LeastSquaresStrategy to compute the coefficients based on\n      least squares.\n    * Uses LeastSquaresMetaModelSelectionFactory to use the LARS selection method.\n    * Uses FixedStrategy in order to keep all the coefficients that the\n      LARS method selected.\n\n    Parameters\n    ----------\n    inputTrain : ot.Sample\n        The input design of experiments.\n    outputTrain : ot.Sample\n        The output design of experiments.\n    multivariateBasis : ot.Basis\n        The multivariate chaos basis.\n    basisSize : int\n        The size of the function basis.\n    distribution : ot.Distribution.\n        The distribution of the input variable.\n    sparse: bool\n        If True, create a sparse PCE.\n\n    Returns\n    -------\n    result : ot.PolynomialChaosResult\n        The estimated polynomial chaos.\n    \"\"\"\n    if sparse:\n        selectionAlgorithm = ot.LeastSquaresMetaModelSelectionFactory()\n    else:\n        selectionAlgorithm = ot.PenalizedLeastSquaresAlgorithmFactory()\n    projectionStrategy = ot.LeastSquaresStrategy(\n        inputTrain, outputTrain, selectionAlgorithm\n    )\n    adaptiveStrategy = ot.FixedStrategy(multivariateBasis, basisSize)\n    chaosAlgorithm = ot.FunctionalChaosAlgorithm(\n        inputTrain, outputTrain, distribution, adaptiveStrategy, projectionStrategy\n    )\n    chaosAlgorithm.run()\n    chaosResult = chaosAlgorithm.getResult()\n    return chaosResult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the next cell, we create a training sample from the\nIshigami test function.\nWe choose a sample size equal to 1000.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ot.Log.Show(ot.Log.NONE)\not.RandomGenerator.SetSeed(0)\nim = ishigami_function.IshigamiModel()\ninput_names = im.inputDistribution.getDescription()\nsampleSize = 1000\ninputSample = im.inputDistribution.getSample(sampleSize)\noutputSample = im.model(inputSample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then create a sparce PCE of the Ishigami function using\na candidate basis up to the total degree equal to 12.\nThis leads to 455 candidate coefficients.\nThe coefficients are computed from least squares.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "multivariateBasis = ot.OrthogonalProductPolynomialFactory([im.X1, im.X2, im.X3])\ntotalDegree = 12\nenumerateFunction = multivariateBasis.getEnumerateFunction()\nbasisSize = enumerateFunction.getBasisSizeFromTotalDegree(totalDegree)\nprint(\"Basis size = \", basisSize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we create the PCE.\nOnly 61 coefficients are selected by the :class:`~openturns.LARS`\nalgorithm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "chaosResult = computeSparseLeastSquaresFunctionalChaos(\n    inputSample,\n    outputSample,\n    multivariateBasis,\n    basisSize,\n    im.inputDistribution,\n)\nprint(\"Selected basis size = \", chaosResult.getIndices().getSize())\nchaosResult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to see the structure of the data, we create a grid of\nplots which shows all projections of $Y$ versus $X_i$\nfor $i = 1, 2, 3$.\nWe see that the Ishigami function is particularly non linear.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grid = ot.VisualTest.DrawPairsXY(inputSample, outputSample)\ngrid.setTitle(f\"n = {sampleSize}\")\nview = otv.View(grid, figure_kw={\"figsize\": (8.0, 3.0)})\nplt.subplots_adjust(wspace=0.4, bottom=0.25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parametric function\n\nWe now create the parametric function where $X_i$ is free\nand the other variables are set to their mean values.\nWe can show that a parametric PCE is, again, a PCE.\nThe library does not currently implement this feature.\nIn the next cell, we create it from the `meanParametricPCE` we defined\npreviously.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create different parametric functions for the PCE.\nIn the next cell, we create the parametric PCE function\nwhere $X_1$ is active while $X_2$ and $X_3$ are\nset to their mean values.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "indices = [1, 2]\nparametricPCEFunction = meanParametricPCE(chaosResult, indices)\nprint(parametricPCEFunction.getInputDimension())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we know how the `meanParametricPCE` works, we loop over\nthe input marginal indices and consider the three functions\n$\\widehat{\\model}_1(\\inputReal_1)$,\n$\\widehat{\\model}_2(\\inputReal_2)$ and\n$\\widehat{\\model}_3(\\inputReal_3)$.\nFor each marginal index `i`, we we plot the output $Y$\nagainst the input marginal $X_i$ of the sample.\nThen we plot the parametric function depending on $X_i$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inputDimension = im.inputDistribution.getDimension()\nnpPoints = 100\ninputRange = im.inputDistribution.getRange()\ninputLowerBound = inputRange.getLowerBound()\ninputUpperBound = inputRange.getUpperBound()\n# Create the palette with transparency\npalette = ot.Drawable().BuildDefaultPalette(2)\nfirstColor = palette[0]\nr, g, b, a = ot.Drawable.ConvertToRGBA(firstColor)\nnewAlpha = 64\nnewColor = ot.Drawable.ConvertFromRGBA(r, g, b, newAlpha)\npalette[0] = newColor\ngrid = ot.VisualTest.DrawPairsXY(inputSample, outputSample)\nreducedBasisSize = chaosResult.getCoefficients().getSize()\ngrid.setTitle(\n    f\"n = {sampleSize}, total degree = {totalDegree}, \"\n    f\"basis = {basisSize}, selected = {reducedBasisSize}\"\n)\nfor i in range(inputDimension):\n    graph = grid.getGraph(0, i)\n    graph.setLegends([\"Data\"])\n    graph.setXTitle(f\"$x_{1 + i}$\")\n    # Set all indices except i\n    indices = list(range(inputDimension))\n    indices.pop(i)\n    parametricPCEFunction = meanParametricPCE(chaosResult, indices)\n    xiMin = inputLowerBound[i]\n    xiMax = inputUpperBound[i]\n    curve = parametricPCEFunction.draw(xiMin, xiMax, npPoints).getDrawable(0)\n    curve.setLineWidth(2.0)\n    curve.setLegend(r\"$PCE(x_i, x_{-i} = \\mathbb{E}[X_{-i}])$\")\n    graph.add(curve)\n    if i < inputDimension - 1:\n        graph.setLegends([\"\"])\n    graph.setColors(palette)\n    grid.setGraph(0, i, graph)\n\ngrid.setLegendPosition(\"topright\")\nview = otv.View(\n    grid,\n    figure_kw={\"figsize\": (8.0, 3.0)},\n    legend_kw={\"bbox_to_anchor\": (1.0, 1.0), \"loc\": \"upper left\"},\n)\nplt.subplots_adjust(wspace=0.4, right=0.7, bottom=0.25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the parametric function is located within each cloud, but\nsometimes seems a little vertically on the edges of the data.\nMore precisely, the function represents well how $Y$ depends\non $X_2$, but does not seem to represent well how $Y$\ndepends on $X_1$ or $X_3$.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conditional expectation\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the next cell, we create the conditional expectation function\n$\\Expect{\\model(\\inputReal) \\; | \\; \\inputRV_1 = \\inputReal_1}$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "conditionalPCE = chaosResult.getConditionalExpectation([0])\nconditionalPCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "On output, we see that the result is, again, a PCE.\nMoreover, a subset of the previous coefficients are presented in this\nconditional expectation: only multi-indices which involve\n$X_1$ are presented (and the other marginal components are removed).\nWe observe that the value of the coefficients are unchanged with respect to the\nprevious PCE.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the next cell, we create the conditional expectation function\n$\\Expect{\\model(\\inputReal) \\; | \\; \\inputRV_2 = \\inputReal_2, \\inputRV_3 = \\inputReal_3}$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "conditionalPCE = chaosResult.getConditionalExpectation([1, 2])\nconditionalPCE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the conditional PCE has input dimension 2.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the next cell, we compare the parametric PCE and the conditional\nexpectation of the PCE.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_number = 3\ninputDimension = im.inputDistribution.getDimension()\nnpPoints = 100\ninputRange = im.inputDistribution.getRange()\ninputLowerBound = inputRange.getLowerBound()\ninputUpperBound = inputRange.getUpperBound()\n# Create the palette with transparency\npalette = ot.Drawable().BuildDefaultPalette(3)\nfirstColor = palette[0]\nr, g, b, a = ot.Drawable.ConvertToRGBA(firstColor)\nnewAlpha = 64\nnewColor = ot.Drawable.ConvertFromRGBA(r, g, b, newAlpha)\npalette[0] = newColor\ngrid = ot.VisualTest.DrawPairsXY(inputSample, outputSample)\ngrid.setTitle(f\"n = {sampleSize}, total degree = {totalDegree}\")\nfor i in range(inputDimension):\n    graph = grid.getGraph(0, i)\n    graph.setLegends([\"Data\"])\n    graph.setXTitle(f\"$x_{1 + i}$\")\n    xiMin = inputLowerBound[i]\n    xiMax = inputUpperBound[i]\n    # Set all indices except i to the mean\n    indices = list(range(inputDimension))\n    indices.pop(i)\n    parametricPCEFunction = meanParametricPCE(chaosResult, indices)\n    # Draw the parametric function\n    curve = parametricPCEFunction.draw(xiMin, xiMax, npPoints).getDrawable(0)\n    curve.setLineWidth(2.0)\n    curve.setLineStyle(\"dashed\")\n    curve.setLegend(r\"$PCE\\left(x_i, x_{-i} = \\mathbb{E}[X_{-i}]\\right)$\")\n    graph.add(curve)\n    # Compute conditional expectation given Xi\n    conditionalPCE = chaosResult.getConditionalExpectation([i])\n    print(f\"i = {i}\")\n    print(conditionalPCE)\n    conditionalPCEFunction = conditionalPCE.getMetaModel()\n    curve = conditionalPCEFunction.draw(xiMin, xiMax, npPoints).getDrawable(0)\n    curve.setLineWidth(2.0)\n    curve.setLegend(r\"$\\mathbb{E}\\left[PCE | X_i = x_i\\right]$\")\n    graph.add(curve)\n    if i < inputDimension - 1:\n        graph.setLegends([\"\"])\n    graph.setColors(palette)\n    # Set the graph into the grid\n    grid.setGraph(0, i, graph)\n\ngrid.setLegendPosition(\"topright\")\nview = otv.View(\n    grid,\n    figure_kw={\"figsize\": (8.0, 3.0)},\n    legend_kw={\"bbox_to_anchor\": (1.0, 1.0), \"loc\": \"upper left\"},\n)\nplt.subplots_adjust(wspace=0.4, right=0.7, bottom=0.25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the conditional expectation of the PCE is a better\napproximation of the data set than the parametric PCE.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\nIn this example, we have seen how to compute the conditional\nexpectation of a PCE.\nWe have seen that this function is a good approximation of the Ishigami\nfunction when we reduce the input dimension.\nWe have also seen that the parametric PCE might be a poor\napproximation of the Ishigami function.\nThis is because the parametric PCE depends on the particular value\nthat we have chosen to create the parametric function.\n\nThe fact that the conditional expectation of the PCE is a\ngood approximation of the function when we reduce the input dimension\nis a consequence of a theorem which states that the\nconditional expectation is the best approximation of the\nfunction in the least squares sense (see [girardin2018]_ page 79).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "otv.View.ShowAll()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}