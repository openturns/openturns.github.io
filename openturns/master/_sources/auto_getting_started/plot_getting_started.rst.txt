
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_getting_started/plot_getting_started.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_getting_started_plot_getting_started.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_getting_started_plot_getting_started.py:


Getting started
===============

.. GENERATED FROM PYTHON SOURCE LINES 7-9

The goal of this example is to highlight the main features of the library.
It assumes a basic knowledge of the few concepts of the uncertainty methodology (inference, probabilistic modelling, simulation, sensitivity).

.. GENERATED FROM PYTHON SOURCE LINES 11-17

.. code-block:: Python

    from openturns.usecases import cantilever_beam
    import openturns as ot
    import openturns.experimental as otexp
    import openturns.viewer as otv









.. GENERATED FROM PYTHON SOURCE LINES 18-20

Inference of the input distribution
-----------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 22-23

Load a sample of the input variables from the cantilever beam use-case

.. GENERATED FROM PYTHON SOURCE LINES 23-27

.. code-block:: Python

    cb = cantilever_beam.CantileverBeam()
    data = cb.data
    print(data[:5])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

        [ E             F             L             I             ]
    0 : [   6.78712e+10 263.092         2.53306       1.55122e-07 ]
    1 : [   6.50254e+10 309.118         2.53613       1.56701e-07 ]
    2 : [   6.83691e+10 323.088         2.5319        1.47726e-07 ]
    3 : [   6.50185e+10 262.654         2.50948       1.46362e-07 ]
    4 : [   6.88439e+10 294.387         2.52877       1.49355e-07 ]




.. GENERATED FROM PYTHON SOURCE LINES 28-29

Infer marginals from most common 1-d parametric distributions

.. GENERATED FROM PYTHON SOURCE LINES 29-45

.. code-block:: Python

    marginal_factories = [
        ot.NormalFactory(),
        ot.BetaFactory(),
        ot.UniformFactory(),
        ot.LogNormalFactory(),
        ot.TriangularFactory(),
        ot.WeibullMinFactory(),
        ot.WeibullMaxFactory(),
    ]
    estimated_marginals = []
    for index in range(data.getDimension()):
        best_model, _ = ot.FittingTest.BestModelBIC(data[:, index], marginal_factories)
        print(best_model)
        estimated_marginals.append(best_model)
    print(estimated_marginals)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    WeibullMin(beta = 2.22602e+09, alpha = 1.07869, gamma = 6.50071e+10)
    Normal(mu = 302.464, sigma = 27.4306)
    Uniform(a = 2.4996, b = 2.59926)
    WeibullMin(beta = 1.72867e-08, alpha = 2.20598, gamma = 1.29686e-07)
    [class=WeibullMin name=WeibullMin dimension=1 beta=2.22602e+09 alpha=1.07869 gamma=6.50071e+10, class=Normal name=Normal dimension=1 mean=class=Point name=Unnamed dimension=1 values=[302.464] sigma=class=Point name=Unnamed dimension=1 values=[27.4306] correlationMatrix=class=CorrelationMatrix dimension=1 implementation=class=MatrixImplementation name=Unnamed rows=1 columns=1 values=[1], class=Uniform name=Uniform dimension=1 a=2.4996 b=2.59926, class=WeibullMin name=WeibullMin dimension=1 beta=1.72867e-08 alpha=2.20598 gamma=1.29686e-07]




.. GENERATED FROM PYTHON SOURCE LINES 46-47

Assess the goodness of fit of the second marginal

.. GENERATED FROM PYTHON SOURCE LINES 47-50

.. code-block:: Python

    graph = ot.VisualTest.DrawQQplot(data[:, 1], estimated_marginals[1])
    _ = otv.View(graph)




.. image-sg:: /auto_getting_started/images/sphx_glr_plot_getting_started_001.svg
   :alt: Sample versus model QQ-plot
   :srcset: /auto_getting_started/images/sphx_glr_plot_getting_started_001.svg
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 51-53

Infer the copula from common n-d parametric copulas in the ranks space
If the copula is known it can be provided directly through :class:`~openturns.NormalCopula` for example

.. GENERATED FROM PYTHON SOURCE LINES 53-64

.. code-block:: Python

    copula_factories = [
        ot.IndependentCopulaFactory(),
        ot.NormalCopulaFactory(),
        ot.StudentCopulaFactory(),
    ]
    copula_sample = ot.Sample(data.getSize(), data.getDimension())
    for index in range(data.getDimension()):
        copula_sample[:, index] = estimated_marginals[index].computeCDF(data[:, index])
    estimated_copula, _ = ot.FittingTest.BestModelBIC(copula_sample, copula_factories)
    print(estimated_copula)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    IndependentCopula(dimension = 4)




.. GENERATED FROM PYTHON SOURCE LINES 65-66

Assemble the joint distribution from marginals and copula

.. GENERATED FROM PYTHON SOURCE LINES 66-69

.. code-block:: Python

    estimated_distribution = ot.JointDistribution(estimated_marginals, estimated_copula)
    print(estimated_distribution)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    JointDistribution(WeibullMin(beta = 2.22602e+09, alpha = 1.07869, gamma = 6.50071e+10), Normal(mu = 302.464, sigma = 27.4306), Uniform(a = 2.4996, b = 2.59926), WeibullMin(beta = 1.72867e-08, alpha = 2.20598, gamma = 1.29686e-07), IndependentCopula(dimension = 4))




.. GENERATED FROM PYTHON SOURCE LINES 70-72

Uncertainty propagation
-----------------------

.. GENERATED FROM PYTHON SOURCE LINES 74-75

Creation of the model

.. GENERATED FROM PYTHON SOURCE LINES 75-86

.. code-block:: Python



    def fpython(X):
        E, F, L, II = X
        Y = F * L**3 / (3 * E * II)
        return [Y]


    model = ot.PythonFunction(4, 1, fpython)









.. GENERATED FROM PYTHON SOURCE LINES 87-88

The distribution can also be given directly when known

.. GENERATED FROM PYTHON SOURCE LINES 88-90

.. code-block:: Python

    distribution = cb.independentDistribution








.. GENERATED FROM PYTHON SOURCE LINES 91-93

Propagate the input distribution through the model
Here the function evaluation can benefit parallelization depending on its type, see also n_cpus option from :class:`~openturns.PythonFunction`.

.. GENERATED FROM PYTHON SOURCE LINES 93-96

.. code-block:: Python

    sample_x = distribution.getSample(1000)
    sample_y = model(sample_x)








.. GENERATED FROM PYTHON SOURCE LINES 97-98

Estimate a non-parametric distribution (see :class:`~openturns.KernelSmoothing`) of the output variable

.. GENERATED FROM PYTHON SOURCE LINES 98-105

.. code-block:: Python

    ks = ot.KernelSmoothing().build(sample_y)
    grid = ot.GridLayout(1, 2)
    grid.setGraph(0, 0, ks.drawPDF())
    grid.setGraph(0, 1, ks.drawCDF())
    _ = otv.View(grid)





.. image-sg:: /auto_getting_started/images/sphx_glr_plot_getting_started_002.svg
   :alt: plot getting started
   :srcset: /auto_getting_started/images/sphx_glr_plot_getting_started_002.svg
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 106-108

Build a metamodel with polynomial chaos expansion
-------------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 108-120

.. code-block:: Python

    algo = ot.LeastSquaresExpansion(sample_x, sample_y, distribution)
    algo.run()
    metamodel_result = algo.getResult()
    metamodel = metamodel_result.getMetaModel()
    test_x = distribution.getSample(1000)
    test_y = model(test_x)
    predictions = metamodel(test_x)
    validation = ot.MetaModelValidation(test_y, predictions)
    graph = validation.drawValidation()
    graph.setTitle(graph.getTitle() + f" R2={validation.computeR2Score()}")
    _ = otv.View(graph)




.. image-sg:: /auto_getting_started/images/sphx_glr_plot_getting_started_003.svg
   :alt: Metamodel validation - n = 1000 R2=[1]
   :srcset: /auto_getting_started/images/sphx_glr_plot_getting_started_003.svg
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 121-123

Sensitivity analysis
----------------------

.. GENERATED FROM PYTHON SOURCE LINES 125-126

For simplicity we can use a method that does not impose special requirements on the design of experiments

.. GENERATED FROM PYTHON SOURCE LINES 126-131

.. code-block:: Python

    sobol_x = distribution.getSample(5000)
    sobol_y = metamodel(sobol_x)
    algo = otexp.RankSobolSensitivityAlgorithm(sobol_x, sobol_y)
    print(algo.getFirstOrderIndices())





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [0.0477717,0.710556,0.0780995,0.164337]




.. GENERATED FROM PYTHON SOURCE LINES 132-134

Plot the sensitivity indices
The most influential variable is `F`.

.. GENERATED FROM PYTHON SOURCE LINES 134-137

.. code-block:: Python

    graph = algo.draw()
    _ = otv.View(graph)




.. image-sg:: /auto_getting_started/images/sphx_glr_plot_getting_started_004.svg
   :alt: Sobol' indices - RankSobolSensitivityAlgorithm
   :srcset: /auto_getting_started/images/sphx_glr_plot_getting_started_004.svg
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 138-139

.. code-block:: Python

    otv.View.ShowAll()








.. _sphx_glr_download_auto_getting_started_plot_getting_started.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_getting_started.ipynb <plot_getting_started.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_getting_started.py <plot_getting_started.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_getting_started.zip <plot_getting_started.zip>`
