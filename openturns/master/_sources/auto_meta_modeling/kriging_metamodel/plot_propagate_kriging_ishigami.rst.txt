
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_meta_modeling/kriging_metamodel/plot_propagate_kriging_ishigami.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_meta_modeling_kriging_metamodel_plot_propagate_kriging_ishigami.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_meta_modeling_kriging_metamodel_plot_propagate_kriging_ishigami.py:


Kriging: propagate uncertainties
================================

In this example we propagate uncertainties through a Kriging metamodel of the :ref:`Ishigami model<use-case-ishigami>`.

.. GENERATED FROM PYTHON SOURCE LINES 7-13

.. code-block:: Python


    import openturns as ot
    from matplotlib import pylab as plt
    import openturns.viewer as otv









.. GENERATED FROM PYTHON SOURCE LINES 14-18

We first build the metamodel and then compute its mean with a Monte-Carlo
computation.

We load the Ishigami model from the usecases module:

.. GENERATED FROM PYTHON SOURCE LINES 18-23

.. code-block:: Python


    from openturns.usecases import ishigami_function

    im = ishigami_function.IshigamiModel()








.. GENERATED FROM PYTHON SOURCE LINES 24-26

We build a design of experiments with a Latin Hypercube Sampling (LHS) for the three input variables
supposed independent.

.. GENERATED FROM PYTHON SOURCE LINES 26-29

.. code-block:: Python

    experiment = ot.LHSExperiment(im.inputDistribution, 30, False, True)
    xdata = experiment.generate()








.. GENERATED FROM PYTHON SOURCE LINES 30-32

We get the exact model and evaluate it at the input training data `xdata` to
build the output data `ydata`.

.. GENERATED FROM PYTHON SOURCE LINES 32-35

.. code-block:: Python

    model = im.model
    ydata = model(xdata)








.. GENERATED FROM PYTHON SOURCE LINES 36-41

We define our Kriging strategy :

 - a constant basis in :math:`\mathbb{R}^3` ;
 - a squared exponential covariance function.


.. GENERATED FROM PYTHON SOURCE LINES 41-48

.. code-block:: Python

    dimension = 3
    basis = ot.ConstantBasisFactory(dimension).build()
    covarianceModel = ot.SquaredExponential([0.1] * dimension, [1.0])
    algo = ot.KrigingAlgorithm(xdata, ydata, covarianceModel, basis)
    algo.run()
    result = algo.getResult()








.. GENERATED FROM PYTHON SOURCE LINES 49-50

We finally get the metamodel to use with Monte-Carlo.

.. GENERATED FROM PYTHON SOURCE LINES 50-52

.. code-block:: Python

    metamodel = result.getMetaModel()








.. GENERATED FROM PYTHON SOURCE LINES 53-57

We want to estmate the mean of the Ishigami model with Monte-Carlo using the
metamodel instead of the exact model.

We first create a random vector following the input distribution :

.. GENERATED FROM PYTHON SOURCE LINES 57-59

.. code-block:: Python

    X = ot.RandomVector(im.inputDistribution)








.. GENERATED FROM PYTHON SOURCE LINES 60-62

And then we create a random vector from the image of the input random vector
by the metamodel :

.. GENERATED FROM PYTHON SOURCE LINES 62-64

.. code-block:: Python

    Y = ot.CompositeRandomVector(metamodel, X)








.. GENERATED FROM PYTHON SOURCE LINES 65-66

We now set our :class:`~openturns.ExpectationSimulationAlgorithm` object :

.. GENERATED FROM PYTHON SOURCE LINES 66-71

.. code-block:: Python

    algo = ot.ExpectationSimulationAlgorithm(Y)
    algo.setMaximumOuterSampling(50000)
    algo.setBlockSize(1)
    algo.setCoefficientOfVariationCriterionType("NONE")








.. GENERATED FROM PYTHON SOURCE LINES 72-73

We run it and store the results :

.. GENERATED FROM PYTHON SOURCE LINES 73-76

.. code-block:: Python

    algo.run()
    result = algo.getResult()








.. GENERATED FROM PYTHON SOURCE LINES 77-78

The expectation ( :math:`\mathbb{E}(Y)` mean ) is obtained with :

.. GENERATED FROM PYTHON SOURCE LINES 78-80

.. code-block:: Python

    expectation = result.getExpectationEstimate()








.. GENERATED FROM PYTHON SOURCE LINES 81-82

The mean estimate of the metamodel is

.. GENERATED FROM PYTHON SOURCE LINES 82-84

.. code-block:: Python

    print("Mean of the Ishigami metamodel : %.3e" % expectation[0])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Mean of the Ishigami metamodel : 3.430e+00




.. GENERATED FROM PYTHON SOURCE LINES 85-86

We draw the convergence history.

.. GENERATED FROM PYTHON SOURCE LINES 86-89

.. code-block:: Python

    graph = algo.drawExpectationConvergence()
    view = otv.View(graph)




.. image-sg:: /auto_meta_modeling/kriging_metamodel/images/sphx_glr_plot_propagate_kriging_ishigami_001.png
   :alt: Expectation convergence graph at level 0.95
   :srcset: /auto_meta_modeling/kriging_metamodel/images/sphx_glr_plot_propagate_kriging_ishigami_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 90-91

For reference, the exact mean of the Ishigami model is :

.. GENERATED FROM PYTHON SOURCE LINES 91-94

.. code-block:: Python

    print("Mean of the Ishigami model : %.3e" % im.expectation)

    plt.show()




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Mean of the Ishigami model : 3.500e+00





.. _sphx_glr_download_auto_meta_modeling_kriging_metamodel_plot_propagate_kriging_ishigami.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_propagate_kriging_ishigami.ipynb <plot_propagate_kriging_ishigami.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_propagate_kriging_ishigami.py <plot_propagate_kriging_ishigami.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_propagate_kriging_ishigami.zip <plot_propagate_kriging_ishigami.zip>`
