
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_meta_modeling/kriging_metamodel/plot_gpr_hyperparameters_optimization.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_meta_modeling_kriging_metamodel_plot_gpr_hyperparameters_optimization.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_meta_modeling_kriging_metamodel_plot_gpr_hyperparameters_optimization.py:


Gaussian process fitter: configure the optimization solver
==========================================================

.. GENERATED FROM PYTHON SOURCE LINES 7-44

The goal of this example is to show how to fine-tune the optimization solver used to estimate the hyperparameters of the covariance model of the Gaussian process regression.

Introduction
------------

In a Gaussian process regression, there are various types of parameters which are estimated from the data.

* The parameters :math:`{\bf \beta}` associated with the deterministic trend. These parameters are computed based on linear least squares.
* The parameters of the covariance model.

We only consider the following parameters of the covariance model:

* The magnitude parameter :math:`\sigma^2` is estimated from the data.
  If the output dimension is equal to one, this parameter is estimated using
  the analytic variance estimator which maximizes the likelihood.
  Otherwise, if output dimension is greater than one or analytical sigma disabled,
  this parameter is estimated from numerical optimization.
* The scale parameters :math:`{\bf \theta}\in\mathbb{R}^d` where :math:`d` is
  the spatial dimension of the covariance model.
  Often, the parameter :math:`{\bf \theta}` is a scale parameter.
  This step involves an optimization algorithm.

All these parameters are estimated with the :class:`~openturns.experimental.GaussianProcessFitter` class.

The estimation of the :math:`\vect{\theta}` parameters is the step which has the highest CPU cost.
Moreover, the maximization of likelihood may be associated with difficulties e.g. many local maximums or even the non convergence of the optimization algorithm.
In this case, it might be useful to fine tune the optimization algorithm so that the convergence of the optimization algorithm is, hopefully, improved.

Furthermore, there are several situations in which the optimization can be initialized or completely bypassed.
Suppose for example that we have already created an initial Gaussian process regression with :math:`N` points and we want to add a single new point.

* It might be interesting to initialize the optimization algorithm with the optimum found for the previous Gaussian process regression:
  this may reduce the number of iterations required to maximize the likelihood.
* We may as well completely bypass the optimization step: if the previous covariance model was correctly estimated,
  the update of the parameters may or may not significantly improve the estimates.

This is why the goal of this example is to see how to configure the optimization of the hyperparameters of a Gaussian process regression.

.. GENERATED FROM PYTHON SOURCE LINES 46-48

Definition of the model
-----------------------

.. GENERATED FROM PYTHON SOURCE LINES 50-55

.. code-block:: Python

    import openturns as ot
    import openturns.experimental as otexp
    import openturns.viewer as otv









.. GENERATED FROM PYTHON SOURCE LINES 56-57

We define the symbolic function which evaluates the output `Y` depending on the inputs `E`, `F`, `L` and `I`.

.. GENERATED FROM PYTHON SOURCE LINES 59-61

.. code-block:: Python

    model = ot.SymbolicFunction(["E", "F", "L", "I"], ["F*L^3/(3*E*I)"])








.. GENERATED FROM PYTHON SOURCE LINES 62-63

Then we define the distribution of the input random vector.

.. GENERATED FROM PYTHON SOURCE LINES 65-66

Young's modulus `E`

.. GENERATED FROM PYTHON SOURCE LINES 66-79

.. code-block:: Python

    E = ot.Beta(0.9, 2.27, 2.5e7, 5.0e7)  # in N/m^2
    E.setDescription("E")
    # Load F
    F = ot.LogNormal()  # in N
    F.setParameter(ot.LogNormalMuSigma()([30.0e3, 9e3, 15.0e3]))
    F.setDescription("F")
    # Length L
    L = ot.Uniform(250.0, 260.0)  # in cm
    L.setDescription("L")
    # Moment of inertia I
    II = ot.Beta(2.5, 1.5, 310, 450)  # in cm^4
    II.setDescription("I")








.. GENERATED FROM PYTHON SOURCE LINES 80-81

Finally, we define the dependency using a :class:`~openturns.NormalCopula`.

.. GENERATED FROM PYTHON SOURCE LINES 83-89

.. code-block:: Python

    dim = 4  # number of inputs
    R = ot.CorrelationMatrix(dim)
    R[2, 3] = -0.2
    myCopula = ot.NormalCopula(ot.NormalCopula.GetCorrelationFromSpearmanCorrelation(R))
    myDistribution = ot.JointDistribution([E, F, L, II], myCopula)








.. GENERATED FROM PYTHON SOURCE LINES 90-92

Create the design of experiments
--------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 94-97

We consider a simple Monte-Carlo sampling as a design of experiments.
This is why we generate an input sample using the :meth:`~openturns.Distribution.getSample` method of the distribution.
Then we evaluate the output using the `model` function.

.. GENERATED FROM PYTHON SOURCE LINES 99-103

.. code-block:: Python

    sampleSize_train = 10
    X_train = myDistribution.getSample(sampleSize_train)
    Y_train = model(X_train)








.. GENERATED FROM PYTHON SOURCE LINES 104-106

Create the metamodel
--------------------

.. GENERATED FROM PYTHON SOURCE LINES 108-113

In order to create the Gaussian process regression, we first select a constant trend with the :class:`~openturns.ConstantBasisFactory` class.
Then we use a squared exponential covariance model.
Finally, we use the :class:`~openturns.experimental.GaussianProcessFitter` class to calibrate a covariance model for a Gaussian process regression,
taking the training sample, the covariance model and the trend basis as input arguments.
The scale parameters must be positive.

.. GENERATED FROM PYTHON SOURCE LINES 115-136

.. code-block:: Python

    dimension = myDistribution.getDimension()
    basis = ot.ConstantBasisFactory(dimension).build()

    x_range = X_train.computeRange()
    print("x_range:")
    print(x_range)
    scale_max_factor = 4.0  # Must be > 1, tune this to match your problem
    scale_min_factor = 0.1  # Must be < 1, tune this to match your problem
    maximum_scale_bounds = scale_max_factor * x_range
    minimum_scale_bounds = scale_min_factor * x_range
    scaleOptimizationBounds = ot.Interval(minimum_scale_bounds, maximum_scale_bounds)
    print("scaleOptimizationBounds")
    print(scaleOptimizationBounds)

    covarianceModel = ot.SquaredExponential([1.0] * dimension, [1.0])
    covarianceModel.setScale(x_range * 0.5)
    algo = otexp.GaussianProcessFitter(X_train, Y_train, covarianceModel, basis)
    algo.setOptimizationBounds(scaleOptimizationBounds)
    algo.run()
    result = algo.getResult()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    x_range:
    [1.31501e+07,13652.8,8.1952,63.8489]
    scaleOptimizationBounds
    [1.31501e+06, 5.26006e+07]
    [1365.28, 54611.3]
    [0.81952, 32.7808]
    [6.38489, 255.395]




.. GENERATED FROM PYTHON SOURCE LINES 137-141

The :meth:`~openturns.experimental.GaussianProcessFitter.run` method has optimized the hyperparameters of the metamodel.

We can then print the constant trend of the metamodel, which have been
estimated using the least squares method.

.. GENERATED FROM PYTHON SOURCE LINES 143-145

.. code-block:: Python

    result.getTrendCoefficients()






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    class=Point name=Unnamed dimension=1 values=[11.8658]
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 146-147

We can also print the hyperparameters of the covariance model, which have been estimated by maximizing the likelihood.

.. GENERATED FROM PYTHON SOURCE LINES 149-152

.. code-block:: Python

    basic_covariance_model = result.getCovarianceModel()
    print(basic_covariance_model)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    SquaredExponential(scale=[6.57507e+06,6829.88,12.6525,101.068], amplitude=[2.61397])




.. GENERATED FROM PYTHON SOURCE LINES 153-155

Get the optimizer algorithm
---------------------------

.. GENERATED FROM PYTHON SOURCE LINES 157-161

The :meth:`~openturns.experimental.GaussianProcessFitter.getOptimizationAlgorithm` method
returns the optimization algorithm used to optimize the
:math:`\vect{\theta}` parameters of the
:class:`~openturns.SquaredExponential` covariance model.

.. GENERATED FROM PYTHON SOURCE LINES 163-165

.. code-block:: Python

    solver = algo.getOptimizationAlgorithm()








.. GENERATED FROM PYTHON SOURCE LINES 166-167

Get the default optimizer.

.. GENERATED FROM PYTHON SOURCE LINES 169-172

.. code-block:: Python

    solverImplementation = solver.getImplementation()
    solverImplementation.getClassName()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    'Cobyla'



.. GENERATED FROM PYTHON SOURCE LINES 173-178

The :meth:`~openturns.experimental.GaussianProcessFitter.getOptimizationBounds` method
returns the bounds.
The dimension of these bounds correspond to the spatial dimension of
the covariance model.
In the metamodeling context, this correspond to the input dimension of the model.

.. GENERATED FROM PYTHON SOURCE LINES 180-183

.. code-block:: Python

    bounds = algo.getOptimizationBounds()
    bounds.getDimension()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    4



.. GENERATED FROM PYTHON SOURCE LINES 184-188

.. code-block:: Python

    lbounds = bounds.getLowerBound()
    print("lbounds")
    print(lbounds)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    lbounds
    [1.31501e+06,1365.28,0.81952,6.38489]




.. GENERATED FROM PYTHON SOURCE LINES 189-193

.. code-block:: Python

    ubounds = bounds.getUpperBound()
    print("ubounds")
    print(ubounds)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ubounds
    [5.26006e+07,54611.3,32.7808,255.395]




.. GENERATED FROM PYTHON SOURCE LINES 194-195

The :meth:`~openturns.experimental.GaussianProcessFitter.getOptimizeParameters` method returns `True` if these parameters are to be optimized.

.. GENERATED FROM PYTHON SOURCE LINES 197-201

.. code-block:: Python

    isOptimize = algo.getOptimizeParameters()
    print(isOptimize)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    True




.. GENERATED FROM PYTHON SOURCE LINES 202-204

Configure the starting point of the optimization
------------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 206-207

The starting point of the optimization is based on the parameters of the covariance model.

.. GENERATED FROM PYTHON SOURCE LINES 209-213

.. code-block:: Python

    covarianceModel = ot.SquaredExponential(maximum_scale_bounds, [1.0])
    algo = otexp.GaussianProcessFitter(X_train, Y_train, covarianceModel, basis)
    algo.setOptimizationBounds(scaleOptimizationBounds)








.. GENERATED FROM PYTHON SOURCE LINES 214-216

.. code-block:: Python

    algo.run()








.. GENERATED FROM PYTHON SOURCE LINES 217-221

.. code-block:: Python

    result = algo.getResult()
    new_covariance_model = result.getCovarianceModel()
    print(new_covariance_model)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    SquaredExponential(scale=[5.26006e+07,54523.6,32.7808,255.395], amplitude=[18.9295])




.. GENERATED FROM PYTHON SOURCE LINES 222-223

In order to see the difference with the default optimization, we print the previous optimized covariance model.

.. GENERATED FROM PYTHON SOURCE LINES 225-227

.. code-block:: Python

    print(basic_covariance_model)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    SquaredExponential(scale=[6.57507e+06,6829.88,12.6525,101.068], amplitude=[2.61397])




.. GENERATED FROM PYTHON SOURCE LINES 228-229

We draw the graphs that validate the meta model.

.. GENERATED FROM PYTHON SOURCE LINES 231-240

.. code-block:: Python

    algo_gpr = otexp.GaussianProcessRegression(result)
    algo_gpr.run()
    metamodel = algo_gpr.getResult().getMetaModel()
    X_test = myDistribution.getSample(100)
    Y_test = model(X_test)
    validation = ot.MetaModelValidation(Y_test, metamodel(X_test))
    g = validation.drawValidation()
    view = otv.View(g)




.. image-sg:: /auto_meta_modeling/kriging_metamodel/images/sphx_glr_plot_gpr_hyperparameters_optimization_001.svg
   :alt: Metamodel validation - n = 100
   :srcset: /auto_meta_modeling/kriging_metamodel/images/sphx_glr_plot_gpr_hyperparameters_optimization_001.svg
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 241-242

We observe that this does not change much the values of the parameters in this case.

.. GENERATED FROM PYTHON SOURCE LINES 244-246

Disabling the optimization
--------------------------

.. GENERATED FROM PYTHON SOURCE LINES 248-251

It is sometimes useful to completely disable the optimization of the parameters.
In order to see the effect of this, we first initialize the parameters of
the covariance model with the arbitrary values `[12.0, 34.0, 56.0, 78.0]`.

.. GENERATED FROM PYTHON SOURCE LINES 253-256

.. code-block:: Python

    covarianceModel = ot.SquaredExponential([12.0, 34.0, 56.0, 78.0], [91.0])
    algo = otexp.GaussianProcessFitter(X_train, Y_train, covarianceModel, basis)








.. GENERATED FROM PYTHON SOURCE LINES 257-259

The :meth:`~openturns.experimental.GaussianProcessFitter.setOptimizeParameters` method can be
used to disable the optimization of the parameters.

.. GENERATED FROM PYTHON SOURCE LINES 261-263

.. code-block:: Python

    algo.setOptimizeParameters(False)








.. GENERATED FROM PYTHON SOURCE LINES 264-265

Then we run the algorithm and get the result.

.. GENERATED FROM PYTHON SOURCE LINES 267-270

.. code-block:: Python

    algo.run()
    result = algo.getResult()








.. GENERATED FROM PYTHON SOURCE LINES 271-273

We observe that the covariance model is unchanged:
the parameters have not been optimized, as required.

.. GENERATED FROM PYTHON SOURCE LINES 275-278

.. code-block:: Python

    updated_covariance_model = result.getCovarianceModel()
    print(updated_covariance_model)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    SquaredExponential(scale=[12,34,56,78], amplitude=[91])




.. GENERATED FROM PYTHON SOURCE LINES 279-280

The trend, however, is still optimized, using linear least squares.

.. GENERATED FROM PYTHON SOURCE LINES 282-284

.. code-block:: Python

    result.getTrendCoefficients()






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    class=Point name=Unnamed dimension=1 values=[11.607]
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 285-292

Reuse the parameters from a previous optimization
-------------------------------------------------

In this example, we show how to reuse the optimized parameters of a previous Gaussian process fit and configure a new one.
Furthermore, we disable the optimization so that the parameters of the covariance model are not updated.
This makes the process of adding a new point very fast:
it improves the quality by adding a new point in the design of experiments without paying the price of the update of the covariance model.

.. GENERATED FROM PYTHON SOURCE LINES 294-295

Step 1: Run a first Gaussian process fitter algorithm.

.. GENERATED FROM PYTHON SOURCE LINES 297-305

.. code-block:: Python

    dimension = myDistribution.getDimension()
    basis = ot.ConstantBasisFactory(dimension).build()
    covarianceModel = ot.SquaredExponential(maximum_scale_bounds, [1.0])
    algo = otexp.GaussianProcessFitter(X_train, Y_train, covarianceModel, basis)
    algo.setOptimizationBounds(scaleOptimizationBounds)
    algo.run()
    result = algo.getResult()








.. GENERATED FROM PYTHON SOURCE LINES 306-307

Now retrieve the optimized the covariance model.

.. GENERATED FROM PYTHON SOURCE LINES 307-310

.. code-block:: Python

    covarianceModel = result.getCovarianceModel()
    print(covarianceModel)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    SquaredExponential(scale=[5.26006e+07,54523.6,32.7808,255.395], amplitude=[18.9295])




.. GENERATED FROM PYTHON SOURCE LINES 311-312

Step 2: Create a new point and add it to the previous training design.

.. GENERATED FROM PYTHON SOURCE LINES 314-317

.. code-block:: Python

    X_new = myDistribution.getSample(20)
    Y_new = model(X_new)








.. GENERATED FROM PYTHON SOURCE LINES 318-321

.. code-block:: Python

    X_train.add(X_new)
    X_train.getSize()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    30



.. GENERATED FROM PYTHON SOURCE LINES 322-325

.. code-block:: Python

    Y_train.add(Y_new)
    Y_train.getSize()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    30



.. GENERATED FROM PYTHON SOURCE LINES 326-327

Step 3: Create an updated Gaussian process fit, using the new point with the old covariance parameters.

.. GENERATED FROM PYTHON SOURCE LINES 329-337

.. code-block:: Python

    algo = otexp.GaussianProcessFitter(X_train, Y_train, covarianceModel, basis)
    algo.setOptimizeParameters(False)
    algo.run()
    result = algo.getResult()
    notUpdatedCovarianceModel = result.getCovarianceModel()
    print(notUpdatedCovarianceModel)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    SquaredExponential(scale=[5.26006e+07,54523.6,32.7808,255.395], amplitude=[18.9295])




.. GENERATED FROM PYTHON SOURCE LINES 338-347

.. code-block:: Python

    def printCovarianceParameterChange(covarianceModel1, covarianceModel2):
        parameters1 = covarianceModel1.getFullParameter()
        parameters2 = covarianceModel2.getFullParameter()
        for i in range(parameters1.getDimension()):
            deltai = abs(parameters1[i] - parameters2[i])
            print("Change in the parameter #%d = %s" % (i, deltai))
        return









.. GENERATED FROM PYTHON SOURCE LINES 348-350

.. code-block:: Python

    printCovarianceParameterChange(covarianceModel, notUpdatedCovarianceModel)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Change in the parameter #0 = 0.0
    Change in the parameter #1 = 0.0
    Change in the parameter #2 = 0.0
    Change in the parameter #3 = 0.0
    Change in the parameter #4 = 0.0
    Change in the parameter #5 = 0.0




.. GENERATED FROM PYTHON SOURCE LINES 351-355

We see that the parameters did not change *at all*: disabling the optimization allows one to keep a constant covariance model.
In a practical algorithm, we may, for example, add a block of 10 new points before updating the parameters of the covariance model.
At this point, we may reuse the previous covariance model so that the optimization starts from a better point, compared to the parameters default values.
This will reduce the cost of the optimization.

.. GENERATED FROM PYTHON SOURCE LINES 357-359

Configure the local optimization solver
---------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 361-363

The following example shows how to set the local optimization solver.
We choose the `SLSQP` algorithm from :class:`~openturns.NLopt`.

.. GENERATED FROM PYTHON SOURCE LINES 365-374

.. code-block:: Python

    problem = solver.getProblem()
    local_solver = ot.NLopt(problem, "LD_SLSQP")
    covarianceModel = ot.SquaredExponential(maximum_scale_bounds, [1.0])
    algo = otexp.GaussianProcessFitter(X_train, Y_train, covarianceModel, basis)
    algo.setOptimizationBounds(scaleOptimizationBounds)
    algo.setOptimizationAlgorithm(local_solver)
    algo.run()
    result = algo.getResult()








.. GENERATED FROM PYTHON SOURCE LINES 375-378

.. code-block:: Python

    finetune_covariance_model = result.getCovarianceModel()
    print(finetune_covariance_model)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    SquaredExponential(scale=[5.26006e+07,54611.3,32.7808,255.395], amplitude=[40.2295])




.. GENERATED FROM PYTHON SOURCE LINES 379-382

.. code-block:: Python

    printCovarianceParameterChange(finetune_covariance_model, basic_covariance_model)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Change in the parameter #0 = 46025491.132901676
    Change in the parameter #1 = 47781.40659674547
    Change in the parameter #2 = 20.12824320768039
    Change in the parameter #3 = 154.32738547072643
    Change in the parameter #4 = 0.0
    Change in the parameter #5 = 37.61553351469246




.. GENERATED FROM PYTHON SOURCE LINES 383-385

Configure the global optimization solver
----------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 387-394

The following example checks the robustness of the optimization of the
Gaussian process fitter with respect to the optimization of the likelihood
function in the covariance model parameters estimation.
We use a :class:`~openturns.MultiStart` algorithm in order to avoid to be trapped by a local minimum.
Furthermore, we generate the design of experiments using a
:class:`~openturns.LHSExperiment`, which guarantees that the points
will fill the space.

.. GENERATED FROM PYTHON SOURCE LINES 396-400

.. code-block:: Python

    sampleSize_train = 10
    X_train = myDistribution.getSample(sampleSize_train)
    Y_train = model(X_train)








.. GENERATED FROM PYTHON SOURCE LINES 401-404

First, we create a multivariate distribution, based on independent
:class:`~openturns.Uniform` marginals which have the bounds required
by the covariance model.

.. GENERATED FROM PYTHON SOURCE LINES 406-409

.. code-block:: Python

    distributions = [ot.Uniform(lbounds[i], ubounds[i]) for i in range(dim)]
    boundedDistribution = ot.JointDistribution(distributions)








.. GENERATED FROM PYTHON SOURCE LINES 410-412

We first generate a Latin Hypercube Sampling (LHS) design made of 25 points in the sample space.
This LHS is optimized so as to fill the space.

.. GENERATED FROM PYTHON SOURCE LINES 414-423

.. code-block:: Python

    K = 25  # design size
    LHS = ot.LHSExperiment(boundedDistribution, K)
    SA_profile = ot.GeometricProfile(10.0, 0.95, 20000)
    LHS_optimization_algo = ot.SimulatedAnnealingLHS(LHS, ot.SpaceFillingC2(), SA_profile)
    LHS_optimization_algo.generate()
    LHS_design = LHS_optimization_algo.getResult()
    starting_points = LHS_design.getOptimalDesign()
    starting_points.getSize()





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    25



.. GENERATED FROM PYTHON SOURCE LINES 424-425

We check that all the starting points are within the bounds.

.. GENERATED FROM PYTHON SOURCE LINES 425-427

.. code-block:: Python

    scaleOptimizationBounds.contains(starting_points)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]



.. GENERATED FROM PYTHON SOURCE LINES 428-429

Then we create a :class:`~openturns.MultiStart` algorithm based on the LHS starting points.

.. GENERATED FROM PYTHON SOURCE LINES 431-434

.. code-block:: Python

    solver.setMaximumIterationNumber(10000)
    multiStartSolver = ot.MultiStart(solver, starting_points)








.. GENERATED FROM PYTHON SOURCE LINES 435-438

Finally, we configure the optimization algorithm so as to use the :class:`~openturns.MultiStart`
algorithm. The bounds of the algorithm are set to match the range of the distribution used to
generate the starting points.

.. GENERATED FROM PYTHON SOURCE LINES 440-446

.. code-block:: Python

    algo = otexp.GaussianProcessFitter(X_train, Y_train, covarianceModel, basis)
    algo.setOptimizationBounds(boundedDistribution.getRange())
    algo.setOptimizationAlgorithm(multiStartSolver)
    algo.run()
    result = algo.getResult()








.. GENERATED FROM PYTHON SOURCE LINES 447-450

.. code-block:: Python

    finetune_covariance_model = result.getCovarianceModel()
    print(finetune_covariance_model)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    SquaredExponential(scale=[1.46401e+07,19344.8,32.7808,189.355], amplitude=[4.73898])




.. GENERATED FROM PYTHON SOURCE LINES 451-453

.. code-block:: Python

    printCovarianceParameterChange(finetune_covariance_model, basic_covariance_model)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Change in the parameter #0 = 8065045.432727364
    Change in the parameter #1 = 12514.895981386926
    Change in the parameter #2 = 20.12824320768039
    Change in the parameter #3 = 88.28706845291623
    Change in the parameter #4 = 0.0
    Change in the parameter #5 = 2.1250101745400554




.. GENERATED FROM PYTHON SOURCE LINES 454-455

We see that there are no changes in the estimated parameters. This shows that the first optimization of the parameters worked fine.

.. GENERATED FROM PYTHON SOURCE LINES 458-459

Display figures

.. GENERATED FROM PYTHON SOURCE LINES 459-460

.. code-block:: Python

    otv.View.ShowAll()








.. _sphx_glr_download_auto_meta_modeling_kriging_metamodel_plot_gpr_hyperparameters_optimization.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_gpr_hyperparameters_optimization.ipynb <plot_gpr_hyperparameters_optimization.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_gpr_hyperparameters_optimization.py <plot_gpr_hyperparameters_optimization.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_gpr_hyperparameters_optimization.zip <plot_gpr_hyperparameters_optimization.zip>`
