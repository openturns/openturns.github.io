
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_surrogate_modeling/gaussian_process_regression/plot_gpr_normalization.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_surrogate_modeling_gaussian_process_regression_plot_gpr_normalization.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_surrogate_modeling_gaussian_process_regression_plot_gpr_normalization.py:


Gaussian Process Regression: Normalization for optimization
===========================================================

.. GENERATED FROM PYTHON SOURCE LINES 7-14

This example aims to illustrate Gaussian Process Fitter metamodel with normalization of data.
Like other machine learning techniques, heteregeneous data (i.e., data defined with different orders of magnitude)
can impact the training process of Gaussian Process Regression (GPR).
Automatic scaling process of the input data for the optimization of GPR hyperparameters can be defined using
the :class:`~openturns.ResourceMap` key `GaussianProcessFitter-OptimizationNormalization`.
In this example, we show the behavior of Gaussian Process Fitter with and without activating
the normalization of hyperparameters for the optimization.

.. GENERATED FROM PYTHON SOURCE LINES 17-21

Loading of the fire satellite use case
-----------------------------------------------------
This model involves 9 input variables and 3 output variables. We select only the first output variable in this example.
We load the :ref:`Fire satellite use case<use-case-fire-satellite>`.

.. GENERATED FROM PYTHON SOURCE LINES 23-28

.. code-block:: Python

    import openturns as ot
    from openturns.usecases.fire_satellite import FireSatelliteModel
    import openturns.viewer as otv









.. GENERATED FROM PYTHON SOURCE LINES 29-30

We define the function that evaluates the outputs depending on the inputs.

.. GENERATED FROM PYTHON SOURCE LINES 30-33

.. code-block:: Python

    m = FireSatelliteModel()
    model = m.model








.. GENERATED FROM PYTHON SOURCE LINES 34-35

We also define the distribution of input variables to build the training and test sets.

.. GENERATED FROM PYTHON SOURCE LINES 35-38

.. code-block:: Python

    inputDistribution = m.inputDistribution









.. GENERATED FROM PYTHON SOURCE LINES 39-42

Generation of data
------------------
We now generate the input and output training sets as 20 times the dimension of the input vector.

.. GENERATED FROM PYTHON SOURCE LINES 42-49

.. code-block:: Python

    experiment = ot.LHSExperiment(inputDistribution, 20 * m.dim)
    inputTrainingSet = experiment.generate()
    outputTrainingSet = model(inputTrainingSet).getMarginal(0)

    print("Lower and upper bounds of inputTrainingSet:")
    print(inputTrainingSet.getMin(), inputTrainingSet.getMax())





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Lower and upper bounds of inputTrainingSet:
    [1.54106e+07,875.065,1350.46,12.155,0.980915,0.235389,2.50699,0.895724,0.197372] [2.05626e+07,1124.35,1454.73,17.52,2.99648,0.767386,7.98289,3.05663,1.75649]




.. GENERATED FROM PYTHON SOURCE LINES 50-53

Creation of metamodel
---------------------
We choose to use a constant trend.

.. GENERATED FROM PYTHON SOURCE LINES 53-56

.. code-block:: Python

    basis = ot.LinearBasisFactory(m.dim).build()









.. GENERATED FROM PYTHON SOURCE LINES 57-58

For the purpose of illustration, we consider :class:`~openturns.MaternModel`.

.. GENERATED FROM PYTHON SOURCE LINES 58-61

.. code-block:: Python

    covarianceModel = ot.MaternModel(inputTrainingSet.computeRange() * 0.1, 2.5)









.. GENERATED FROM PYTHON SOURCE LINES 62-65

Training without normalization
------------------------------
First, we deactivate the normalization option for the optimization.

.. GENERATED FROM PYTHON SOURCE LINES 65-67

.. code-block:: Python

    ot.ResourceMap.SetAsBool("GaussianProcessFitter-OptimizationNormalization", False)








.. GENERATED FROM PYTHON SOURCE LINES 68-69

We run the algorithm and get the metamodel.

.. GENERATED FROM PYTHON SOURCE LINES 69-78

.. code-block:: Python

    fitter_algo = ot.GaussianProcessFitter(
        inputTrainingSet, outputTrainingSet, covarianceModel, basis
    )
    fitter_algo.run()
    fitter_result = fitter_algo.getResult()
    gpr_algo = ot.GaussianProcessRegression(fitter_result)
    gpr_algo.run()
    gpr_result = gpr_algo.getResult()








.. GENERATED FROM PYTHON SOURCE LINES 79-80

Inspect hyperparameters

.. GENERATED FROM PYTHON SOURCE LINES 80-84

.. code-block:: Python

    theta = gpr_result.getCovarianceModel().getParameter()
    print("theta=", theta)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    theta= [515199,25.7338,12.2715,3.30446,2.33972,1.02203,3.20161,0.829491,0.387141,0.00376948]#10




.. GENERATED FROM PYTHON SOURCE LINES 85-87

Validation of metamodel
To validate the metamodel, we create a validation set of size equal to 50 times the input vector dimension to evaluate the functions.

.. GENERATED FROM PYTHON SOURCE LINES 87-93

.. code-block:: Python

    gprMetamodel = gpr_result.getMetaModel()
    ot.RandomGenerator.SetSeed(1)
    experimentTest = ot.LHSExperiment(inputDistribution, 50 * m.dim)
    inputTestSet = experimentTest.generate()
    outputTestSet = model(inputTestSet).getMarginal(0)








.. GENERATED FROM PYTHON SOURCE LINES 94-95

Then, we use the :class:`~openturns.MetaModelValidation` class to validate the metamodel.

.. GENERATED FROM PYTHON SOURCE LINES 95-100

.. code-block:: Python

    metamodelPredictions = gprMetamodel(inputTestSet)
    val = ot.MetaModelValidation(outputTestSet, metamodelPredictions)
    r2Score = val.computeR2Score()
    print("R2=", r2Score)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    R2= [0.6223]




.. GENERATED FROM PYTHON SOURCE LINES 101-102

Graphical validation

.. GENERATED FROM PYTHON SOURCE LINES 102-112

.. code-block:: Python

    label = "Accuracy of metamodel without activating normalization for optimization"
    graph = val.drawValidation().getGraph(0, 0)
    graph.setLegends([""])
    graph.setLegends(["R2 = %.2f%%" % (100 * r2Score[0]), ""])
    graph.setLegendPosition("upper left")
    graph.setXTitle("Exact function")
    graph.setYTitle("Metamodel prediction")
    graph.setTitle(label)
    _ = otv.View(graph)




.. image-sg:: /auto_surrogate_modeling/gaussian_process_regression/images/sphx_glr_plot_gpr_normalization_001.svg
   :alt: Accuracy of metamodel without activating normalization for optimization
   :srcset: /auto_surrogate_modeling/gaussian_process_regression/images/sphx_glr_plot_gpr_normalization_001.svg
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 113-116

Training with normalization
---------------------------
Then, we activate the normalization option for the optimization.

.. GENERATED FROM PYTHON SOURCE LINES 118-120

.. code-block:: Python

    ot.ResourceMap.SetAsBool("GaussianProcessFitter-OptimizationNormalization", True)








.. GENERATED FROM PYTHON SOURCE LINES 121-122

We run the algorithm and get the metamodel.

.. GENERATED FROM PYTHON SOURCE LINES 124-133

.. code-block:: Python

    fitter_algo = ot.GaussianProcessFitter(
        inputTrainingSet, outputTrainingSet, covarianceModel, basis
    )
    fitter_algo.run()
    fitter_result = fitter_algo.getResult()
    gpr_algo = ot.GaussianProcessRegression(fitter_result)
    gpr_algo.run()
    gpr_result = gpr_algo.getResult()








.. GENERATED FROM PYTHON SOURCE LINES 134-135

Inspect hyperparameters: we can see that parameters are much different this time

.. GENERATED FROM PYTHON SOURCE LINES 135-138

.. code-block:: Python

    theta = gpr_result.getCovarianceModel().getParameter()
    print("theta=", theta)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    theta= [1.0304e+07,498.57,208.542,10.73,4.03113,1.06399,10.9518,2.55495,0.896912,0.00186723]#10




.. GENERATED FROM PYTHON SOURCE LINES 139-140

Validation of metamodel

.. GENERATED FROM PYTHON SOURCE LINES 140-146

.. code-block:: Python

    gprMetamodel = gpr_result.getMetaModel()
    metamodelPredictions = gprMetamodel(inputTestSet)
    val = ot.MetaModelValidation(outputTestSet, metamodelPredictions)
    r2Score = val.computeR2Score()
    print("R2=", r2Score)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    R2= [0.999505]




.. GENERATED FROM PYTHON SOURCE LINES 147-148

Graphical validation

.. GENERATED FROM PYTHON SOURCE LINES 148-158

.. code-block:: Python

    label = "Accuracy of metamodel with activating normalization for optimization"
    graph2 = val.drawValidation().getGraph(0, 0)
    graph2.setLegends([""])
    graph2.setLegends(["R2 = %.2f%%" % (100 * r2Score[0]), ""])
    graph2.setLegendPosition("upper left")
    graph2.setXTitle("Exact function")
    graph2.setYTitle("Metamodel prediction")
    graph2.setTitle(label)
    _ = otv.View(graph2)
    otv.View.ShowAll()



.. image-sg:: /auto_surrogate_modeling/gaussian_process_regression/images/sphx_glr_plot_gpr_normalization_002.svg
   :alt: Accuracy of metamodel with activating normalization for optimization
   :srcset: /auto_surrogate_modeling/gaussian_process_regression/images/sphx_glr_plot_gpr_normalization_002.svg
   :class: sphx-glr-single-img






.. _sphx_glr_download_auto_surrogate_modeling_gaussian_process_regression_plot_gpr_normalization.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_gpr_normalization.ipynb <plot_gpr_normalization.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_gpr_normalization.py <plot_gpr_normalization.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_gpr_normalization.zip <plot_gpr_normalization.zip>`
