.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_numerical_methods_optimization_plot_ego.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_auto_numerical_methods_optimization_plot_ego.py:


EfficientGlobalOptimization examples
====================================

The EGO algorithm (Jones, 1998) is an adaptative optimization method based on
kriging.

An initial design of experiment is used to build a first metamodel.
At each iteration a new point that maximizes a criterion is chosen as
optimizer candidate.

The criterion uses a tradeoff between the metamodel value and the conditional
variance.

Then the new point is evaluated using the original model and the metamodel is
relearnt on the extended design of experiment.

Ackley test-case
----------------

Introduction
^^^^^^^^^^^^

The Ackley test case is a real function defined in dimension :math:`d` where :math:`d` is an integer. 

The Ackley function is defined by the equation:

.. math::
   f(\mathbf{x}) = -a \exp\left(-b\sqrt{\frac{1}{d}\sum_{i=1}^d}x_i^2\right)-\exp\left(\frac{1}{d}\sum_{i=1}^d \cos(c x_i)\right)+a+\exp(1)


for any :math:`\mathbf{x} \in [-15,15]^d`. However, we consider the smaller interval :math:`[-4,4]^d` in this example, for visual purposes.

We use the dimension :math:`d=2` with the parameters :math:`a=20`, :math:`b=0.2`, :math:`c=2\pi`. 

The solution is 

.. math::
   \mathbf{x}^\star=(0,0,...,0)


where 

.. math::
   f_{min} = f(\mathbf{x}^\star) = 0.


Define the problem
^^^^^^^^^^^^^^^^^^


.. code-block:: default

    import openturns as ot
    import openturns.viewer as viewer
    from matplotlib import pylab as plt
    import math as m
    ot.ResourceMap.SetAsString("KrigingAlgorithm-LinearAlgebra",  "LAPACK")
    ot.Log.Show(ot.Log.NONE)









.. code-block:: default

    dim = 2

    # model
    def ackley(X):
        a = 20.0
        b = 0.2
        c = 2.0 * m.pi
        d = len(X)
        sumOfSquared = sum(x**2 for x in X) / d
        sumOfCos = sum(m.cos(c * x) for x in X) / d
        f = - a * m.exp(- b * m.sqrt(sumOfSquared)) \
            - m.exp(sumOfCos) + a + m.exp(1.0)
        return [f]

    model = ot.PythonFunction(dim, 1, ackley)









.. code-block:: default

    lowerbound = ot.Point([-4.0] * dim)
    upperbound = ot.Point([4.0] * dim)









.. code-block:: default

    xexact = [0.0] * dim









.. code-block:: default

    fexact = model(xexact)
    fexact






.. raw:: html

    <p>[4.44089e-16]</p>
    <br />
    <br />


.. code-block:: default

    graph = model.draw(lowerbound, upperbound, [100]*dim)
    graph.setTitle("Ackley function")
    view = viewer.View(graph)




.. image:: /auto_numerical_methods/optimization/images/sphx_glr_plot_ego_001.png
    :alt: Ackley function
    :class: sphx-glr-single-img





We see that the Ackley function has many local minimas. The global minimum, however, is unique and located at the center of the domain. 

Create the initial kriging
^^^^^^^^^^^^^^^^^^^^^^^^^^

Before using the EGO algorithm, we must create an initial kriging. In order to do this, we must create a design of experiment which fills the space. In this situation, the `LHSExperiment` is a good place to start (but other design of experiments may allow to better fill the space). We use a uniform distribution in order to create a LHS design with 50 points. 


.. code-block:: default

    listUniformDistributions = [ot.Uniform(lowerbound[i], upperbound[i]) for i in range(dim)]
    distribution = ot.ComposedDistribution(listUniformDistributions)
    sampleSize = 50
    experiment = ot.LHSExperiment(distribution, sampleSize)
    inputSample = experiment.generate()
    outputSample = model(inputSample)









.. code-block:: default

    graph = ot.Graph("Initial LHS design of experiment - n=%d" % (sampleSize), ":math:`x_0`", ":math:`x_1`", True)
    cloud = ot.Cloud(inputSample)
    graph.add(cloud)
    view = viewer.View(graph)




.. image:: /auto_numerical_methods/optimization/images/sphx_glr_plot_ego_002.png
    :alt: Initial LHS design of experiment - n=50
    :class: sphx-glr-single-img





We now create the kriging metamodel. We selected the `SquaredExponential` covariance model with a constant basis (the `MaternModel` may perform better in this case). We use default settings (1.0) for the scale parameters of the covariance model, but configure the amplitude to 0.1, which better corresponds to the properties of the Ackley function. 


.. code-block:: default

    covarianceModel = ot.SquaredExponential([1.0] * dim, [0.5])
    basis = ot.ConstantBasisFactory(dim).build()
    kriging = ot.KrigingAlgorithm(inputSample, outputSample, covarianceModel, basis)
    kriging.run()








Create the optimization problem
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We finally create the `OptimizationProblem` and solve it with `EfficientGlobalOptimization`. 


.. code-block:: default

    problem = ot.OptimizationProblem()
    problem.setObjective(model)
    bounds = ot.Interval(lowerbound, upperbound)
    problem.setBounds(bounds)








In order to show the various options, we configure them all, even if most could be left to default settings in this case. 

The most important method is `setMaximumEvaluationNumber` which limits the number of iterations that the algorithm can perform. In the Ackley example, we choose to perform 10 iterations of the algorithm. 


.. code-block:: default

    algo = ot.EfficientGlobalOptimization(problem, kriging.getResult())
    algo.setMaximumEvaluationNumber(10)
    algo.run()
    result = algo.getResult()









.. code-block:: default

    result.getIterationNumber()





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    10




.. code-block:: default

    result.getOptimalPoint()






.. raw:: html

    <p>[-0.0470574,-0.0611692]</p>
    <br />
    <br />


.. code-block:: default

    result.getOptimalValue()






.. raw:: html

    <p>[0.370718]</p>
    <br />
    <br />


.. code-block:: default

    fexact






.. raw:: html

    <p>[4.44089e-16]</p>
    <br />
    <br />

Compared to the minimum function value, we see that the EGO algorithm provides solution which is not very accurate. However, the optimal point is in the neighbourhood of the exact solution, and this is quite an impressive success given the limited amount of function evaluations: only 60 evaluations for the initial DOE and 10 iterations of the EGO algorithm, for a total equal to 70 function evaluations. 


.. code-block:: default

    result.drawOptimalValueHistory()





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    class=Graph name=Optimal value history implementation=class=GraphImplementation name=Optimal value history title=Optimal value history xTitle=Iteration number yTitle=Optimal value axes=ON grid=ON legendposition=topright legendFontSize=1 drawables=[class=Drawable name=optimal value implementation=class=Curve name=optimal value derived from class=DrawableImplementation name=optimal value legend=optimal value data=class=Sample name=Unnamed implementation=class=SampleImplementation name=Unnamed size=10 dimension=2 data=[[0,4.47222],[1,4.23797],[2,3.30371],[3,3.30371],[4,3.30371],[5,3.30371],[6,1.51179],[7,1.51179],[8,1.51179],[9,0.370718]] color=red fillStyle=solid lineStyle=solid pointStyle=none lineWidth=1]




.. code-block:: default

    inputHistory = result.getInputSample()









.. code-block:: default

    graph = model.draw(lowerbound, upperbound, [100]*dim)
    graph.setLegends([""])
    graph.setTitle("Ackley function. Initial : black bullet. Solution : green diamond.")
    cloud = ot.Cloud(inputSample)
    cloud.setPointStyle("bullet")
    cloud.setColor("black")
    graph.add(cloud)
    cloud = ot.Cloud(inputHistory)
    cloud.setPointStyle("diamond")
    cloud.setColor("forestgreen")
    graph.add(cloud)
    view = viewer.View(graph)




.. image:: /auto_numerical_methods/optimization/images/sphx_glr_plot_ego_003.png
    :alt: Ackley function. Initial : black bullet. Solution : green diamond.
    :class: sphx-glr-single-img





We see that the initial (black) points are dispersed in the whole domain, while the solution points are much closer to the solution.

However, the final solution produced by the EGO algorithm is not very accurate. This is why we finalize the process by adding a local optimization step. 


.. code-block:: default

    algo2 = ot.NLopt(problem, 'LD_LBFGS')
    algo2.setStartingPoint(result.getOptimalPoint())
    algo2.run()
    result = algo2.getResult()









.. code-block:: default

    result.getOptimalPoint()






.. raw:: html

    <p>[4.58055e-07,7.97697e-07]</p>
    <br />
    <br />

The corrected solution is now extremely accurate. 


.. code-block:: default

    graph = result.drawOptimalValueHistory()
    view = viewer.View(graph)




.. image:: /auto_numerical_methods/optimization/images/sphx_glr_plot_ego_004.png
    :alt: Optimal value history
    :class: sphx-glr-single-img





Branin test-case
----------------

Introduction
^^^^^^^^^^^^

The Branin function is defined in 2 dimensions based on the functions :math:`g`:

.. math:: 
   g(u_1, u_2) = \frac{\left(u_2-5.1\frac{u_1^2}{4\pi^2}+5\frac{u_1}{\pi}-6\right)^2+10\left(1-\frac{1}{8 \pi}\right)  \cos(u_1)+10-54.8104}{51.9496}


and :math:`t`:

.. math:: 
   t(x_1, x2) = (15 x_1 - 5, 15 x_2)^T.


Finally, the Branin function is the composition of the two previous functions:

.. math::
   f_{Branin}(x_1, x_2) = g \circ  t(x_1, x_2)


for any :math:`\mathbf{x} \in [0, 1]^2`. 

There are three global minimas:

.. math::
   \mathbf{x}^\star=(0.123895, 0.818329),


.. math::
   \mathbf{x}^\star=(0.542773, 0.151666),


and :

.. math::
   \mathbf{x}^\star=(0.961652, 0.165000)


where the function value is:

.. math::
   f_{min} = f_{Branin}(\mathbf{x}^\star) = -0.97947643837.



We assume that the output of the Branin function is noisy, with a gaussian noise. 
In other words, the objective function is:

.. math::
   f(x_1, x_2) = f_{Branin}(x_1, x_2) + \epsilon


where :math:`\epsilon` is a random variable with gaussian distribution. 

This time the AEI formulation is used, meaning that the objective has two outputs: the first one is the objective function value and the second one is the noise variance.

Here we assume a constant noise variance: 

.. math::
   \sigma_{\epsilon} = 0.1.


Define the problem
^^^^^^^^^^^^^^^^^^


.. code-block:: default

    dim = 2

    trueNoiseFunction = 0.1
    # model
    branin = ot.SymbolicFunction(['x1', 'x2'], 
                                 ['((x2-(5.1/(4*pi_^2))*x1^2+5*x1/pi_-6)^2+10*(1-1/8*pi_)*cos(x1)+10-54.8104)/51.9496', 
                                  str(trueNoiseFunction)])
    transfo = ot.SymbolicFunction(['u1', 'u2'], 
                                  ['15*u1-5', '15*u2'])
    model = ot.ComposedFunction(branin, transfo)









.. code-block:: default

    lowerbound = ot.Point([0.0] * dim)
    upperbound = ot.Point([1.0] * dim)









.. code-block:: default

    objectiveFunction = model.getMarginal(0)









.. code-block:: default

    xexact1 = ot.Point([0.123895,0.818329])
    xexact2 = ot.Point([0.542773,0.151666])
    xexact3 = ot.Point([0.961652,0.165000])
    xexact = ot.Sample([xexact1, xexact2, xexact3])









.. code-block:: default

    fexact = objectiveFunction(xexact)
    fexact






.. raw:: html

    <TABLE><TR><TD></TD><TH>y0</TH></TR>
    <TR><TH>0</TH><TD>-0.9794764</TD></TR>
    <TR><TH>1</TH><TD>-0.9794764</TD></TR>
    <TR><TH>2</TH><TD>-0.9794764</TD></TR>
    </TABLE>
    <br />
    <br />


.. code-block:: default

    graph = objectiveFunction.draw(lowerbound, upperbound, [100]*dim)
    graph.setTitle("Branin function")
    view = viewer.View(graph)




.. image:: /auto_numerical_methods/optimization/images/sphx_glr_plot_ego_005.png
    :alt: Branin function
    :class: sphx-glr-single-img





The Branin function has three local minimas. 

Create the initial kriging
^^^^^^^^^^^^^^^^^^^^^^^^^^


.. code-block:: default

    distribution = ot.ComposedDistribution([ot.Uniform(0.0, 1.0)] * dim)
    sampleSize = 50
    experiment = ot.LHSExperiment(distribution, sampleSize)
    inputSample = experiment.generate()
    modelEval = model(inputSample)
    outputSample = modelEval.getMarginal(0)









.. code-block:: default

    graph = ot.Graph("Initial LHS design of experiment - n=%d" % (sampleSize), ":math:`x_0`", ":math:`x_1`", True)
    cloud = ot.Cloud(inputSample)
    graph.add(cloud)
    view = viewer.View(graph)




.. image:: /auto_numerical_methods/optimization/images/sphx_glr_plot_ego_006.png
    :alt: Initial LHS design of experiment - n=50
    :class: sphx-glr-single-img






.. code-block:: default

    covarianceModel = ot.SquaredExponential([1.0] * dim, [1.0])
    basis = ot.ConstantBasisFactory(dim).build()
    kriging = ot.KrigingAlgorithm(inputSample, outputSample, covarianceModel, basis)









.. code-block:: default

    noise = [x[1] for x in modelEval]
    kriging.setNoise(noise)
    kriging.run()








Create and solve the problem
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

problem


.. code-block:: default

    problem = ot.OptimizationProblem()
    problem.setObjective(model)
    bounds = ot.Interval(lowerbound, upperbound)
    problem.setBounds(bounds)








We configure the maximum number of function evaluations to 20. We assume that the function is noisy, with a constant variance. 

algo


.. code-block:: default

    algo = ot.EfficientGlobalOptimization(problem, kriging.getResult())
    # assume constant noise var
    guessedNoiseFunction = 0.1
    noiseModel = ot.SymbolicFunction(['x1', 'x2'], [str(guessedNoiseFunction)])
    algo.setNoiseModel(noiseModel) 
    algo.setMaximumEvaluationNumber(20)
    algo.run()
    result = algo.getResult()









.. code-block:: default

    result.getIterationNumber()





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    20




.. code-block:: default

    result.getOptimalPoint()






.. raw:: html

    <p>[0.131883,0.773615]</p>
    <br />
    <br />


.. code-block:: default

    result.getOptimalValue()






.. raw:: html

    <p>[-0.97579]</p>
    <br />
    <br />


.. code-block:: default

    fexact






.. raw:: html

    <TABLE><TR><TD></TD><TH>y0</TH></TR>
    <TR><TH>0</TH><TD>-0.9794764</TD></TR>
    <TR><TH>1</TH><TD>-0.9794764</TD></TR>
    <TR><TH>2</TH><TD>-0.9794764</TD></TR>
    </TABLE>
    <br />
    <br />


.. code-block:: default

    inputHistory = result.getInputSample()









.. code-block:: default

    graph = objectiveFunction.draw(lowerbound, upperbound, [100]*dim)
    graph.setLegends([""])
    graph.setTitle("Branin function. Initial : black bullet. Solution : green diamond.")
    cloud = ot.Cloud(inputSample)
    cloud.setPointStyle("bullet")
    cloud.setColor("black")
    graph.add(cloud)
    cloud = ot.Cloud(inputHistory)
    cloud.setPointStyle("diamond")
    cloud.setColor("forestgreen")
    graph.add(cloud)
    view = viewer.View(graph)




.. image:: /auto_numerical_methods/optimization/images/sphx_glr_plot_ego_007.png
    :alt: Branin function. Initial : black bullet. Solution : green diamond.
    :class: sphx-glr-single-img





We see that the EGO algorithm found the second local minimum. Given the limited number of function evaluations, the other local minimas have not been explored. 


.. code-block:: default

    graph = result.drawOptimalValueHistory()
    view = viewer.View(graph)

    plt.show()



.. image:: /auto_numerical_methods/optimization/images/sphx_glr_plot_ego_008.png
    :alt: Optimal value history
    :class: sphx-glr-single-img






.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  6.900 seconds)


.. _sphx_glr_download_auto_numerical_methods_optimization_plot_ego.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_ego.py <plot_ego.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_ego.ipynb <plot_ego.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
