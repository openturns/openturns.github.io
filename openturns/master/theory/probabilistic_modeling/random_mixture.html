
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Random Mixture: affine combination of independent univariate distributions &#8212; OpenTURNS 1.19dev documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/openturns.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/mysearchtools.js"></script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Stochastic process definitions" href="process_definitions.html" />
    <link rel="prev" title="Copulas" href="copulas.html" />
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300,400,700'
          rel='stylesheet' type='text/css' />
 

  </head><body>
<div class="pageheader">
  <ul>
    <li><a href="http://www.openturns.org/">Home</a></li>
    <li><a href="../../install.html">Get it</a></li>
    <li><a href="../../contents.html">Doc</a></li>
    <li><a href="https://openturns.discourse.group/">Forum</a></li>
    <li><a href="https://gitter.im/openturns/community">Chat</a></li>
    <li><a href="https://github.com/openturns/openturns/wiki/Modules">Modules</a></li>
    <li><a href="https://github.com/openturns">Code</a></li>
    <li><a href="https://github.com/openturns/openturns/issues">Bugs</a></li>
  </ul>
  <a href="../../index.html">
    <h1>
      <img src="../../_static/logo-openturns-wo-bg.png" alt="" width=100px height=100px />
      OpenTURNS
    </h1>
    <h2> An Open source initiative for the Treatment of Uncertainties, Risks'N Statistics</h2>
  </a>
</div>

    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="process_definitions.html" title="Stochastic process definitions"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="copulas.html" title="Copulas"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">OpenTURNS 1.19dev documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../contents.html" >Contents</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../theory.html" >Theory</a> &#187;</li>
          <li class="nav-item nav-item-3"><a href="probabilistic_modeling.html" accesskey="U">Probabilistic modeling</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Random Mixture: affine combination of independent univariate distributions</a></li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <div>
    <h4>Previous topic</h4>
    <p class="topless"><a href="copulas.html"
                          title="previous chapter">Copulas</a></p>
  </div>
  <div>
    <h4>Next topic</h4>
    <p class="topless"><a href="process_definitions.html"
                          title="next chapter">Stochastic process definitions</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../../_sources/theory/probabilistic_modeling/random_mixture.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="random-mixture-affine-combination-of-independent-univariate-distributions">
<span id="random-mixture"></span><h1>Random Mixture: affine combination of independent univariate distributions<a class="headerlink" href="#random-mixture-affine-combination-of-independent-univariate-distributions" title="Permalink to this headline">¶</a></h1>
<p>A multivariate random variable <img class="math" src="../../_images/math/f226966057cd198bc5b8e728a3f9b629b5f15b2f.svg" alt="\vect{Y}"/> may be defined as an
affine transform of <img class="math" src="../../_images/math/80b394abd4fb264a3879675f92f191c3e346c3a0.svg" alt="n"/> independent univariate random variable, as
follows:</p>
<div class="math" id="equation-randommixtureformula">
<p><span class="eqno">(1)<a class="headerlink" href="#equation-randommixtureformula" title="Permalink to this equation">¶</a></span><img src="../../_images/math/de1a16d576d6ccb0d1ef2173574ab8eea01c719f.svg" alt="\displaystyle \vect{Y}=\vect{y}_0+\mat{M}\,\vect{X}"/></p>
</div><p>where <img class="math" src="../../_images/math/19398389488af85a724c4bb7decd241c28181842.svg" alt="\vect{y}_0\in\mathbb{R}^d"/> is a deterministic vector with
<img class="math" src="../../_images/math/1d55f9058b6781e8a835583b45f6ae21d568830c.svg" alt="d\in\{1,2,3\}"/>, <img class="math" src="../../_images/math/6f55abb68ec5e51ff90e0a6b5b963fbdb0d1f63a.svg" alt="\mat{M}\in\mathcal{M}_{d,n}(\mathbb{R})"/> a
deterministic matrix and <img class="math" src="../../_images/math/710eacca29ec11e69e139e22ebdb101a313b7344.svg" alt="(X_k)_{ 1 \leq k \leq n}"/> are some
independent univariate distributions.</p>
<p>In such a case, it is possible to evaluate directly the distribution of
<img class="math" src="../../_images/math/f226966057cd198bc5b8e728a3f9b629b5f15b2f.svg" alt="\vect{Y}"/> and then to ask <img class="math" src="../../_images/math/f226966057cd198bc5b8e728a3f9b629b5f15b2f.svg" alt="\vect{Y}"/> any request compatible
with a distribution: moments, probability and cumulative density
functions, quantiles (in dimension 1 only) …</p>
<p><strong>Evaluation of the probability density function of the Random Mixture</strong></p>
<p>As the univariate random variables <img class="math" src="../../_images/math/56c02f12d2f1519026c2ada380fc921e0d8a0a69.svg" alt="X_i"/> are independent, the
characteristic function of <img class="math" src="../../_images/math/f226966057cd198bc5b8e728a3f9b629b5f15b2f.svg" alt="\vect{Y}"/>, denoted <img class="math" src="../../_images/math/14a7f517746e5fd82769673cf37b328458a2ffba.svg" alt="\phi_Y"/>, is
easily defined from the characteristic function of <img class="math" src="../../_images/math/4394acaf9636e03e33fb9c9a027ad91bb35bdc98.svg" alt="X_k"/> denoted
<img class="math" src="../../_images/math/52fb71a52c9547b7ca45a68df9c0184f8cfa1e1f.svg" alt="\phi_{X_k}"/> as follows :</p>
<div class="math" id="equation-charactfuncy">
<p><span class="eqno">(2)<a class="headerlink" href="#equation-charactfuncy" title="Permalink to this equation">¶</a></span><img src="../../_images/math/5276dc33a1722a35110e6b024301e687f05168c8.svg" alt="\displaystyle \phi_Y(u_1,\hdots,u_d)=\prod_{j=1}^de^{iu_j{y_0}_j}\prod_{k=1}^n\phi_{X_k}((M^tu)_k), \mbox{  for } \vect{u}\in\mathbb{R}^d"/></p>
</div><div class="line-block">
<div class="line">Once <img class="math" src="../../_images/math/14a7f517746e5fd82769673cf37b328458a2ffba.svg" alt="\phi_Y"/> evaluated, it is possible to evaluate the
probability density function of <img class="math" src="../../_images/math/d58d4863379b3eb734fd9d329f850fe196c3abc5.svg" alt="Y"/>, denoted <img class="math" src="../../_images/math/161a3dcd3ded2b04cad6b9b6f187a21ac34b794b.svg" alt="p_Y"/> :
several techniques are possible, as the inversion of the Fourier
transformation. This technique is not easy to implement.</div>
<div class="line">Another technique is used, based on the Poisson sum
formulation, defined as follows:</div>
</div>
<blockquote>
<div><div class="math" id="equation-poissonsum">
<p><span class="eqno">(3)<a class="headerlink" href="#equation-poissonsum" title="Permalink to this equation">¶</a></span><img src="../../_images/math/bf5ec4f35ae5144f26ef9df6b67890867813451a.svg" alt="\displaystyle \sum_{j_1\in\mathbb{Z}}\hdots\sum_{j_d\in\mathbb{Z}} p_Y\left(y_1+\frac{2\pi j_1}{h_1},\hdots,y_d+\frac{2\pi j_d}{h_d}\right)=
     \prod_{j=1}^d \frac{h_j}{2*\pi}\sum_{k_1\in\mathbb{Z}}\hdots\sum_{k_d\in\mathbb{Z}}\phi\left(k_1h_1,\hdots,k_dh_d\right)e^{-\imath(\sum_{m=1}^{d}k_m h_m y_m)}"/></p>
</div></div></blockquote>
<div class="line-block">
<div class="line">By fixing <img class="math" src="../../_images/math/fcf9af1cd4fa374da65a2c23980d1afa2afd3863.svg" alt="h_1,\hdots,h_d"/> small enough,
<img class="math" src="../../_images/math/2e938374c883368cedf06db5e3238b709baffa41.svg" alt="\frac{2k\pi}{h_j} \approx +\infty"/> and
<img class="math" src="../../_images/math/8ee65fa6fca2bb115a6472d2a731d21291cb9181.svg" alt="p_Y(\hdots,\frac{2k\pi}{h_j},\hdots) \approx 0"/> because of the
decreasing properties of <img class="math" src="../../_images/math/161a3dcd3ded2b04cad6b9b6f187a21ac34b794b.svg" alt="p_Y"/>. Thus the nested sums of the left
term of <a class="reference internal" href="#equation-poissonsum">(3)</a> are reduced to the central term
<img class="math" src="../../_images/math/8895ed051886ed5ca73eca64fe575b7bfd2d8a09.svg" alt="j_1=\hdots=j_d = 0"/>: the left term is approximatively equal to
<img class="math" src="../../_images/math/9388a11d1a6cff6e3be1cd13f6afd91f9db2b316.svg" alt="p_Y(y)"/>.</div>
<div class="line">Furthermore, the right term of <a class="reference internal" href="#equation-poissonsum">(3)</a> is a series which
converges very fast: only few terms of the series are enough to get
machine-precision accuracy. Let us note that the factors
<img class="math" src="../../_images/math/e4954522add2db1460b4507bbd5dec8f1e8085cc.svg" alt="\phi_Y(k_1 h_1,\hdots,k_d,h_d)"/>, which are expensive to
evaluate, do not depend on <img class="math" src="../../_images/math/7887520f8387f92cf22143f7bdce04904c79e3ef.svg" alt="y"/> and are evaluated once only.</div>
</div>
<div class="line-block">
<div class="line">It is also possible to greatly improve the performance of the
algorithm by noticing that equation  is linear between <img class="math" src="../../_images/math/161a3dcd3ded2b04cad6b9b6f187a21ac34b794b.svg" alt="p_Y"/> and
<img class="math" src="../../_images/math/14a7f517746e5fd82769673cf37b328458a2ffba.svg" alt="\phi_Y"/>. We denote <img class="math" src="../../_images/math/ea57e33dd83541897d0db04f4a6d91db0a1fccdf.svg" alt="q_Y"/> and <img class="math" src="../../_images/math/656c1bb738dd3757eb02b6a00991edb267e96960.svg" alt="\psi_Y"/> respectively
the density and the characteristic function of the multivariate normal
distribution with the same mean <img class="math" src="../../_images/math/c3c1d698a41f95f7391a7e8153cadab64fae08a0.svg" alt="\vect{\mu}"/> and same covariance
matrix <img class="math" src="../../_images/math/b4ac9842ac1b1dda0ebab3c2337777ff7932cc24.svg" alt="\vect{C}"/> as the random mixture. By applying this
multivariate normal distribution to the equation , we obtain by
subtraction:</div>
</div>
<blockquote>
<div><div class="math" id="equation-algopoisson">
<p><span class="eqno">(4)<a class="headerlink" href="#equation-algopoisson" title="Permalink to this equation">¶</a></span><img src="../../_images/math/93b6bd8781c0482c5b57e507f2ea0d8eb08ee68b.svg" alt="\displaystyle  p_Y\left(y\right) = \sum_{j\in\mathbb{Z}^d} q_Y\left(y_1+\frac{2\pi j_1}{h_1},\cdots,y_d+\frac{2\pi j_d}{h_d}\right)+
   \frac{H}{2^d\pi^d}\sum_{|k_1|\leq N}\cdots\sum_{|k_d|\leq N} \delta_Y\left(k_1h_1,\cdots,k_dh_d\right)e^{-\imath(\sum_{m=1}^{d}k_m h_m y_m)}"/></p>
</div></div></blockquote>
<p>where <img class="math" src="../../_images/math/c7493e74841f72d5047f8160e299c5c0053380f5.svg" alt="H = h_1\times\cdots\times h_d"/>,
<img class="math" src="../../_images/math/ddc4728e6360b024d0e96775588e3062d5687a4a.svg" alt="j=(j_1,\cdots,j_d)"/>, <img class="math" src="../../_images/math/d850ad270990e9ef38946136e62749c81a542a78.svg" alt="\delta_Y:=\phi_Y - \psi_Y"/></p>
<div class="line-block">
<div class="line">In the case where <img class="math" src="../../_images/math/c123cefcac3b5c50320b9f2e4d4a0efd75b12a87.svg" alt="n \gg 1"/>, using the limit central theorem,
the law of <img class="math" src="../../_images/math/f226966057cd198bc5b8e728a3f9b629b5f15b2f.svg" alt="\vect{Y}"/> tends to the normal distribution density
<img class="math" src="../../_images/math/0673f3d4e1420605f352372985b6a22266d58cd4.svg" alt="q"/>, which will drastically reduce <img class="math" src="../../_images/math/6bed8a44f63e1eb8e61608a5c93374a505943eec.svg" alt="N"/>. The sum on
<img class="math" src="../../_images/math/0673f3d4e1420605f352372985b6a22266d58cd4.svg" alt="q"/> will become the most CPU-intensive part, because in the
general case we will have to keep more terms than the central one in
this sum, since the parameters <img class="math" src="../../_images/math/686a6e70a17fc782a96c9769635a3ee943421187.svg" alt="h_1, \dots  h_d"/> were
calibrated with respect to <img class="math" src="../../_images/math/17b12496a489ee11edbaa2998e911d495fddf5be.svg" alt="p"/> and not <img class="math" src="../../_images/math/0673f3d4e1420605f352372985b6a22266d58cd4.svg" alt="q"/>.</div>
</div>
<p>The parameters <img class="math" src="../../_images/math/686a6e70a17fc782a96c9769635a3ee943421187.svg" alt="h_1, \dots  h_d"/> are calibrated using the
following formula:</p>
<div class="math">
<p><img src="../../_images/math/bf8c751c75b74aa881ce218560ded278c6b8434f.svg" alt="h_\ell = \frac{2\pi}{(\beta+4\alpha)\sigma_\ell}"/></p>
</div><p>where <img class="math" src="../../_images/math/d3fa4ef8b485f5866b634ee3101eff47a89becd6.svg" alt="\sigma_\ell=\sqrt{\Cov{\vect{Y}}_{\ell,\ell}}"/> and
<img class="math" src="../../_images/math/586e8e332570955fa6dfcb1ba54f5ce2823c0fb7.svg" alt="\alpha"/>, <img class="math" src="../../_images/math/233b263354a1b8b4463fe36dd42563b01bed21f3.svg" alt="\beta"/> are respectively the number of standard
deviations covered by the marginal distribution (<img class="math" src="../../_images/math/c8781378d035f0ff3d2e6931ab2492e08f775405.svg" alt="\alpha=5"/> by
default) and <img class="math" src="../../_images/math/233b263354a1b8b4463fe36dd42563b01bed21f3.svg" alt="\beta"/> the number of marginal deviations beyond
which the density is negligible (<img class="math" src="../../_images/math/4f7e722c4fa023785f6baf4b7a6a1e3cbefb3bd2.svg" alt="\beta=8.5"/> by default).</p>
<p>The <img class="math" src="../../_images/math/6bed8a44f63e1eb8e61608a5c93374a505943eec.svg" alt="N"/> parameter is dynamically calibrated: we start with
<img class="math" src="../../_images/math/0871191d4f2d2388741c3238ad35d0a4b57c825c.svg" alt="N=8"/> then we double <img class="math" src="../../_images/math/6bed8a44f63e1eb8e61608a5c93374a505943eec.svg" alt="N"/> value until the total contribution
of the additional terms is negligible.</p>
<p><strong>Evaluation of the moments of the Random Mixture</strong></p>
<p>The relation <a class="reference internal" href="#equation-randommixtureformula">(1)</a> enables to evaluate all the
moments of the random mixture, if mathematically defined. For example,
we have:</p>
<div class="math">
<p><img src="../../_images/math/108bf064615d5b921eec275b696a92efd7125a24.svg" alt="\left\{
\begin{array}{lcl}
  \Expect{\vect{Y}} &amp; = &amp; \vect{y_0} + \mat{M}\Expect{\vect{X}} \\
  \Cov{\vect{Y}} &amp; = &amp; \mat{M}\,\Cov{\vect{X}}\mat{M}^t
\end{array}\right\}"/></p>
</div><p><strong>Computation on a regular grid</strong></p>
<p>The interest is to compute the density function on a regular grid.
Purposes are to get an approximation quickly. The regular grid is of
form:</p>
<div class="math">
<p><img src="../../_images/math/dba3d9761aadb571a686711b2cbee7963fcd09df.svg" alt="\begin{aligned}
    \forall r\in\{1,\hdots,d\},\forall m\in\{0,\hdots,M-1\},\:y_{r,m}=\mu_r+b\left(\frac{2m+1}{M} - 1\right)\sigma_r
  \end{aligned}"/></p>
</div><p>By denoting <img class="math" src="../../_images/math/7054d598a58ad380621f552ee6f26a21d3019cff.svg" alt="p_{m_1,\hdots,m_d}=p_{\vect{Y}}(y_{1,m_1},\hdots,y_{d,m_d})"/>:</p>
<div class="math">
<p><img src="../../_images/math/1192e55ebacc729408239316634f69d3fba05d8b.svg" alt="\begin{aligned}
    p_{m_1,\hdots,m_d}= Q_{m_1,\hdots,m_d}+S_{m_1,\hdots,m_d}
  \end{aligned}"/></p>
</div><p>for which the term <img class="math" src="../../_images/math/0092c1450eb87dbb5074037e6acb0f981865062d.svg" alt="S_{m_1,\hdots,m_d}"/> is the most CPU
consuming. This term rewrites:</p>
<div class="math">
<p><img src="../../_images/math/9ff25a174276614cfd61bb300c653f2279bb5d3a.svg" alt="\begin{aligned}
  S_{m_1,\hdots,m_d}=&amp;\frac{H}{2^d\pi^d}\sum_{k_1=-N}^{N}\hdots\sum_{k_d=-N}^{N}\delta\left(k_1h_1,\hdots,k_dh_d\right)
  E_{m_1,\hdots,m_d}(k_1,\hdots,k_d) \label{Eq:S}
  \end{aligned}"/></p>
</div><p>with:</p>
<div class="math">
<p><img src="../../_images/math/f673b4c2c4856a96c862e6bf7fdff123866f7f9f.svg" alt="\begin{aligned}
    \delta\left(k_1h_1,\hdots,k_dh_d\right)&amp;=(\phi-\psi)\left(k_1h_1,\hdots,k_dh_d\right)\\
    E_{m_1,\hdots,m_d}(k_1,\hdots,k_d)&amp;=e^{-i\sum_{j=1}^d k_jh_j\left(\mu_j+b\left(\frac{2m_j+1}{M}-1\right)\sigma_j\right)}
  \end{aligned}"/></p>
</div><p>The aim is to rewrite the previous expression as a <img class="math" src="../../_images/math/5da6e6d65e4ae3032c58fe8c94955247fc4ebe22.svg" alt="d"/>- discrete
Fourier transform, in order to apply Fast Fourier Transform (<em>FFT</em>) for
its evaluation.</p>
<p>We set <img class="math" src="../../_images/math/f291427b7c53ab2038a3c226d0f5ce404cb8f093.svg" alt="M=N"/> and
<img class="math" src="../../_images/math/5a6767736991f70577128ab5b0d0ba9c72af3687.svg" alt="\forall j \in\{1,\hdots,d\},\: h_j=\frac{\pi}{b\sigma_j}"/> and
<img class="math" src="../../_images/math/3367eddd84031693a4dd080dfeb5a6dd51b3f10f.svg" alt="\tau_j=\frac{\mu_j}{b\sigma_j}"/>. For convenience, we introduce
the functions:</p>
<div class="math">
<p><img src="../../_images/math/f0ac185155d7c92b0c46eaedacf79caa29978012.svg" alt="f_j(k) = e^{-i\pi (k+1)\left(\tau_j-1+\frac{1}{N}\right)}"/></p>
</div><p>We use <img class="math" src="../../_images/math/123a51471a8326c3c944ca12fb47e00ec032b095.svg" alt="k+1"/> instead of <img class="math" src="../../_images/math/14feef167e3dd938706565e34c0c9ac19065a615.svg" alt="k"/> in this function to simplify
expressions below.</p>
<p>We obtain:</p>
<div class="math">
<p><img src="../../_images/math/19599672c804e97c6aa65ea871cc0b30e0d307cc.svg" alt="\begin{aligned}
  E_{m_1,\hdots,m_d}(k_1,\hdots,k_d)&amp;=e^{-i\sum_{j=1}^{d} k_jh_jb\sigma_j\left(\frac{\mu_j}{b\sigma_j}+\frac{2m_j}{N}+\frac{1}{N}-1\right)}\notag\\
    &amp;=e^{-2i\pi\left(\frac{\sum_{j=1}^{d}k_j m_j}{N}\right)}e^{-i\pi\sum_{j=1}^{d} k_j\left(\tau_j-1+\frac{1}{N}\right)} \notag\\
    &amp;=e^{-2i\pi\left(\frac{\sum_{j=1}^{d}k_j m_j}{N}\right)} f_1(k_1-1) \times\hdots\times f_d(k_d-1) \label{Eq:E}
  \end{aligned}"/></p>
</div><p>For performance reasons, we want to use the discrete Fourier transform
with the following convention in dimension 1:</p>
<div class="math">
<p><img src="../../_images/math/4af46495add027eb3e1af42c513dd2ae4ad9edfe.svg" alt="A_m = \sum_{k=0}^{N-1} a_k e^{-2i\pi\frac{km}{N}}"/></p>
</div><p>which extension to dimensions 2 and 3 are respectively:</p>
<div class="math">
<p><img src="../../_images/math/6452000c6fd6fe2072aeb77e8931a46748f55669.svg" alt="A_{m,n} = \sum_{k=0}^{N-1}\sum_{l=0}^{N-1} a_{k,l} e^{-2i\pi\frac{km}{N}} e^{-2i\pi\frac{ln}{N}}\\"/></p>
</div><div class="math">
<p><img src="../../_images/math/6568c234568f722f77731e66d24145e4862be26f.svg" alt="A_{m,n,p} = \sum_{k=0}^{N-1}\sum_{l=0}^{N-1}\sum_{s=0}^{N-1} a_{k,l,s} e^{-2i\pi\frac{km}{N}} e^{-2i\pi\frac{ln}{N}} e^{-2i\pi\frac{sp}{N}}"/></p>
</div><p>We decompose sums of  on the interval <img class="math" src="../../_images/math/66b6f0823cb0988ba032bba6304faa198ee75e5e.svg" alt="[-N,N]"/> into three parts:</p>
<div class="math" id="equation-decomposition-sum">
<p><span class="eqno">(5)<a class="headerlink" href="#equation-decomposition-sum" title="Permalink to this equation">¶</a></span><img src="../../_images/math/75a524e3a6405f8c924995b7013ae1d368fdeebc.svg" alt="\begin{aligned}
    \sum_{k_j=-N}^{N}\delta\left(k_1h_1,\hdots,k_dh_d\right) E_{m_1,\hdots,m_d}(k_1,\hdots,k_d)
      = &amp; \sum_{k_j=-N}^{-1} \delta\left(k_1h_1,\hdots,k_dh_d\right) E_{m_1,\hdots,m_d}(k_1,\hdots,k_d) \notag\\
      &amp; + \delta\left(k_1h_1,\hdots,0,\hdots,k_dh_d\right) E_{m_1,\hdots,0,\hdots,m_d}(k_1,\hdots,0,\hdots,k_d) \notag\\
      &amp; + \sum_{k_j=1}^{N}\delta\left(k_1h_1,\hdots,k_dh_d\right) E_{m_1,\hdots,m_d}(k_1,\hdots,k_d)
    \end{aligned}"/></p>
</div><p>If we already computed <img class="math" src="../../_images/math/7b093bea7d8744f39f2a40eafb4574ed61b97fc5.svg" alt="E"/> for dimension <img class="math" src="../../_images/math/dd1baf718707fc6803c525f5a912e10626f8d4b2.svg" alt="d-1"/>, then the
middle term in this sum is trivial.</p>
<p>To compute the last sum of equation, we apply a change of variable
<img class="math" src="../../_images/math/4cd3d049cd8d573509f1bc98747e028a3b17623f.svg" alt="k_j'=k_j-1"/>:</p>
<div class="math">
<p><img src="../../_images/math/a2db70ce9ad97f17bc9caedf55d0c9c4c5db1b56.svg" alt="\begin{aligned}
  \sum_{k_j=1}^{N}\delta\left(k_1h_1,\hdots,k_dh_d\right) E_{m_1,\hdots,m_d}(k_1,\hdots,k_d)
  = &amp; \sum_{k_j=0}^{N-1}\delta\left(k_1h_1,\hdots,(k_j+1)h_j,\hdots,k_dh_d\right) \times\notag\\
    &amp; \hspace*{3cm} E_{m_1,\hdots,m_d}(k_1,\hdots,k_j+1,\hdots,k_d)
  \end{aligned}"/></p>
</div><p>Equation gives:</p>
<div class="math">
<p><img src="../../_images/math/c91f6defbec746713c2e522100ac736920b39513.svg" alt="\begin{aligned}
  E_{m_1,\hdots,m_d}(k_1,\hdots,k_j+1,\hdots,k_d)
  &amp;=
      e^{-2i\pi\left(\frac{\sum_{l=1}^{d}k_l m_l}{N} +\frac{m_j}{N}\right)}
      f_1(k_1-1)\times\hdots\times f_j(k_j)\times\hdots\times f_d(k_d-1)\notag\\
  &amp;=
      e^{-2i\pi\left(\frac{m_j}{N}\right)}
      e^{-2i\pi\left(\frac{\sum_{l=1}^{d}k_l m_l}{N}\right)}
      f_1(k_1-1)\times\hdots\times f_j(k_j)\times\hdots\times f_d(k_d-1)
  \end{aligned}"/></p>
</div><p>Thus</p>
<div class="math">
<p><img src="../../_images/math/6ee50eefe52457e4e4d3a13c7c6e1e57c4007fc9.svg" alt="\begin{aligned}
  \sum_{k_j=1}^{N}\delta\left(k_1h_1,\hdots,k_dh_d\right) E_{m_1,\hdots,m_d}&amp;(k_1,\hdots,k_d)
    = e^{-2i\pi\left(\frac{m_j}{N}\right)} \sum_{k_j=0}^{N-1}\delta\left(k_1h_1,\hdots,(k_j+1)h_j,\hdots,k_dh_d\right) \times\notag\\
    &amp; e^{-2i\pi\left(\frac{\sum_{l=1}^{d}k_l m_l}{N}\right)}
      f_1(k_1-1)\times\hdots\times f_j(k_j)\times\hdots\times f_d(k_d-1) \label{Eq:j-sigma+}
  \end{aligned}"/></p>
</div><p>To compute the first sum of equation, we apply a change of variable
<img class="math" src="../../_images/math/5f8d4ac3593c6f8b88c02186de4e142d5fb5b658.svg" alt="k_j'=N+k_j"/>:</p>
<div class="math">
<p><img src="../../_images/math/dae3a9756b8fee8bbee9d820e426de44cde9b431.svg" alt="\begin{aligned}
  \sum_{k_j=-N}^{-1}\delta\left(k_1h_1,\hdots,k_dh_d\right) E_{m_1,\hdots,m_d}(k_1,\hdots,k_d)
  = &amp; \sum_{k_j=0}^{N-1}\delta\left(k_1h_1,\hdots,(k_j-N)h_j,\hdots,k_dh_d\right) \times\notag\\
    &amp; \hspace*{3cm} E_{m_1,\hdots,m_d}(k_1,\hdots,k_j-N,\hdots,k_d)
  \end{aligned}"/></p>
</div><p>Equation  gives:</p>
<div class="math">
<p><img src="../../_images/math/3adb8d3c0140649d3f9168bd5eb36dde7b94c91a.svg" alt="\begin{aligned}
  E_{m_1,\hdots,m_d}(k_1,\hdots,k_j-N,\hdots,k_d)
  &amp;=
      e^{-2i\pi\left(\frac{\sum_{l=1}^{d}k_l m_l}{N} -m_j\right)}
      f_1(k_1-1)\times\hdots\times f_j(k_j-1-N)\times\hdots\times f_d(k_d-1) \notag\\
  &amp;=
      e^{-2i\pi\left(\frac{\sum_{l=1}^{d}k_l m_l}{N}\right)}
      f_1(k_1-1)\times\hdots\times \overline{f}_j(N-1-k_j)\times\hdots\times f_d(k_d-1)
  \end{aligned}"/></p>
</div><p>Thus:</p>
<div class="math">
<p><img src="../../_images/math/a6c6677bfe5a17c4590703195f4f5b1b5baf98e8.svg" alt="\begin{aligned}
  \sum_{k_j=-N}^{-1}\delta\left(k_1h_1,\hdots,k_dh_d\right) E_{m_1,\hdots,m_d}&amp;(k_1,\hdots,k_d)
    = \sum_{k_j=0}^{N-1}\delta\left(k_1h_1,\hdots,(k_j-N)h_j,\hdots,k_dh_d\right) \times\notag\\
    &amp; e^{-2i\pi\left(\frac{\sum_{l=1}^{d}k_l m_l}{N}\right)}
      f_1(k_1-1)\times\hdots\times \overline{f}_j(N-1-k_j)\times\hdots\times f_d(k_d-1) \label{Eq:j-sigma-}
  \end{aligned}"/></p>
</div><p>To summarize:</p>
<ol class="arabic simple">
<li><p>In order to compute sum from <img class="math" src="../../_images/math/88272e93804b7619f2569ab478173cc6f6bdd97a.svg" alt="k_1=1"/> to <img class="math" src="../../_images/math/6bed8a44f63e1eb8e61608a5c93374a505943eec.svg" alt="N"/>, we multiply
by <img class="math" src="../../_images/math/2e036a0f52ed43cfb216f70979048a0950884dec.svg" alt="e^{-2i\pi\left(\frac{m_1}{N}\right)}"/> and consider
<img class="math" src="../../_images/math/7656bbb243645a6e7a151e73320a805be0092f4d.svg" alt="\delta((k_1+1)h,\hdots)f_1(k_1)"/></p></li>
<li><p>In order to compute sum from <img class="math" src="../../_images/math/6eb7b3d2af79b18ff3ee560751c5794d6aeb228c.svg" alt="k_1=-N"/> to <img class="math" src="../../_images/math/7ee1de95f8c832f78f668c60df554d0a6b607404.svg" alt="-1"/>, we
consider <img class="math" src="../../_images/math/0212c0fc19abc8d0e030b3b16149349219f27e10.svg" alt="\delta((k_1-N)h,\hdots)\overline{f}_1(N-1-k_1)"/></p></li>
</ol>
<div class="topic">
<p class="topic-title">API:</p>
<ul class="simple">
<li><p>See <a class="reference internal" href="../../user_manual/_generated/openturns.RandomMixture.html#openturns.RandomMixture" title="openturns.RandomMixture"><code class="xref py py-class docutils literal notranslate"><span class="pre">RandomMixture</span></code></a></p></li>
</ul>
</div>
<div class="topic">
<p class="topic-title">Examples:</p>
<ul class="simple">
<li><p>See <a class="reference internal" href="../../auto_probabilistic_modeling/distributions/plot_create_random_mixture.html"><span class="doc">Create a random mixture</span></a></p></li>
</ul>
</div>
<div class="topic">
<p class="topic-title">References:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../../bibliography.html#abate1992" id="id1"><span>[abate1992]</span></a></p></li>
</ul>
</div>
</div>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="process_definitions.html" title="Stochastic process definitions"
             >next</a> |</li>
        <li class="right" >
          <a href="copulas.html" title="Copulas"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">OpenTURNS 1.19dev documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../contents.html" >Contents</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../theory.html" >Theory</a> &#187;</li>
          <li class="nav-item nav-item-3"><a href="probabilistic_modeling.html" >Probabilistic modeling</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Random Mixture: affine combination of independent univariate distributions</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2005-2022 Airbus-EDF-IMACS-ONERA-Phimeca.
      Last updated on Jan 01, 2022.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 4.5.0.
    </div>
  </body>
</html>