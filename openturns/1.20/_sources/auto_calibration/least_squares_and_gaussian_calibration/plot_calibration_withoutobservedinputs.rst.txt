
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_calibration/least_squares_and_gaussian_calibration/plot_calibration_withoutobservedinputs.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_calibration_least_squares_and_gaussian_calibration_plot_calibration_withoutobservedinputs.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_calibration_least_squares_and_gaussian_calibration_plot_calibration_withoutobservedinputs.py:


Calibration without observed inputs
===================================

.. GENERATED FROM PYTHON SOURCE LINES 8-15

The goal of this example is to present the calibration of a parametric model which
does not have observed inputs.
We are going to see how to use the :class:`~openturns.Sample` class
and create an empty sample.
Indeed, this is mandatory in order to fit within the calibration
framework that is used in the library.
In this example there are, however, several outputs to be calibrated.

.. GENERATED FROM PYTHON SOURCE LINES 15-20

.. code-block:: default


    import openturns as ot
    from matplotlib import pylab as plt
    import openturns.viewer as viewer








.. GENERATED FROM PYTHON SOURCE LINES 21-35

The vector of parameters is
:math:`\boldsymbol{\theta} = (a, b, c)^T \in \mathbb{R}^3`.
This model is linear in :math:`\boldsymbol{\theta}` and identifiable.
It is derived from the equation:

.. math::
    y(x) = a + b x + c x^2

at :math:`x=-1.0, -0.6, -0.2, 0.2, 0.6, 1.0`.
In the model, however, the abscissas are fixed and constant.
Therefore, the parametric model has 3 parameters, no input and 6 outputs
:math:`y_1, ..., y_6`.
This produces a set of 5 observations for each output, leading to a total
of 5 (observations per output) * 6 (outputs) = 30 observations.

.. GENERATED FROM PYTHON SOURCE LINES 35-49

.. code-block:: default

    g = ot.SymbolicFunction(
        ["a", "b", "c"],
        [
            "a +  -1.0  * b +  1.0  * c",
            "a +  -0.6  * b +  0.36  * c",
            "a +  -0.2  * b +  0.04  * c",
            "a +  0.2  * b +  0.04  * c",
            "a +  0.6  * b +  0.36  * c",
            "a +  1.0  * b +  1.0  * c",
        ],
    )
    outputDimension = g.getOutputDimension()
    print(outputDimension)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    6




.. GENERATED FROM PYTHON SOURCE LINES 50-51

We set the true value of the parameters.

.. GENERATED FROM PYTHON SOURCE LINES 51-54

.. code-block:: default

    trueParameter = ot.Point([12.0, 7.0, -8.0])
    print(trueParameter)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [12,7,-8]




.. GENERATED FROM PYTHON SOURCE LINES 55-57

In order to generate the observed outputs, we create a distribution
in dimension 3, with Dirac (i.e. constant) marginals.

.. GENERATED FROM PYTHON SOURCE LINES 57-61

.. code-block:: default

    inputRandomVector = ot.ComposedDistribution(
        [ot.Dirac(theta) for theta in trueParameter]
    )








.. GENERATED FROM PYTHON SOURCE LINES 62-63

The candidate value is chosen to be different from the true parameter value.

.. GENERATED FROM PYTHON SOURCE LINES 63-67

.. code-block:: default

    candidate = ot.Point([8.0, 9.0, -6.0])
    calibratedIndices = [0, 1, 2]
    model = ot.ParametricFunction(g, calibratedIndices, candidate)








.. GENERATED FROM PYTHON SOURCE LINES 68-71

We consider a multivariate gaussian noise with zero mean, standard deviation
equal to 0.05 and independent copula.
The independent copula implies that the errors of the 6 outputs are independent.

.. GENERATED FROM PYTHON SOURCE LINES 71-78

.. code-block:: default

    outputObservationNoiseSigma = 1.0
    meanNoise = ot.Point(outputDimension)
    covarianceNoise = [outputObservationNoiseSigma] * outputDimension
    R = ot.IdentityMatrix(outputDimension)
    observationOutputNoise = ot.Normal(meanNoise, covarianceNoise, R)
    print(observationOutputNoise)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Normal(mu = [0,0,0,0,0,0], sigma = [1,1,1,1,1,1], R = 6x6
    [[ 1 0 0 0 0 0 ]
     [ 0 1 0 0 0 0 ]
     [ 0 0 1 0 0 0 ]
     [ 0 0 0 1 0 0 ]
     [ 0 0 0 0 1 0 ]
     [ 0 0 0 0 0 1 ]])




.. GENERATED FROM PYTHON SOURCE LINES 79-82

Finally, we generate the outputs by evaluating the exact outputs of the
function and adding the noise.
We use a sample with size equal to 5.

.. GENERATED FROM PYTHON SOURCE LINES 82-90

.. code-block:: default

    size = 5
    # Generate exact outputs
    inputSample = inputRandomVector.getSample(size)
    outputStress = g(inputSample)
    # Add noise
    sampleNoise = observationOutputNoise.getSample(size)
    outputObservations = outputStress + sampleNoise








.. GENERATED FROM PYTHON SOURCE LINES 91-93

Now is the important part of this script: there are no input observations.
This is why we create a sample of dimension equal to 0.

.. GENERATED FROM PYTHON SOURCE LINES 93-95

.. code-block:: default

    inputObservations = ot.Sample(0, 0)








.. GENERATED FROM PYTHON SOURCE LINES 96-97

We are now ready to perform the calibration.

.. GENERATED FROM PYTHON SOURCE LINES 97-106

.. code-block:: default

    algo = ot.LinearLeastSquaresCalibration(
        model, inputObservations, outputObservations, candidate, "SVD"
    )
    algo.run()
    calibrationResult = algo.getResult()
    parameterMAP = calibrationResult.getParameterMAP()
    print(parameterMAP)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [12.1939,7.07928,-8.30656]




.. GENERATED FROM PYTHON SOURCE LINES 107-109

We observe that the estimated parameter is relatively close to
the true parameter value.

.. GENERATED FROM PYTHON SOURCE LINES 109-112

.. code-block:: default

    print(parameterMAP - trueParameter)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [0.193869,0.0792835,-0.306565]




.. GENERATED FROM PYTHON SOURCE LINES 113-118

Graphical validation
--------------------

We now validate the calculation by drawing the exact function and compare
it to the function with estimated parameters.

.. GENERATED FROM PYTHON SOURCE LINES 118-129

.. code-block:: default

    fullModel = ot.SymbolicFunction(["a", "b", "c", "x"], ["a + b * x + c * x^2"])
    parameterIndices = [0, 1, 2]
    trueFunction = ot.ParametricFunction(fullModel, parameterIndices, trueParameter)
    print(trueFunction)
    beforeCalibrationFunction = ot.ParametricFunction(
        fullModel, parameterIndices, candidate
    )
    print(beforeCalibrationFunction)
    calibratedFunction = ot.ParametricFunction(fullModel, parameterIndices, parameterMAP)
    print(calibratedFunction)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ParametricEvaluation([a,b,c,x]->[a + b * x + c * x^2], parameters positions=[0,1,2], parameters=[a : 12, b : 7, c : -8], input positions=[3])
    ParametricEvaluation([a,b,c,x]->[a + b * x + c * x^2], parameters positions=[0,1,2], parameters=[a : 8, b : 9, c : -6], input positions=[3])
    ParametricEvaluation([a,b,c,x]->[a + b * x + c * x^2], parameters positions=[0,1,2], parameters=[a : 12.1939, b : 7.07928, c : -8.30656], input positions=[3])




.. GENERATED FROM PYTHON SOURCE LINES 130-133

In order to validate the calibration, we compare the parametric function
with true parameters at given abscissas with the parametric function
with calibrated parameters.

.. GENERATED FROM PYTHON SOURCE LINES 133-166

.. code-block:: default

    abscissas = [-1.0, -0.6, -0.2, 0.2, 0.6, 1.0]
    xmin = min(abscissas)
    xmax = max(abscissas)

    npoints = 50
    graph = ot.Graph("Calibration without observations", "x", "y", True, "bottomright")
    curve = trueFunction.draw(xmin, xmax, npoints).getDrawable(0)
    curve.setLineStyle("dashed")
    curve.setLegend("True model")
    curve.setColor("darkorange1")
    graph.add(curve)
    # Before calibration
    curve = beforeCalibrationFunction.draw(xmin, xmax, npoints)
    curve.setLegends(["Model before calibration"])
    curve.setColors(["red"])
    graph.add(curve)
    # After calibration
    curve = calibratedFunction.draw(xmin, xmax, npoints)
    curve.setLegends(["Model after calibration"])
    curve.setColors(["green"])
    graph.add(curve)
    # Observations
    for i in range(outputDimension):
        cloud = ot.Cloud(ot.Sample([[abscissas[i]]] * size), outputObservations[:, i])
        cloud.setColor("blue")
        if i == 0:
            cloud.setLegend("Observations")
        graph.add(cloud)
    viewer = viewer.View(graph)

    plt.show()





.. image-sg:: /auto_calibration/least_squares_and_gaussian_calibration/images/sphx_glr_plot_calibration_withoutobservedinputs_001.png
   :alt: Calibration without observations
   :srcset: /auto_calibration/least_squares_and_gaussian_calibration/images/sphx_glr_plot_calibration_withoutobservedinputs_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 167-175

We notice that the calibration produces a good fit to the data.
Still, we notice a small discrepancy between the true mode and the model
after calibration, but this discrepancy is very small.
Since the model is linear with respect to the parameters :math:`a`, :math:`b`, :math:`c`,
the LinearLeastSquares method performs well.
If the model were non linear w.r.t. :math:`a`, :math:`b`, :math:`c`, then the linear least
squares method would not work that well and the parameters would be estimated
with less accuracy.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.092 seconds)


.. _sphx_glr_download_auto_calibration_least_squares_and_gaussian_calibration_plot_calibration_withoutobservedinputs.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_calibration_withoutobservedinputs.py <plot_calibration_withoutobservedinputs.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_calibration_withoutobservedinputs.ipynb <plot_calibration_withoutobservedinputs.ipynb>`
