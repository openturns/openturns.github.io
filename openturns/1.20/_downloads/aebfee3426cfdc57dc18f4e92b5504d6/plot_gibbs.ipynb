{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Gibbs sampling of the posterior distribution\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We sample the from the posterior distribution of the parameters of a mixture model.\n\n\\begin{align}X \\sim 0.7 \\mathcal{N}(\\mu_0, 1) + 0.3 \\mathcal{N}(\\mu_1, 1),\\end{align}\n\nwhere $\\mu_0$ and $\\mu_1$ are unknown parameters.\nThey are a priori i.i.d. with prior distribution $\\mathcal{N}(0, \\sqrt{10})$.\nThis example is drawn from Example 9.2 from *Monte-Carlo Statistical methods* by Robert and Casella (2004).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import openturns as ot\nfrom openturns.viewer import View\nimport numpy as np\n\not.RandomGenerator.SetSeed(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sample data with $\\mu_0 = 0$ and $\\mu_1 = 2.7$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "N = 500\np = 0.3\n\nmu0 = 0.0\nmu1 = 2.7\nnor0 = ot.Normal(mu0, 1.0)\nnor1 = ot.Normal(mu1, 1.0)\ntrue_distribution = ot.Mixture([nor0, nor1], [1 - p, p])\nobservations = np.array(true_distribution.getSample(500))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the true distribution.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = true_distribution.drawPDF()\ngraph.setTitle(\"True distribution\")\ngraph.setXTitle(\"\")\ngraph.setLegends([\"\"])\n_ = View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A natural step at this point is to introduce\nan auxiliary (unobserved) random variable $Z$\ntelling from which distribution $X$\nwas sampled.\n\nFor any nonnegative integer $i$,\n$Z_i$ follows the Bernoulli distribution with $p=0.3$,\nand $X_i | Z_i \\sim \\mathcal{N}(\\mu_{Z_i}, 1.0)$.\n\nLet $n_0$ (resp. $n_1$) denote the number of indices $i$\nsuch that $Z_i=0$ (resp. $Z_i=1$).\n\nConditionally to all $X_i$ and all $Z_i$,\n$\\mu_0$ and $\\mu_1$ are independent:\n$\\mu_0$ follows\n$\\mathcal{N} \\left(\\sum_{Z_i=0} \\frac{X_i}{0.1 + n_0}, \\frac{1}{0.1 + n_0} \\right)$\nand $\\mu_1$ follows\n$\\mathcal{N} \\left(\\sum_{Z_i=1} \\frac{X_i}{0.1 + n_1}, \\frac{1}{0.1 + n_1} \\right)$.\n\nFor any $i$, conditionally to $X_i$, $\\mu_0$ and $\\mu_1$,\n$Z_i$ is independent from all $Z_j$ ($j \\neq i$)\nand follows the Bernoulli distribution with parameter\n\n  .. math::\n\n     p = \\frac{p \\exp \\left( -(X_i - \\mu_1)^2 / 2 \\right) }{p \\exp \\left( -(X_i - \\mu_1)^2 / 2 \\right) + (1-p) \\exp \\left( -(X_i - \\mu_0)^2 / 2 \\right) }\n\nWe now sample from the joint distribution of $(\\mu_0, \\mu1, Z_0,...,Z_{N-1})$ conditionally to the $X_i$ using the Gibbs algorithm.\nWe define functions that will translate a given state of the Gibbs algorithm into the correct parameters\nfor the distributions of $\\mu_0$, $\\mu_1$, and the $Z_i$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def nor0post(pt):\n    z = np.array(pt)[2:]\n    x0 = observations[z == 0]\n    mu0 = x0.sum() / (0.1 + len(x0))\n    sigma0 = 1.0 / (0.1 + len(x0))\n    return [mu0, sigma0]\n\n\ndef nor1post(pt):\n    z = np.array(pt)[2:]\n    x1 = observations[z == 1]\n    mu1 = x1.sum() / (0.1 + len(x1))\n    sigma1 = 1.0 / (0.1 + len(x1))\n    return [mu1, sigma1]\n\n\ndef zpost(pt):\n    mu0 = pt[0]\n    mu1 = pt[1]\n    term1 = p * np.exp(-((observations - mu1) ** 2) / 2)\n    term0 = (1.0 - p) * np.exp(-((observations - mu0) ** 2) / 2)\n    res = term1 / (term1 + term0)\n    # output must be a 1d list or array in order to create a PythonFunction\n    return res.reshape(-1)\n\n\nnor0posterior = ot.PythonFunction(2 + N, 2, nor0post)\nnor1posterior = ot.PythonFunction(2 + N, 2, nor1post)\nzposterior = ot.PythonFunction(2 + N, N, zpost)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now construct the Gibbs algorithm\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "initialState = [0.0] * (N + 2)\n\nsampler0 = ot.RandomVectorMetropolisHastings(\n    ot.RandomVector(ot.Normal()), initialState, [0], nor0posterior\n)\nsampler1 = ot.RandomVectorMetropolisHastings(\n    ot.RandomVector(ot.Normal()), initialState, [1], nor1posterior\n)\n\nbig_bernoulli = ot.ComposedDistribution([ot.Bernoulli()] * N)\n\nsampler2 = ot.RandomVectorMetropolisHastings(\n    ot.RandomVector(big_bernoulli), initialState, range(2, N + 2), zposterior\n)\n\ngibbs = ot.Gibbs([sampler0, sampler1, sampler2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the Gibbs algorithm\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "s = gibbs.getSample(10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract the relevant marginals: the first ($mu_0$) and the second ($\\mu_1$).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "posterior_sample = s[:, 0:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us plot the posterior density.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ks = ot.KernelSmoothing().build(posterior_sample)\ngraph = ks.drawPDF()\ngraph.setTitle(\"Posterior density\")\ngraph.setLegendPosition(\"bottomright\")\ngraph.setXTitle(r\"$\\mu_0$\")\ngraph.setYTitle(r\"$\\mu_1$\")\n_ = View(graph)\n\nView.ShowAll()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}