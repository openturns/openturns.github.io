{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Gaussian Process Regression : quick-start\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Abstract\n\nIn this example, we create a Gaussian Process Regression for a function which has\nscalar real inputs and outputs.\nWe show how to create the learning and the validation samples.\nWe show how to create the surrogate model by choosing a trend and a covariance model.\nFinally, we compute the predicted confidence interval using the conditional variance.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n\nWe consider the sine function:\n\n\\begin{align}\\model(x) = \\sin(x)\\end{align}\n\n\nfor any $x\\in[0,12]$.\n\nWe want to create a surrogate model of this function. This is why we create a sample of $n$ observations of the function:\n\n\\begin{align}y_i = \\model(x_i)\\end{align}\n\n\nfor $i=1,...,7$, where $x_i$ is the i-th input and $y_i$ is the corresponding output.\n\nWe consider the seven following inputs :\n\n============ === === === === ===== ==== ======\n $i$    1   2   3   4    5     6    7\n============ === === === === ===== ==== ======\n $x_i$  1   3   4   6   7.9   11   11.5\n============ === === === === ===== ==== ======\n\nWe are going to consider a Gaussian Process Regression surrogate model with:\n\n* a constant trend,\n* a Matern covariance model.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import openturns as ot\nfrom openturns import viewer\nimport openturns.experimental as otexp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We begin by defining the function $\\model$ as a symbolic function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "g = ot.SymbolicFunction([\"x\"], [\"sin(x)\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we define the `x_train` variable which contains the inputs of the design of experiments of the training step.\nThen we compute the `y_train` corresponding outputs. The variable `n_train` is the size of the training design of experiments.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x_train = ot.Sample([[x] for x in [1.0, 3.0, 4.0, 6.0, 7.9, 11.0, 11.5]])\ny_train = g(x_train)\nn_train = x_train.getSize()\nn_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to compare the function and its surrogate model, we use a test (i.e. validation) design of experiments made of a regular grid of 100 points from 0 to 12.\nThen we convert this grid into a :class:`~openturns.Sample` and we compute the outputs of the\nfunction on this sample.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "xmin = 0.0\nxmax = 12.0\nn_test = 100\nstep = (xmax - xmin) / (n_test - 1)\nmyRegularGrid = ot.RegularGrid(xmin, step, n_test)\nx_test = myRegularGrid.getVertices()\ny_test = g(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to observe the function and the location of the points in the input design of experiments, we define the following function which plots the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_1d_data(x_data, y_data, type=\"Curve\", legend=None, color=None, linestyle=None):\n    \"\"\"Plot the data (x_data,y_data) as a Cloud/Curve\"\"\"\n    if type == \"Curve\":\n        graphF = ot.Curve(x_data, y_data)\n    else:\n        graphF = ot.Cloud(x_data, y_data)\n    if legend is not None:\n        graphF.setLegend(legend)\n    if color is not None:\n        graphF.setColor(color)\n    if linestyle is not None:\n        graphF.setLineStyle(linestyle)\n    return graphF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we draw the model and the train sample.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = ot.Graph(\"Model and Train sample\", \"X\", \"Y\", True, \"\")\ngraph.add(\n    plot_1d_data(x_test, y_test, legend=\"model\", color=\"black\", linestyle=\"dashed\")\n)\ngraph.add(\n    plot_1d_data(x_train, y_train, type=\"Cloud\", legend=\"train sample\", color=\"red\")\n)\ngraph.setLegendPosition(\"upper right\")\nview = viewer.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creation of the surrogate model\n\nWe use the :class:`~openturns.ConstantBasisFactory` class to define the trend and the\n:class:`~openturns.MaternModel` class to define the covariance model.\nIn this example, the smoothness parameter of the Mat\u00e9rn model is fixed to $\\nu=3/2$ and\nwe only estimate the scale and the amplitude parameters.\n\nNevertheless, we could modify the list of the\nparameters that have to be estimated (the *active* parameters) and in particular we can add the\nestimation of $\\nu$: see the documentation of the method\n:meth:`~openturns.CovarianceModel.setActiveParameter` of\nthe class :class:`~openturns.CovarianceModel` to get more details.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dimension = 1\nbasis = ot.ConstantBasisFactory(dimension).build()\ncovarianceModel = ot.MaternModel([1.0] * dimension, 1.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The class :class:`~openturns.experimental.GaussianProcessFitter`  builds the Gaussian process $Y$ defined by:\n\n\\begin{align}Y(\\omega, x) = \\mu(x) + W(\\omega, x)\\end{align}\n\nwhere:\n\n- $\\mu(x) = \\sum_{j=1}^{b} \\beta_j \\varphi_j(x)$ and $\\varphi_j: \\Rset \\rightarrow \\Rset$\n  the trend function for $1 \\leq j \\leq b$. Here the functional basis is reduced to the constant\n  function;\n- $W$ is a Gaussian process of dimension 1 with zero mean and a Mat\u00e9rn covariance model\n  which covariance function is denoted by $C$.\n\nThe coefficients of the trend function and the active covariance model parameters are estimated by\nmaximizing the *reduced* log-likelihood of the model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fitter_algo = otexp.GaussianProcessFitter(x_train, y_train, covarianceModel, basis)\nfitter_algo.run()\nfitter_result = fitter_algo.getResult()\nprint(fitter_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can draw the trend function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "trend_func = fitter_result.getMetaModel()\ng_trend = trend_func.draw(xmin, xmax, 256)\ng_trend.setTitle(r\"Trend function of the Gaussian process $Y$\")\ng_trend.setXTitle(r\"$x$\")\ng_trend.setYTitle(r\"$\\mu(x)$\")\nview = viewer.View(g_trend)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The class :class:`~openturns.experimental.GaussianProcessRegression` is built from the  Gaussian process $Y$ and makes\nthe  Gaussian process approximation $\\vect{Z}$ interpolate the data set and is defined as:\n\n\\begin{align}:label: GPRdefEx\n\n   \\vect{Z}(\\omega, \\vect{x}) = \\vect{Y}(\\omega, \\vect{x})\\, | \\,  \\cC\\end{align}\n\nwhere $\\cC$ is the condition $\\vect{Y}(\\omega, \\vect{x}_k) = \\vect{y}_k$ for\n$1 \\leq k \\leq \\sampleSize$. The Gaussian process regression surrogate model is defined by the mean of $\\vect{Z}$:\n\n\\begin{align}\\metaModel(\\vect{x}) = \\vect{\\mu}(\\vect{x}) + \\sum_{i=1}^\\sampleSize \\gamma_i \\mat{C}( \\vect{x},  \\vect{x}_i)\\end{align}\n\nwhere the $\\gamma_i$ are called the *covariance coefficients* and $C$ the covariance function of the Mat\u00e9rn\ncovariance model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gpr_algo = otexp.GaussianProcessRegression(fitter_result)\ngpr_algo.run()\ngpr_result = gpr_algo.getResult()\nprint(gpr_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We observe that the `scale` and `amplitude` parameters have been optimized by the\n:meth:`~openturns.experimental.GaussianProcessFitter.run` method, while the $\\nu$\nparameter has remained unchanged.\nThen we get the surrogate model with\n:meth:`~openturns.experimental.GaussianProcessFitterResult.getMetaModel` and we\nevaluate the outputs of the surrogate model on the test\ndesign of experiments.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gprMetamodel = gpr_result.getMetaModel()\ny_test_MM = gprMetamodel(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we plot Gaussian process regression surrogate model, in addition to the previous plots.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = ot.Graph(\"Gaussian process regression surrogate model\", \"X\", \"Y\", True, \"\")\ngraph.add(\n    plot_1d_data(x_test, y_test, legend=\"model\", color=\"black\", linestyle=\"dashed\")\n)\ngraph.add(\n    plot_1d_data(x_train, y_train, type=\"Cloud\", legend=\"train sample\", color=\"red\")\n)\ngraph.add(plot_1d_data(x_test, y_test_MM, legend=\"GPR\", color=\"blue\"))\ngraph.setLegendPosition(\"upper right\")\nview = viewer.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We observe that the Gaussian process regression surrogate model is interpolating. This is what is meant by\n*conditioning* a Gaussian process.\n\nWe see that, when the sine function has a strong curvature between two points which are separated\nby a\nlarge distance (e.g. between $x=4$ and $x=6$),\nthen the Gaussian regression is not close to the function $g$.\nHowever, when the training points are close (e.g. between $x=11$ and $x=11.5$) or when the function is nearly\nlinear (e.g. between $x=8$ and $x=11$),\nthen the Gaussian process regression is quite accurate.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute confidence bounds\n\nIn order to assess the quality of the surrogate model, we can estimate the variance and compute a\n$1-\\alpha = 95\\%$ confidence interval associated with the conditioned Gaussian process.\n\nWe denote by $q_{p}$ the quantile of order $p$ of the Gaussian distribution.\nTherefore, the confidence interval of level $1-\\alpha$ is $\\left[q_{\\alpha/2},q_{1-\\alpha/2}\\right]$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "alpha = 0.05\n\n\ndef computeQuantileAlpha(alpha):\n    bilateralCI = ot.Normal().computeBilateralConfidenceInterval(1 - alpha)\n    return bilateralCI.getUpperBound()[0]\n\n\nquantileAlpha = computeQuantileAlpha(alpha)\nprint(\"alpha=%f\" % (alpha))\nprint(\"Quantile alpha=%f\" % (quantileAlpha))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Gaussian process regression computed on the sample $(\\xi_1, \\dots, \\xi_N)$ is a Gaussian vector. It is possible to\nget the variance of each $\\vect{Z}_i(\\omega) = \\vect{Y}(\\omega, \\vect{\\xi}_i)\\, | \\,  \\cC$ for $1 \\leq i \\leq N$\nwith\nthe :meth:`~openturns.experimental.GaussianProcessConditionalCovariance.getConditionalMarginalVariance` method. That method\nreturns a point which is the sequence of the variances of each $\\vect{Z}_i(\\omega)$.\nSince this is a variance, we use the square root in order to compute the\nstandard deviation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sqrt = ot.SymbolicFunction([\"x\"], [\"sqrt(x)\"])\ngccc = otexp.GaussianProcessConditionalCovariance(gpr_result)\nconditionalVariance = gccc.getConditionalMarginalVariance(x_test)\nconditionalSigma = sqrt(conditionalVariance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following figure presents the conditional standard deviation depending on $x$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = ot.Graph(\n    \"Conditional standard deviation\", \"x\", \"Conditional standard deviation\", True, \"\"\n)\ncurve = ot.Curve(x_test, conditionalSigma)\ngraph.add(curve)\nview = viewer.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now compute the bounds of the confidence interval. For this purpose we define a small function\n`computeBoundsConfidenceInterval` :\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def computeBoundsConfidenceInterval(quantileAlpha):\n    dataLower = [\n        [y_test_MM[i, 0] - quantileAlpha * conditionalSigma[i, 0]]\n        for i in range(n_test)\n    ]\n    dataUpper = [\n        [y_test_MM[i, 0] + quantileAlpha * conditionalSigma[i, 0]]\n        for i in range(n_test)\n    ]\n    dataLower = ot.Sample(dataLower)\n    dataUpper = ot.Sample(dataUpper)\n    return dataLower, dataUpper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We define two small lists to draw three different confidence intervals (defined by the alpha value) :\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "alphas = [0.05, 0.1, 0.2]\n# three different green colors defined by HSV values\nmycolors = [[120, 1.0, 1.0], [120, 1.0, 0.75], [120, 1.0, 0.5]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are ready to display all the previous information and the three confidence intervals we want.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "sphinx_gallery_thumbnail_number = 5\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = ot.Graph(\n    \"Gaussian process regression surrogate model and confidence bounds\",\n    \"X\",\n    \"Y\",\n    True,\n    \"\",\n)\n\n# Now we loop over the different values :\nfor idx, v in enumerate(alphas):\n    quantileAlpha = computeQuantileAlpha(v)\n    vLow, vUp = computeBoundsConfidenceInterval(quantileAlpha)\n    boundsPoly = ot.Polygon.FillBetween(x_test, vLow, vUp)\n    boundsPoly.setColor(\n        ot.Drawable.ConvertFromHSV(mycolors[idx][0], mycolors[idx][1], mycolors[idx][2])\n    )\n    boundsPoly.setLegend(\" %d%% bounds\" % ((1.0 - v) * 100))\n    graph.add(boundsPoly)\n\ngraph.add(\n    plot_1d_data(x_test, y_test, legend=\"model\", color=\"black\", linestyle=\"dashed\")\n)\ngraph.add(plot_1d_data(x_train, y_train, type=\"Cloud\", legend=\"Data\", color=\"red\"))\ngraph.add(plot_1d_data(x_test, y_test_MM, legend=\"GPR\", color=\"blue\"))\ngraph.setLegendPosition(\"upper right\")\nview = viewer.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the confidence intervals are small in the regions where two\nconsecutive training points are close to each other\n(e.g. between $x=11$ and $x=11.5$) and large when the two points\nare not (e.g. between $x=8.$ and $x=11$) or when the curvature\nof the function is large (between $x=4$ and $x=6$).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display all figures.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "viewer.View.ShowAll()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}