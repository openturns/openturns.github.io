{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Linear Regression with interval-censored observations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Model formulation\n\nWe consider the following linear model:\n\n\\begin{align}\\vect{Y} = \\mat{X} \\vect{\\theta} + \\vect{\\varepsilon},\\end{align}\n\nwhere the observation vector $\\vect{Y} \\in \\mathbb R^{n}$ is modeled as\nthe sum of:\n\n- a linear part, with an $n \\times p$ design matrix $\\mat{X}$\n  and unknown regression coefficients $\\vect{\\theta} \\in\\mathbb R^p$,\n- a Gaussian error term\n  $\\vect{\\varepsilon} \\sim \\mathcal N_n(\\vect{0}, \\tau^{-1} \\mat{I}_n+\\mat{Q}^{-1})$,\n\nwhere $\\mat{Q}$ represents a known *precision* (inverse variance) matrix\nfor measurement errors,\nand $\\tau$ an unknown precision parameter\nquantifying the variability of the observed phenomenon.\n\n### 1.1. Likelihood of the linear regression model\n\nThe above linear regression model can thus be written:\n\n\\begin{align}\\vect{Y} | \\vect{\\theta}, \\tau\n   {\\sim} \\mathcal N_{n}(\\mat{X} \\vect{\\theta} ; \\tau^{-1} \\mat{I}_n +\\mat{Q}^{-1}),\\end{align}\n\nWe then have the following likelihood:\n\n\\begin{align}\\mathcal L(\\vect{Y}|\\vect{\\theta}, \\tau) =\n   (2\\pi)^{-\\frac{n}{2}}\n   {\\rm det}(\\tau^{-1} \\mat{I}_n+\\mat{Q}^{-1})^{-\\frac{1}{2}}\n   e^{-\\frac{1}{2}\n   ||\\vect{Y} - \\mat{X} \\vect{\\theta} ||_{\\tau^{-1} \\mat{I}_n +\\mat{Q}^{-1}}^2\n   }\\end{align}\n\nwhere $||\\vect{X}-\\vect{Y}||_{\\mat{V}}^2$ is the Mahalanobis distance between\n$\\vect{X}$ and $\\vect{Y}$ with covariance matrice $\\mat{V}$:\n\n\\begin{align}||\\vect{X}-\\vect{Y}||_{\\mat{V}}^2 = (\\vect{X}-\\vect{Y})^\\top\\ \\mat{V}^{-1}\\ (\\vect{X}-\\vect{Y}).\\end{align}\n\n### 1.2. Interval Censorship\n\nWe now assume that, instead of observing directly the\n$Y_i, i=1,\\ldots,n$ as described above, we only have access to\ndiscretized values $Y_i^{obs} \\in \\delta \\times \\mathbb N$, where\n$\\delta > 0$ is a grid length and $Y_i^{obs} = \\delta \\times k$ means that\n$Y_i \\in \\delta \\times \\left[ k - \\frac{1}{2}; k + \\frac{1}{2} \\right[$.\n\n### 1.3. Remarks\n\n-  The presence of a composite matrix $\\tau^{-1} \\mat{I}_n+\\mat{Q}^{-1}$ makes\n   estimation more complex than with a spherical one\n   $\\tau^{-1}\\mat{I}_n,$ since we would then have explicit (closed-form)\n   maximum likelihood estimators, and also conjugate priors leading to\n   explicit full joint posterior distributions.\n-  Another difficulty is the presence of censored data, since the\n   likelihood is no more available in closed-form. As we will see, this\n   can be overcome thanks to Bayesian inference.\n-  Heteroscedastic linear modeling under interval censorship as\n   formulated above was originally motivated by an industrial case-study\n   in seismology, wherein the $Y_i$ correspond to the observed\n   intensity of an earthquake in a distant site, and explanatory\n   variables $\\vect{X}_{i}$ are derived from the epicentral distance to the\n   earthquake's source as well as its characteristics (magnitude,\n   depth). But it can also arise in many different contexts, as soon as\n   observations are available with known statistical precisions (hence\n   the heteroscedasticity) and limited numerical accuracy (hence the\n   censorship).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Simulate the dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import openturns as ot\nfrom openturns.viewer import View\nimport numpy as np\n\not.Log.Show(ot.Log.NONE)\not.RandomGenerator.SetSeed(1)\n\nn = 10\ndelta = 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build the design matrix $\\mat{X}$\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X = ot.Sample([[1.0]] * n)\nX.stack(ot.Normal(0.0, 10.0).getSample(n))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Make the precision matrix $\\mat{Q}$ a diagonal matrix and sample\nits diagonal coefficients from an :class:`~openturns.Exponential` distribution.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "Q = np.ones((n, 1)) + ot.Exponential().getSample(n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Choose values for the parameters $\\vect{\\theta}$ and $\\tau$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "theta_true = np.array([[0.0], [1.0]])\np = len(theta_true)\ntau_true = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First sample uncensored, and then censored observation data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mean_true = np.dot(X, theta_true).ravel()\nstd_true = np.sqrt(1.0 / tau_true + 1.0 / Q).ravel()\n\nY_sim = mean_true + [x[0] for x in ot.Normal().getSample(n)] * std_true\nYobs_sim = np.round(Y_sim / delta) * delta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the simulated dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = ot.Graph(\"Simulated data\", \"$X_1$\", \"$Y$\", True, \"upper left\", 16)\ncloud_obs = ot.Cloud(X[:, 1].asPoint(), Yobs_sim)\ncloud_obs.setPointStyle(\"bullet\")\ncloud_sim = ot.Cloud(X[:, 1].asPoint(), Y_sim)\ncloud_sim.setPointStyle(\"bullet\")\ncurve = ot.Curve(X[:, 1].asPoint(), mean_true)\ncurve.setLineWidth(1.5)\ngraph.add(curve)\ngraph.add(cloud_sim)\ngraph.add(cloud_obs)\ngraph.setLegends([\"Trend\", \"$Y^{sim}$\", \"$Y^{obs}$\"])\n_ = View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Bayesian Inference\n\n### 2.1. Choice of a prior law\n\nWe use the standard Normal-Gamma prior for $\\vect{\\theta}$ and\n$\\tau$:\n\n\\begin{align}\\vect{\\theta} | \\tau \\sim \\mathcal N_{p}(\\vect{\\mu}_0; \\tau^{-1}\\mat{\\Sigma}_0) \\\\\n   \\tau \\sim \\mathcal G(a_0, b_0),\\end{align}\n\nwhere all parameters are assumed *a priori* independent if not stated\notherwise.\n\nFurthermore, a default choice for the hyperparameters consists in having\nall prior variances go to infinity, equivalent to the degenerate case:\n\n\\begin{align}\\vect{\\mu}_0, \\mat{\\Sigma}_0^{-1}, a_0, b_0 \\equiv 0\\end{align}\n\nBut the resulting prior is improper. Hence, posterior propriety needs to\nbe proven first.\n\nA simpler solution is to ensure prior (hence posterior) propriety by\nimposing bounds $\\vect{\\theta}_{\\min}, \\vect{\\theta}_{\\max}, \\tau_{\\min}, \\tau_{\\max}$\non all parameters following:\n\n\\begin{align}\\vect{\\theta} | \\tau \\sim \\mathcal N_{p}(\\vect{\\mu}_0; \\tau^{-1}\\mat{\\Sigma}_0) {\\bf 1}_{\\{\\vect{\\theta}_{\\min} \\leq \\vect{\\theta} \\leq \\vect{\\theta}_{\\max}\\}} ;\\\\\n   \\tau \\sim \\mathcal G(a_0, b_0){\\bf 1}_{\\{\\tau_{\\min} \\leq \\tau \\leq \\tau_{\\max}\\}},\\end{align}\n\nwhere inequalites are taken componentwise. When all hyperparameters go to\n$0$ as described above, the prior converges to a product of\nuniform distributions.\n\nWe will use this product of univariate uniforms as a prior in the\nfollowing. As discussed above, there is no simple way to obtain the\nposterior distribution, justifying the use of Monte-Carlo Markov chain\ntechniques, as described hereafter.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lower = (Yobs_sim.ravel() - delta).tolist()\nupper = (Yobs_sim.ravel() + delta).tolist()\n\n# Global support of the joint distribution: theta, tau, outputs\nsupport = ot.Interval([-2.0] * p + [1e-4] + lower, [2.0] * p + [1e1] + upper)\n\nprior = ot.JointDistribution(\n    [ot.Uniform(-2.0, 2.0), ot.Uniform(-2.0, 2.0), ot.Uniform(1e-4, 1e1)]\n)\n\n# Initialize to true value\ninitial_state = theta_true[:, 0].tolist() + [tau_true] + Y_sim.ravel().tolist()\ninitial_state = ot.Point(initial_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2. Posterior sampling\n\nThe solution we advocate consists in sampling from the joint posterior\ndistribution of all uncertain parameters, including the vector of\ncontinuous intensities $\\vect{Y},$ seen here as a latent variable.\nIndeed, adding $\\vect{Y}$ to the vector of sampled variables yields a\nposterior density which is available in closed form, up to an unknown\nmultiplicative factor\n\n\\begin{align}\\begin{array}{ll}\n   \\pi(\\vect{Y},\\vect{\\theta},\\tau|\\vect{Y}^{obs})\n       & \\propto \\pi(\\vect{\\theta},\\tau)\\times\\mathcal L(\\vect{Y}|\\vect{\\theta},\\tau)\\mathcal L(\\vect{Y}^{obs}|\\vect{Y})\\\\\n       & \\propto\n       {\\bf 1}_{\\{\\vect{\\theta}_{\\min} \\leq \\vect{\\theta} \\leq \\vect{\\theta}_{\\max}\\}}\\times\n       {\\bf 1}_{\\{\\tau_{\\min} \\leq \\tau \\leq \\tau_{\\max}\\}}\\times\n       \\mathcal N_n (\\vect{Y}|\\mat{X} \\vect{\\theta};\\tau^{-1} \\mat{I}_n + \\mat{Q}^{-1}) \\times\n       {\\bf 1}_{\\left[\\vect{Y}^{obs}-\\frac{\\delta}{2}; \\vect{Y}^{obs}+\\frac{\\delta}{2}\\right[}(\\vect{Y}).\n       \\end{array}\\end{align}\n\nThis allows one to perform the following Metropolis within Gibbs sampling\nscheme, wherein the pre-defined blocks of variables\n$(\\vect{Y},\\vect{\\theta},\\tau)$ are updated in turn, according to their\nconditional posterior density, or to a Markov kernel targeting it, as\ndescribed in the following.\n\n#### 2.2.1. Updating $\\vect{Y}$\n\n\\begin{align}\\pi(\\vect{Y}|\\vect{\\theta},\\tau,\\vect{Y}^{obs})\\propto\\prod_{i=1}^{n}\\mathcal N(Y_{i}|\\vect{X}_{i} \\vect{\\theta}, \\tau^{-1}+q_{i}^{-1}) {\\bf 1}_{\\left[Y_{i}^{obs}-\\frac{\\delta}{2};\n   Y_{i}^{obs}+\\frac{\\delta}{2}\\right[}(Y_{i})\\end{align}\n\nhence the latent variables $Y_i$ are updated by simply simulating\nindependent univariate truncated normals.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 1 : Create associated :class:`~openturns.RandomVector`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "marginals_trunc = [\n    ot.TruncatedNormal(Yobs_sim[i], 1.0, lower[i], upper[i]) for i in range(len(X))\n]\n\ntrunc_cond_Y = ot.JointDistribution(marginals_trunc)\nRV_Y = ot.RandomVector(trunc_cond_Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 2 : Link function, giving the parameters of the univariate truncated normals in function of the chain's current state\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gen_params = np.array(trunc_cond_Y.getParameter())\n\n\ndef py_link_function_y(x):\n    \"\"\"\n    link function for Y conditional density\n\n    Input\n        x: vector with length (p + 1 + n), containing the current state of (theta, tau, Y)\n\n    Output\n        params: vector with length 4*n, corresponding to mean, std, a and b, for each component of Y\n\n    Notes\n        a and b represent the upper and lower bounds for the truncated normals\n    \"\"\"\n    theta = [x[i] for i in range(p)]\n    tau = x[p]\n    # compute conditional mean and standard deviates\n    mean = np.dot(X, theta)\n    std = np.sqrt(1.0 / tau + 1.0 / Q)\n    # inject values in blueprint\n    params = gen_params.copy()\n    params[::4] = mean\n    params[1::4] = std.ravel()\n    return params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.2.2. Updating $\\vect{\\theta}$\n\n$\\pi(\\vect{\\theta}|\\vect{Y}, \\tau, \\vect{Y}^{obs})=\\pi(\\vect{\\theta}|\\vect{Y},\\tau).$ Due to the\npartial conjugacy of the conditional prior $\\pi(\\vect{\\theta}|\\tau),$\nthis is explicit, and given by the following box-constrained\nmultivariate normal:\n\n\\begin{align}\\vect{\\theta}|\\vect{Y},\\tau \\sim \\mathcal N_4( \\vect{\\mu}_n ; \\mat{\\Sigma}_n ){\\bf 1}_{\\{\\vect{\\theta}_{\\min} \\leq \\vect{\\theta} \\leq \\vect{\\theta}_{\\max}\\}},\\end{align}\n\nwith\n\n\\begin{align}\\vect{\\mu}_n = \\vect{\\mu}_0 + \\mat{\\Sigma}_0X^\\top(\\mat{X} \\mat{\\Sigma}_0 \\mat{X}^\\top + \\mat{I}_n + \\tau \\mat{Q}^{-1})^{-1}(\\vect{Y} - \\mat{X} \\vect{\\mu}_0) \\\\\n    \\mat{\\Sigma}_n = \\tau^{-1}\\mat{\\Sigma}_0 - \\tau^{-1}\\mat{\\Sigma}_0 \\mat{X}^\\top (\\mat{X} \\mat{\\Sigma}_0 \\mat{X}^\\top + \\mat{I}_n + \\tau \\mat{Q}^{-1})^{-1} \\mat{X} \\mat{\\Sigma}_0,\\end{align}\n\nor equivalently, thanks to the matrix Woodsbury identity:\n\n\\begin{align}\\vect{\\mu}_n = (\\mat{\\Sigma}_0^{-1} + \\mat{X}^\\top (\\mat{I}_n + \\tau \\mat{Q}^{-1})^{-1} \\mat{X})^{-1}(\\mat{\\Sigma}_0^{-1} \\vect{\\mu}_0\n    + \\mat{X}^\\top (\\mat{I}_n + \\tau \\mat{Q}^{-1})^{-1} \\vect{Y})\\\\\n    \\mat{\\Sigma}_n = \\tau^{-1}(\\mat{\\Sigma}_0^{-1} + \\mat{X}^\\top (\\mat{I}_n + \\tau \\mat{Q}^{-1})^{-1} \\mat{X})^{-1}.\\end{align}\n\nBy having all hyperparameters go to $0,$ we obtain the following\nsimplified form:\n\n\\begin{align}\\vect{\\mu}_n =  (\\mat{X}^\\top (\\mat{I}_n + \\tau \\mat{Q}^{-1})^{-1} \\mat{X})^{-1}(\\mat{X}^\\top (\\mat{I}_n + \\tau \\mat{Q}^{-1})^{-1} \\vect{Y})\\\\\n    \\mat{\\Sigma}_n = \\tau^{-1}(\\mat{X}^\\top (\\mat{I}_n + \\tau \\mat{Q}^{-1})^{-1} \\mat{X})^{-1}.\\end{align}\n\nExplicit simulation from a box-constrained multivariate normal can be\ndone with a simple rejection sampling scheme:\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 1 : Create associated :class:`~openturns.RandomVector`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class BoxConstrainedNormal(ot.PythonRandomVector):\n    \"\"\"\n    Multivariate normal distribution\n    under box constraints\n    \"\"\"\n\n    def __init__(\n        self, d=2, mu=np.zeros(2), Sigma=np.eye(2), r=np.zeros(2), s=np.ones(2)\n    ):\n        for x in mu, Sigma, r, s:\n            if len(x) != d:\n                print(\"expectation or bound does not have size %s\" % d)\n                raise ValueError\n        if Sigma.shape != (d, d):\n            print(\"covariance matrix not have dimensions (%s,%s)\" % (d, d))\n            raise ValueError\n        super(BoxConstrainedNormal, self).__init__(d)\n        self.mu = mu\n        self.Sigma = Sigma\n        self.r = r\n        self.s = s\n        self.interval = ot.Interval(r, s)\n\n    def setParameter(self, parameter):\n        d = self.getDimension()\n        self.mu = np.array(parameter[:d])\n        self.Sigma = np.array(parameter[d: d + d * d]).reshape(d, d)\n        self.r = np.array(parameter[-2 * d: -d])\n        self.s = np.array(parameter[-d:])\n        self.interval = ot.Interval(self.r, self.s)\n\n    def getParameter(self):\n        return np.concatenate(\n            [self.mu.ravel(), self.Sigma.ravel(), self.r.ravel(), self.s.ravel()]\n        )\n\n    def getRealization(self):\n        accept = False\n        proposaldist = ot.Normal(self.mu, ot.CovarianceMatrix(self.Sigma))\n        while not accept:\n            proposal = proposaldist.getRealization()\n            accept = self.interval.contains(proposal)\n        return proposal\n\n\nRV_theta = ot.RandomVector(BoxConstrainedNormal())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 2 : Link function, giving the parameters of the box-constrained normal in function of $\\tau$ and $\\vect{Y}$ values\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def py_link_function_theta(x):\n    tau = x[p]\n    Y = [x[i] for i in range(p + 1, len(x))]\n    # conditional mean and variance\n    # for diagonal Q\n    Itilde_inv = 1.0 / (1.0 + tau / Q)\n    Xtilde = Itilde_inv * X\n    Sigma_n = np.linalg.inv(np.dot(Xtilde.T, X)) / tau\n    mu_n = np.dot(Xtilde.T, Y)\n    mu_n = tau * np.dot(Sigma_n, mu_n)\n    # extract parameters in correct order (coherent with getParameter() method of RV_theta)\n    return np.concatenate(\n        [\n            mu_n.ravel(),\n            Sigma_n.ravel(),\n            support.getLowerBound()[:p],\n            support.getUpperBound()[:p],\n        ]\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 2.2.3. Updating $\\tau$\n\n$\\pi(\\tau|\\vect{Y},\\vect{\\theta})$ is proportional to:\n\n\\begin{align}\\pi(\\tau|\\vect{Y},\\vect{\\theta})\n   \\propto\n   {\\bf 1}_{\\{\\tau_{\\min} \\leq \\tau \\leq \\tau_{\\max}\\}}\\times\n   \\mathcal N_n(\\vect{Y}|\\mat{X} \\vect{\\theta};\\tau^{-1} \\mat{I}_n + \\mat{Q}^{-1}).\\end{align}\n\nHence, $\\tau$ can be updated using the Random-walk\nMetropolis-Hastings algorithm.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 1 : compute tau's conditional posterior density, up to a multiplicative factor\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def marginals_Y(theta, tau):\n    mu = np.dot(X, theta).ravel()\n    sigma = np.sqrt(1.0 / tau + 1.0 / Q).ravel()\n    return [ot.Normal(mu[i], sigma[i]) for i in range(len(X))]\n\n\ndef py_log_density(x):\n    theta = [x[i] for i in range(p)]\n    tau = x[p]\n    Y = [x[p + 1 + i] for i in range(len(X))]\n    ld = ot.JointDistribution(marginals_Y(theta, tau)).computeLogPDF(Y)\n    return [ld]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 2 : define the proposal distribution\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "proposal_tau = ot.Normal(0.0, 1e-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3. Initialization\n\nTo avoid all numerical problems, it is better to provide the algorithm\nwith a starting point not too far from the posterior mode. To this end,\nwe propose to set $\\vect{Y}=\\vect{Y}^{obs}$ for simplicity, then solve the\nfollowing optimization problem\n\n\\begin{align}\\widehat\\theta,\\widehat\\tau = \\arg\\max_{\\vect{\\theta},\\tau} \\pi(\\vect{\\theta},\\tau|\\vect{Y}^{obs},\\vect{Y}=\\vect{Y}^{obs})\n    = \\arg\\max_{\\theta\\in[\\vect{\\theta}_{\\min};\\vect{\\theta}_{\\max}]\n    \\tau\\in[\\tau_{\\min};\\tau_{\\max}]} \\mathcal N(\\vect{Y}^{obs}|\\vect{\\theta} \\mat{X}; \\tau^{-1} \\mat{I}_n + \\mat{Q}^{-1}).\\end{align}\n\nNote that the unconstrained optimization over $\\vect{\\theta}$ for fixed\n$\\tau$ is explicit, since:\n\n\\begin{align}\\arg\\max_{\\vect{\\theta}} \\pi(\\vect{\\theta},\\tau|\\vect{Y}^{obs},\\vect{Y}=\\vect{Y}^{obs})\n   =\\arg\\max_{\\vect{\\theta}} \\pi(\\vect{\\theta}|\\vect{Y}^{obs},\\vect{Y}=\\vect{Y}^{obs},\\tau).\\end{align}\n\nBut we have shown that the conditional posterior density of\n$\\vect{\\theta}$ is Gaussian. Hence the unconstrained conditional\nposterior mode (and mean) is given by:\n\n\\begin{align}\\arg\\max_{\\vect{\\theta}} \\pi(\\vect{\\theta}|\\vect{Y}^{obs},\\vect{Y}=\\vect{Y}^{obs},\\tau)\n    = (\\mat{X}^\\top (\\mat{I}_n + \\tau \\mat{Q}^{-1})^{-1} \\mat{X})^{-1}(\\mat{X}^\\top (\\mat{I}_n + \\tau \\mat{Q}^{-1})^{-1} \\vect{Y}):= \\vect{\\mu}_n(\\tau).\\end{align}\n\nIf this point does not respect the constraints, then we simply project\neach component unto the constrained space.\n\nHence the following 1D problem remains to be solved:\n\n\\begin{align}\\widehat\\tau = \\arg\\max_{\\tau\\in[\\tau_{\\min};\\tau_{\\max}]} \\mathcal N(\\vect{Y}^{obs}|\\vect{\\mu}_n(\\tau) \\mat{X}; \\tau^{-1} \\mat{I}_n + \\mat{Q}^{-1}).\\end{align}\n\nThe optimal value of $\\vect{\\theta}$ is then given by:\n$\\widehat{\\vect{\\theta}} = \\vect{\\mu}_n(\\widehat\\tau).$\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def mu_n(tau):\n    x = ot.Point(initial_state)\n    x[p] = tau\n    # trick: posterior conditional mean is computed by the link function\n    return py_link_function_theta(x)[:p]\n\n\n# optimization criterion\ndef log_cond_tau_post(tau):\n    x = ot.Point(initial_state)\n    x[p] = tau\n    # replace theta by its conditional posterior mean\n    x[:p] = py_link_function_theta(x)[:p]\n    # compute log conditional posterior of tau\n    ll = py_log_density(x)[0]\n    return ll"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1D optimization\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def func(X):\n    return [-log_cond_tau_post(X[0])]\n\n\nproblem = ot.OptimizationProblem(ot.PythonFunction(1, 1, func))\nproblem.setBounds(ot.Interval([1e-4], [1e4]))\nsolver = ot.TNC(problem)\nsolver.setStartingPoint([1.0])\nsolver.run()\ntauhat = solver.getResult().getOptimalPoint()[0]\nprint(\"tauhat =\", tauhat)\n\n# inject result in initialState vector\nx = ot.Point(initial_state)\nx[:p] = mu_n(tauhat)\nx[p] = tauhat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Final step : create a :class:`~openturns.MetropolisHastings` object for each block\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "initialState = x\n\nmi_Y = [i for i in range(p + 1, p + 1 + n)]\nlink_function_y = ot.PythonFunction(len(x), 4 * n, py_link_function_y)\nrvmh_Y = ot.RandomVectorMetropolisHastings(RV_Y, initialState, mi_Y, link_function_y)\n\nmi_theta = [i for i in range(p)]\nlink_function_theta = ot.PythonFunction(len(x), p + (p + 2) * p, py_link_function_theta)\nrvmh_theta = ot.RandomVectorMetropolisHastings(\n    RV_theta, initialState, mi_theta, link_function_theta\n)\n\nlog_pdf_tau = ot.PythonFunction(len(x), 1, py_log_density)\nrwmh_tau = ot.RandomWalkMetropolisHastings(\n    log_pdf_tau, support, initialState, proposal_tau, [p]\n)\n\n# Now, assemble the blocks to create a Gibbs algorithm:\ngibbs = ot.Gibbs([rvmh_Y, rvmh_theta, rwmh_tau])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Launch Algorithm\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sampleSize = 1000\n\n# Joint posterior density sample\nsample = gibbs.getSample(sampleSize)\n\n# compute acceptance rate\ntau_post = np.array(sample[:, p]).ravel()\nacc_rate = (tau_post[1:] != tau_post[:-1]).mean()\nprint(\"Acceptance rate: %s\" % acc_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot posterior distribution marginals\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# extract interest parameters\npost_sample = sample.getMarginal([i for i in range(p + 1)])\npost_sample.setDescription([\"$\\\\theta_0$\", \"$\\\\theta_1$\", \"$\\\\tau$\"])\n\nposterior = ot.KernelSmoothing().build(post_sample)\nposterior = ot.TruncatedDistribution(posterior, prior.getRange())\n\ngrid = ot.GridLayout(1, 3)\ngrid.setTitle(\"Bayesian inference\")\nxlabs = [r\"$\\theta_0$\", r\"$\\theta_1$\", r\"$\\tau$\"]\np_true = [theta_true[0][0], theta_true[1][0], tau_true]\nfor parameter_index in range(3):\n    graph = posterior.getMarginal(parameter_index).drawPDF()\n    bbox = graph.getBoundingBox()\n    bound = bbox.getUpperBound()[1]\n    prior_pdf = prior.getMarginal(parameter_index).drawPDF()\n    graph.add(prior_pdf)\n    curve_true = ot.Curve([p_true[parameter_index]] * 2, [0.0, bound])\n    graph.add(curve_true)\n    graph.setColors(ot.Drawable.BuildDefaultPalette(3))\n    graph.setLegends([\"Posterior\", \"Prior\", \"True value\"])\n    graph.setXTitle(xlabs[parameter_index])\n    grid.setGraph(0, parameter_index, graph)\n_ = View(grid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Draw pairplots of the posterior sample.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# sphinx_gallery_thumbnail_number = 3\ngrid = ot.VisualTest.DrawPairs(post_sample)\n\ngraph = grid.getGraph(0, 0)\npt = ot.Cloud(theta_true.T, \"#2ca02c\", \"circle\", \"True value\")\ngraph.add(pt)\ngrid.setGraph(0, 0, graph)\n\ngraph = grid.getGraph(1, 0)\npt = ot.Cloud([[theta_true[0, 0], tau_true]], \"#2ca02c\", \"circle\", \"True value\")\ngraph.add(pt)\ngrid.setGraph(1, 0, graph)\n\ngraph = grid.getGraph(1, 1)\npt = ot.Cloud([[theta_true[1, 0], tau_true]], \"#2ca02c\", \"circle\", \"True value\")\ngraph.add(pt)\ngrid.setGraph(1, 1, graph)\n\n_ = View(grid)\n\n\nView.ShowAll()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}