
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_surrogate_modeling/gaussian_process_regression/plot_gpr_normalization.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_surrogate_modeling_gaussian_process_regression_plot_gpr_normalization.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_surrogate_modeling_gaussian_process_regression_plot_gpr_normalization.py:


Gaussian Process Regression: Normalization for optimization
===========================================================

.. GENERATED FROM PYTHON SOURCE LINES 7-14

This example aims to illustrate Gaussian Process Fitter metamodel with normalization of data.
Like other machine learning techniques, heteregeneous data (i.e., data defined with different orders of magnitude)
can impact the training process of Gaussian Process Regression (GPR).
Automatic scaling process of the input data for the optimization of GPR hyperparameters can be defined using
the :class:`~openturns.ResourceMap` key `GaussianProcessFitter-OptimizationNormalization`.
In this example, we show the behavior of Gaussian Process Fitter with and without activating
the normalization of hyperparameters for the optimization.

.. GENERATED FROM PYTHON SOURCE LINES 17-21

Loading of the fire satellite use case
-----------------------------------------------------
This model involves 9 input variables and 3 output variables. We select only the first output variable in this example.
We load the :ref:`Fire satellite use case<use-case-fire-satellite>`.

.. GENERATED FROM PYTHON SOURCE LINES 23-29

.. code-block:: Python

    import openturns as ot
    from openturns.usecases.fire_satellite import FireSatelliteModel
    import openturns.viewer as otv
    import openturns.experimental as otexp









.. GENERATED FROM PYTHON SOURCE LINES 30-31

We define the function that evaluates the outputs depending on the inputs.

.. GENERATED FROM PYTHON SOURCE LINES 31-34

.. code-block:: Python

    m = FireSatelliteModel()
    model = m.model








.. GENERATED FROM PYTHON SOURCE LINES 35-36

We also define the distribution of input variables to build the training and test sets.

.. GENERATED FROM PYTHON SOURCE LINES 36-39

.. code-block:: Python

    inputDistribution = m.inputDistribution









.. GENERATED FROM PYTHON SOURCE LINES 40-43

Generation of data
------------------
We now generate the input and output training sets as 20 times the dimension of the input vector.

.. GENERATED FROM PYTHON SOURCE LINES 43-50

.. code-block:: Python

    experiment = ot.LHSExperiment(inputDistribution, 20 * m.dim)
    inputTrainingSet = experiment.generate()
    outputTrainingSet = model(inputTrainingSet).getMarginal(0)

    print("Lower and upper bounds of inputTrainingSet:")
    print(inputTrainingSet.getMin(), inputTrainingSet.getMax())





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Lower and upper bounds of inputTrainingSet:
    [1.54106e+07,875.065,1350.46,12.155,0.980915,0.235389,2.50699,0.895724,0.197372] [2.05626e+07,1124.35,1454.73,17.52,2.99648,0.767386,7.98289,3.05663,1.75649]




.. GENERATED FROM PYTHON SOURCE LINES 51-54

Creation of metamodel
---------------------
We choose to use a constant trend.

.. GENERATED FROM PYTHON SOURCE LINES 54-57

.. code-block:: Python

    basis = ot.LinearBasisFactory(m.dim).build()









.. GENERATED FROM PYTHON SOURCE LINES 58-59

For the purpose of illustration, we consider :class:`~openturns.MaternModel`.

.. GENERATED FROM PYTHON SOURCE LINES 59-62

.. code-block:: Python

    covarianceModel = ot.MaternModel(inputTrainingSet.computeRange() * 0.1, 2.5)









.. GENERATED FROM PYTHON SOURCE LINES 63-66

Training without normalization
------------------------------
First, we deactivate the normalization option for the optimization.

.. GENERATED FROM PYTHON SOURCE LINES 66-68

.. code-block:: Python

    ot.ResourceMap.SetAsBool("GaussianProcessFitter-OptimizationNormalization", False)








.. GENERATED FROM PYTHON SOURCE LINES 69-70

We run the algorithm and get the metamodel.

.. GENERATED FROM PYTHON SOURCE LINES 70-79

.. code-block:: Python

    fitter_algo = otexp.GaussianProcessFitter(
        inputTrainingSet, outputTrainingSet, covarianceModel, basis
    )
    fitter_algo.run()
    fitter_result = fitter_algo.getResult()
    gpr_algo = otexp.GaussianProcessRegression(fitter_result)
    gpr_algo.run()
    gpr_result = gpr_algo.getResult()








.. GENERATED FROM PYTHON SOURCE LINES 80-81

Inspect hyperparameters

.. GENERATED FROM PYTHON SOURCE LINES 81-85

.. code-block:: Python

    theta = gpr_result.getCovarianceModel().getParameter()
    print("theta=", theta)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    theta= [515199,25.7338,12.2715,3.30446,2.33972,1.02203,3.20161,0.829491,0.387141,0.00376948]#10




.. GENERATED FROM PYTHON SOURCE LINES 86-88

Validation of metamodel
To validate the metamodel, we create a validation set of size equal to 50 times the input vector dimension to evaluate the functions.

.. GENERATED FROM PYTHON SOURCE LINES 88-94

.. code-block:: Python

    gprMetamodel = gpr_result.getMetaModel()
    ot.RandomGenerator.SetSeed(1)
    experimentTest = ot.LHSExperiment(inputDistribution, 50 * m.dim)
    inputTestSet = experimentTest.generate()
    outputTestSet = model(inputTestSet).getMarginal(0)








.. GENERATED FROM PYTHON SOURCE LINES 95-96

Then, we use the :class:`~openturns.MetaModelValidation` class to validate the metamodel.

.. GENERATED FROM PYTHON SOURCE LINES 96-101

.. code-block:: Python

    metamodelPredictions = gprMetamodel(inputTestSet)
    val = ot.MetaModelValidation(outputTestSet, metamodelPredictions)
    r2Score = val.computeR2Score()
    print("R2=", r2Score)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    R2= [0.6223]




.. GENERATED FROM PYTHON SOURCE LINES 102-103

Graphical validation

.. GENERATED FROM PYTHON SOURCE LINES 103-113

.. code-block:: Python

    label = "Accuracy of metamodel without activating normalization for optimization"
    graph = val.drawValidation().getGraph(0, 0)
    graph.setLegends([""])
    graph.setLegends(["R2 = %.2f%%" % (100 * r2Score[0]), ""])
    graph.setLegendPosition("upper left")
    graph.setXTitle("Exact function")
    graph.setYTitle("Metamodel prediction")
    graph.setTitle(label)
    _ = otv.View(graph)




.. image-sg:: /auto_surrogate_modeling/gaussian_process_regression/images/sphx_glr_plot_gpr_normalization_001.svg
   :alt: Accuracy of metamodel without activating normalization for optimization
   :srcset: /auto_surrogate_modeling/gaussian_process_regression/images/sphx_glr_plot_gpr_normalization_001.svg
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 114-117

Training with normalization
---------------------------
Then, we activate the normalization option for the optimization.

.. GENERATED FROM PYTHON SOURCE LINES 119-121

.. code-block:: Python

    ot.ResourceMap.SetAsBool("GaussianProcessFitter-OptimizationNormalization", True)








.. GENERATED FROM PYTHON SOURCE LINES 122-123

We run the algorithm and get the metamodel.

.. GENERATED FROM PYTHON SOURCE LINES 125-134

.. code-block:: Python

    fitter_algo = otexp.GaussianProcessFitter(
        inputTrainingSet, outputTrainingSet, covarianceModel, basis
    )
    fitter_algo.run()
    fitter_result = fitter_algo.getResult()
    gpr_algo = otexp.GaussianProcessRegression(fitter_result)
    gpr_algo.run()
    gpr_result = gpr_algo.getResult()








.. GENERATED FROM PYTHON SOURCE LINES 135-136

Inspect hyperparameters: we can see that parameters are much different this time

.. GENERATED FROM PYTHON SOURCE LINES 136-139

.. code-block:: Python

    theta = gpr_result.getCovarianceModel().getParameter()
    print("theta=", theta)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    theta= [1.0304e+07,498.57,208.542,10.73,4.03113,1.06399,10.9518,2.55495,0.896912,0.00186723]#10




.. GENERATED FROM PYTHON SOURCE LINES 140-141

Validation of metamodel

.. GENERATED FROM PYTHON SOURCE LINES 141-147

.. code-block:: Python

    gprMetamodel = gpr_result.getMetaModel()
    metamodelPredictions = gprMetamodel(inputTestSet)
    val = ot.MetaModelValidation(outputTestSet, metamodelPredictions)
    r2Score = val.computeR2Score()
    print("R2=", r2Score)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    R2= [0.999505]




.. GENERATED FROM PYTHON SOURCE LINES 148-149

Graphical validation

.. GENERATED FROM PYTHON SOURCE LINES 149-159

.. code-block:: Python

    label = "Accuracy of metamodel with activating normalization for optimization"
    graph2 = val.drawValidation().getGraph(0, 0)
    graph2.setLegends([""])
    graph2.setLegends(["R2 = %.2f%%" % (100 * r2Score[0]), ""])
    graph2.setLegendPosition("upper left")
    graph2.setXTitle("Exact function")
    graph2.setYTitle("Metamodel prediction")
    graph2.setTitle(label)
    _ = otv.View(graph2)
    otv.View.ShowAll()



.. image-sg:: /auto_surrogate_modeling/gaussian_process_regression/images/sphx_glr_plot_gpr_normalization_002.svg
   :alt: Accuracy of metamodel with activating normalization for optimization
   :srcset: /auto_surrogate_modeling/gaussian_process_regression/images/sphx_glr_plot_gpr_normalization_002.svg
   :class: sphx-glr-single-img






.. _sphx_glr_download_auto_surrogate_modeling_gaussian_process_regression_plot_gpr_normalization.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_gpr_normalization.ipynb <plot_gpr_normalization.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_gpr_normalization.py <plot_gpr_normalization.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_gpr_normalization.zip <plot_gpr_normalization.zip>`
