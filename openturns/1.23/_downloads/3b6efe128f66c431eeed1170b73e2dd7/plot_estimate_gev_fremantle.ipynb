{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Estimate a GEV on the Fremantle sea-levels data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we illustrate various techniques of extreme value modeling applied\nto the annual maximum sea-levels recorded at Fremantle, near Perth, western Australia, over the period\n1897-1989.\nReaders should refer to [coles2001]_ to get more details.\n\nWe illustrate techniques to:\n\n- estimate a stationary and a non stationary GEV depending on time or on the covariates (time, SOI),\n- estimate a return level,\n\nusing:\n\n- the log-likelihood function,\n- the profile log-likelihood function.\n\nWe also illustrate the modelling with covariates.\n\nFirst, we load the Fremantle dataset of the annual maximum sea-levels. We start by looking at them\nthrough time. The data also contain the annual mean value of the Southern Oscillation Index (SOI),\nwhich is a proxy for meteorological volatility due to effects such as El Nino.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import openturns as ot\nimport openturns.viewer as otv\nimport openturns.experimental as otexp\nfrom openturns.usecases import coles\n\ndata = coles.Coles().fremantle\nprint(data[:5])\ngraph = ot.Graph(\n    \"Annual maximum sea-levels at Fremantle\", \"year\", \"level (m)\", True, \"\"\n)\ncloud = ot.Cloud(data[:, :2])\ncloud.setColor(\"red\")\ngraph.add(cloud)\ngraph.setIntegerXTick(True)\nview = otv.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We select the sea-levels column.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sample = data[:, 1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Stationary GEV modeling via the log-likelihood function**\n\nWe first assume that the dependence through time is negligible, so we first model the data as\nindependent observations over the observation period. We estimate the parameters of the\nGEV distribution by maximizing the log-likelihood of the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "factory = ot.GeneralizedExtremeValueFactory()\nresult_LL = factory.buildMethodOfLikelihoodMaximizationEstimator(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We get the fitted GEV and its parameters $(\\hat{\\mu}, \\hat{\\sigma}, \\hat{\\xi})$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fitted_GEV = result_LL.getDistribution()\ndesc = fitted_GEV.getParameterDescription()\nparam = fitted_GEV.getParameter()\nprint(\", \".join([f\"{p}: {value:.3f}\" for p, value in zip(desc, param)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We get the asymptotic distribution of the estimator $(\\hat{\\mu}, \\hat{\\sigma}, \\hat{\\xi})$.\nIn that case, the asymptotic distribution is normal.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "parameterEstimate = result_LL.getParameterDistribution()\nprint(\"Asymptotic distribution of the estimator : \")\nprint(parameterEstimate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We get the covariance matrix  and the standard deviation of $(\\hat{\\mu}, \\hat{\\sigma}, \\hat{\\xi})$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Cov matrix = \\n\", parameterEstimate.getCovariance())\nprint(\"Standard dev = \", parameterEstimate.getStandardDeviation())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We get the marginal confidence intervals of order 0.95.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "order = 0.95\nfor i in range(3):\n    ci = parameterEstimate.getMarginal(i).computeBilateralConfidenceInterval(order)\n    print(desc[i] + \":\", ci)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At last, we can validate the inference result thanks the 4 usual diagnostic plots:\n\n- the probability-probability pot,\n- the quantile-quantile pot,\n- the return level plot,\n- the empirical distribution function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "validation = otexp.GeneralizedExtremeValueValidation(result_LL, sample)\ngraph = validation.drawDiagnosticPlot()\nview = otv.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Stationary GEV modeling via the profile log-likelihood function**\n\nNow, we use the profile log-likehood function rather than log-likehood function  to estimate the parameters of the GEV.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result_PLL = factory.buildMethodOfXiProfileLikelihoodEstimator(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following graph allows one to get the profile log-likelihood plot.\nIt also indicates the optimal value of $\\xi$, the maximum profile log-likelihood and\nthe confidence interval for $\\xi$ of order 0.95 (which is the default value).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "order = 0.95\nresult_PLL.setConfidenceLevel(order)\nview = otv.View(result_PLL.drawProfileLikelihoodFunction())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can get the numerical values of the confidence interval: it appears to be a bit smaller\nthan the interval obtained with the log-likelihood function.\nNote that if the order requested is too high, the confidence interval might not be calculated because\none of its bound is out of the definition domain of the log-likelihood function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    print(\"Confidence interval for xi = \", result_PLL.getParameterConfidenceInterval())\nexcept Exception as ex:\n    print(type(ex))\n    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Return level estimate from the estimated stationary GEV**\n\nWe estimate the $m$-block return level $z_m$: it is computed as a particular quantile of the\nGEV model estimated using the log-likelihood function. We just have to use the maximum log-likelihood\nestimator built in the previous section.\n\nAs the data are annual sea-levels, each block corresponds to one year: the 10-year return level\ncorresponds to $m=10$ and the 100-year return level corresponds to $m=100$.\n\nThe method provides the asymptotic distribution of the estimator $\\hat{z}_m$\nwhich mean is the return-level estimate.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "zm_10 = factory.buildReturnLevelEstimator(result_LL, 10.0)\nreturn_level_10 = zm_10.getMean()\nprint(\"Maximum log-likelihood function : \")\nprint(f\"10-year return level = {return_level_10}\")\nreturn_level_ci10 = zm_10.computeBilateralConfidenceInterval(0.95)\nprint(f\"CI = {return_level_ci10}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "zm_100 = factory.buildReturnLevelEstimator(result_LL, 100.0)\nreturn_level_100 = zm_100.getMean()\nprint(f\"100-year return level = {return_level_100}\")\nreturn_level_ci100 = zm_100.computeBilateralConfidenceInterval(0.95)\nprint(f\"CI = {return_level_ci100}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Return level estimate via the profile log-likelihood function of a stationary GEV**\n\nWe can estimate the $m$-block return level $z_m$ directly from the data using the profile\nlikelihood with respect to $z_m$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result_zm_10_PLL = factory.buildReturnLevelProfileLikelihoodEstimator(sample, 10.0)\nzm_10_PLL = result_zm_10_PLL.getParameter()\nprint(f\"10-year return level (profile) = {zm_10_PLL}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can get the confidence interval of $z_m$:  once more, it appears to be a bit smaller\nthan the interval obtained from the log-likelihood function.\nAs for the confidence interval of $\\xi$, depending on the order requested, the interval might\nnot be calculated.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result_zm_10_PLL.setConfidenceLevel(0.95)\ntry:\n    return_level_ci10 = result_zm_10_PLL.getParameterConfidenceInterval()\nexcept Exception as ex:\n    print(type(ex))\n    pass\nprint(\"Maximum profile log-likelihood function : \")\nprint(f\"CI={return_level_ci10}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also plot the profile log-likelihood function and get the confidence interval, the optimal value\nof $z_m$ and its confidence interval.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "view = otv.View(result_zm_10_PLL.drawProfileLikelihoodFunction())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Non stationary GEV modeling via the log-likelihood function**\n\nIf we look at the data carefully, we see that the pattern of variation has not remained constant over\nthe observation period. There is an increase in the data through time.\nWe want to model this dependence because a slight increase in extreme sea-levels might have\na significant impact on the safety of coastal flood defenses.\n\nWe have define the functional basis for each parameter of the GEV model. Even if we have\nthe possibility to affect a time-varying model to each of the 3 parameters $(\\mu, \\sigma, \\xi)$,\nit is strongly recommended not to vary the parameter $\\xi$ and to let it constant.\n\nFor numerical reasons, it is strongly recommended to normalize all the data as follows:\n\n\\begin{align}\\tau(t) = \\dfrac{t-c}{d}\\end{align}\n\nwhere:\n\n- the *CenterReduce* method where $c = \\dfrac{1}{n} \\sum_{i=1}^n t_i$ is the mean time stamps\n  and $d = \\sqrt{\\dfrac{1}{n} \\sum_{i=1}^n (t_i-c)^2}$ is the standard deviation of the time stamps;\n- the *MinMax* method where $c = t_1$ is the initial time and $d = t_n-t_1$ the final time;\n- the *None* method where $c = 0$ and $d = 1$: in that case, data are not normalized.\n\nWe suppose that $\\mu$ is linear in time, and that the other parameters remain constant:\n\n\\begin{align}:nowrap:\n\n    \\begin{align*}\n      \\mu(t) & = \\beta_1 + \\beta_2\\tau(t) \\\\\n      \\sigma(t) & = \\beta_3 \\\\\n      \\xi(t) & = \\beta_4\n    \\end{align*}\\end{align}\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "constant = ot.SymbolicFunction([\"t\"], [\"1.0\"])\nbasis = ot.Basis([constant, ot.SymbolicFunction([\"t\"], [\"t\"])])\n# basis for mu, sigma, xi\nmuIndices = [0, 1]  # linear\nsigmaIndices = [0]  # stationary\nxiIndices = [0]  # stationary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "timeStamps = data[:, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now estimate the list of coefficients $\\vect{\\beta} = (\\beta_1, \\beta_2, \\beta_3, \\beta_4)$\nusing the log-likelihood of the data.\n\nWe test the 3 normalizing methods and both initial points in order to evaluate their impact on the results.\nWe can see that:\n\n- both normalization methods lead to the same result for $\\beta_1$, $\\beta_3$ and $\\beta_4$\n  (note that $\\beta_2$ depends on the normalization function),\n- both initial points lead to the same result when the data have been normalized,\n- it is very important to normalize all the data: if not, the result strongly depends on the initial point\n  and it differs from the result obtained with normalized data. The results are not optimal in that case\n  since the associated log-likelihood are much smaller than those obtained with normalized data.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Linear mu(t) model:\")\nfor normMeth in [\"MinMax\", \"CenterReduce\", \"None\"]:\n    for initPoint in [\"Gumbel\", \"Static\"]:\n        print(f\"normMeth = {normMeth}, initPoint = {initPoint}\")\n        # The ot.Function() is the identity function.\n        result = factory.buildTimeVarying(\n            sample,\n            timeStamps,\n            basis,\n            muIndices,\n            sigmaIndices,\n            xiIndices,\n            ot.Function(),\n            ot.Function(),\n            ot.Function(),\n            initPoint,\n            normMeth,\n        )\n        beta = result.getOptimalParameter()\n        print(f\"beta = {beta}\")\n        print(f\"Max log-likelihood = {result.getLogLikelihood()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "According to the previous results, we choose the *MinMax* normalization method and the *Gumbel* initial point.\nThis initial point is cheaper than the *Static* one as it requires no optimization computation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result_NonStatLL = factory.buildTimeVarying(\n    sample,\n    timeStamps,\n    basis,\n    muIndices,\n    sigmaIndices,\n    xiIndices,\n    ot.Function(),\n    ot.Function(),\n    ot.Function(),\n    \"Gumbel\",\n    \"MinMax\",\n)\nbeta = result_NonStatLL.getOptimalParameter()\nprint(f\"beta = {beta}\")\nprint(f\"mu(t) = {beta[0]:.4f} + {beta[1]:.4f} * tau(t)\")\nprint(f\"sigma = {beta[2]:.4f}\")\nprint(f\"xi = {beta[3]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can get the expression of the normalizing function $t \\mapsto \\tau(t)$:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "normFunc = result_NonStatLL.getNormalizationFunction()\nprint(\"Function tau(t): \", normFunc)\nprint(\"c = \", normFunc.getEvaluation().getImplementation().getCenter()[0])\nprint(\"1/d = \", normFunc.getEvaluation().getImplementation().getLinear()[0, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can get the function $t \\mapsto \\vect{\\theta}(t)$ where $\\vect{\\theta}(t) = (\\mu(t), \\sigma(t), \\xi(t))$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "functionTheta = result_NonStatLL.getParameterFunction()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We get the asymptotic distribution of $\\vect{\\beta}$ to compute some confidence intervals of\nthe estimates, for example of order $p = 0.95$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dist_beta = result_NonStatLL.getParameterDistribution()\ncondifence_level = 0.95\nfor i in range(beta.getSize()):\n    lower_bound = dist_beta.getMarginal(i).computeQuantile((1 - condifence_level) / 2)[\n        0\n    ]\n    upper_bound = dist_beta.getMarginal(i).computeQuantile((1 + condifence_level) / 2)[\n        0\n    ]\n    print(\n        \"Conf interval for beta_\"\n        + str(i + 1)\n        + \" = [\"\n        + str(lower_bound)\n        + \"; \"\n        + str(upper_bound)\n        + \"]\"\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to compare different modelings, we get the optimal log-likelihood of the data for both stationary\nand non stationary models. The difference is significant enough to be in favor of the non stationary model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Max log-likelihood: \")\nprint(\"Stationary model =  \", result_LL.getLogLikelihood())\nprint(\"Non stationary linear mu(t) model =  \", result_NonStatLL.getLogLikelihood())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to draw some diagnostic plots similar to those drawn in the stationary case, we refer to the\nfollowing result: if $Z_t$ is a non stationary GEV model parametrized by $(\\mu(t), \\sigma(t), \\xi(t))$,\nthen the standardized variables $\\hat{Z}_t$ defined by:\n\n\\begin{align}\\hat{Z}_t = \\dfrac{1}{\\xi(t)} \\log \\left[1+ \\xi(t)\\left( \\dfrac{Z_t-\\mu(t)}{\\sigma(t)} \\right)\\right]\\end{align}\n\nhave  the standard Gumbel distribution which is the GEV model with $(\\mu, \\sigma, \\xi) = (0, 1, 0)$.\n\nAs a result, we can validate the inference result thanks the 4 usual diagnostic plots:\n\n- the probability-probability pot,\n- the quantile-quantile pot,\n- the return level plot,\n- the data histogram and the density of the fitted model.\n\nusing the transformed data compared to the Gumbel model. We can see that the adequation is better\nthan with the stationary model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = result_NonStatLL.drawDiagnosticPlot()\nview = otv.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can draw the mean function  $t \\mapsto \\Expect{\\mbox{GEV}(t)}$. Be careful, it is not the function\n$t \\mapsto \\mu(t)$. As a matter of fact, the mean is defined for $\\xi <1$ only and in that case,\nfor $\\xi \\neq 0$, we have:\n\n\\begin{align}\\Expect{\\mbox{GEV}(t)} = \\mu(t) + \\dfrac{\\sigma(t)}{\\xi(t)} (\\Gamma(1-\\xi(t))-1)\\end{align}\n\nand for $\\xi = 0$, we have:\n\n\\begin{align}\\Expect{\\mbox{GEV}(t)} = \\mu(t) + \\sigma(t)\\gamma\\end{align}\n\nwhere $\\gamma$ is the Euler constant.\n\nWe can also draw the function $t \\mapsto q_p(t)$ where $q_p(t)$ is the quantile of\norder $p$ of the GEV distribution at time $t$.\nHere, $\\mu(t)$ is a linear function and the other parameters are constant, so the mean and the quantile\nfunctions are also linear functions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = ot.Graph(\n    r\"Annual maximum sea-levels at Fremantle - Linear $\\mu(t)$\",\n    \"year\",\n    \"level (m)\",\n    True,\n    \"\",\n)\ngraph.setIntegerXTick(True)\n# data\ncloud = ot.Cloud(data[:, :2])\ncloud.setColor(\"red\")\ngraph.add(cloud)\n# mean function\nmeandata = [\n    result_NonStatLL.getDistribution(t).getMean()[0] for t in data[:, 0].asPoint()\n]\ncurve_meanPoints = ot.Curve(data[:, 0].asPoint(), meandata)\ngraph.add(curve_meanPoints)\n# quantile function\ngraphQuantile = result_NonStatLL.drawQuantileFunction(0.95)\ndrawQuant = graphQuantile.getDrawable(0)\ndrawQuant = graphQuantile.getDrawable(0)\ndrawQuant.setLineStyle(\"dashed\")\ngraph.add(drawQuant)\ngraph.setLegends([\"data\", \"mean function\", \"quantile 0.95  function\"])\ngraph.setLegendPosition(\"lower right\")\nview = otv.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At last, we can test the validity of the stationary model $\\mathcal{M}_0$\nrelative to the model with time varying parameters  $\\mathcal{M}_1$. The\nmodel $\\mathcal{M}_0$ is parametrized by $(\\beta_1, \\beta_3, \\beta_4)$ and the model\n$\\mathcal{M}_1$ is parametrized by $(\\beta_1, \\beta_2, \\beta_3, \\beta_4)$: so we have\n$\\mathcal{M}_0 \\subset \\mathcal{M}_1$.\n\nWe use the Likelihood Ratio test. The null hypothesis is the stationary model $\\mathcal{M}_0$.\nThe Type I error $\\alpha$ is taken equal to 0.05.\n\nThis test confirms that the dependence through time is not negligible: it means that the linear $\\mu(t)$\ncomponent explains a large variation in the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "llh_LL = result_LL.getLogLikelihood()\nllh_NonStatLL = result_NonStatLL.getLogLikelihood()\nmodelM0_Nb_param = 3\nmodelM1_Nb_param = 4\nresultLikRatioTest = ot.HypothesisTest.LikelihoodRatioTest(\n    modelM0_Nb_param, llh_LL, modelM1_Nb_param, llh_NonStatLL, 0.05\n)\naccepted = resultLikRatioTest.getBinaryQualityMeasure()\nprint(\n    f\"Hypothesis H0 (stationary model) vs H1 (linear mu(t) model):  accepted ? = {accepted}\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We detail the statistics of the Likelihood Ratio test: the deviance statistics $\\mathcal{D}_p$ follows\na $\\chi^2_1$ distribution.\nThe model $\\mathcal{M}_0$ is rejected if the deviance statistics estimated on the data is greater than\nthe threshold $c_{\\alpha}$ or if the p-value is less than the Type I error  $\\alpha = 0.05$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(f\"Dp={resultLikRatioTest.getStatistic():.2f}\")\nprint(f\"alpha={resultLikRatioTest.getThreshold():.2f}\")\nprint(f\"p-value={resultLikRatioTest.getPValue():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can perform the same study with a quadratic model for $\\mu(t)$ or a linear model for\n$\\mu(t)$ and $\\sigma(t)$:\n\n\\begin{align}:nowrap:\n\n    \\begin{align*}\n      \\mu(t) & = \\beta_1 + \\beta_2 \\tau(t) + \\beta_3\\tau(t)^2 \\\\\n      \\sigma(t) & = \\beta_4 \\\\\n      \\xi(t) & = \\beta_5\n    \\end{align*}\\end{align}\n\nor\n\n\\begin{align}:nowrap:\n\n    \\begin{align*}\n    \\mu(t) & = \\beta_1 + \\beta_2 \\tau(t) \\\\\n    \\sigma(t) & = \\beta_3 + \\beta_4\\tau(t)\\\\\n    \\xi(t) & = \\beta_5\n    \\end{align*}\\end{align}\n\nFor each model, we give the log-likelihood values and we test the validity of each model with respect\nto the non stationary model where $\\mu(t)$ is linear.\nWe notice that there is no evidence to adopt a quadratic model for $\\mu(t)$ nor a linear model\nfor $\\mu(t)$ and $\\sigma(t)$: the optimal log-likelihood for each model is very near the likelihood\nwe obtained with a linear model for $\\mu(t)$ only. It means that these both models do not bring significant\nimprovements with respect to model tested before.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "basis = ot.Basis(\n    [constant, ot.SymbolicFunction([\"t\"], [\"t\"]), ot.SymbolicFunction([\"t\"], [\"t^2\"])]\n)\nresult_NonStatLL_2 = factory.buildTimeVarying(\n    sample,\n    timeStamps,\n    basis,\n    [0, 1, 2],\n    [0],\n    [0],\n    ot.Function(),\n    ot.Function(),\n    ot.Function(),\n    \"Gumbel\",\n    \"MinMax\",\n)\nresult_NonStatLL_3 = factory.buildTimeVarying(\n    sample,\n    timeStamps,\n    basis,\n    [0, 1],\n    [0, 1],\n    [0],\n    ot.Function(),\n    ot.Function(),\n    ot.Function(),\n    \"Gumbel\",\n    \"MinMax\",\n)\nprint(\"Max log-likelihood = \")\nprint(\"Non stationary quadratic mu(t) model = \", result_NonStatLL_2.getLogLikelihood())\nprint(\n    \"Non stationary linear mu(t) and sigma(t) model = \",\n    result_NonStatLL_3.getLogLikelihood(),\n)\nllh_LL = result_LL.getLogLikelihood()\nllh_NonStatLL_2 = result_NonStatLL_2.getLogLikelihood()\nllh_NonStatLL_3 = result_NonStatLL_3.getLogLikelihood()\nresultLikRatioTest_2 = ot.HypothesisTest.LikelihoodRatioTest(\n    4, llh_NonStatLL, 5, llh_NonStatLL_2, 0.05\n)\nresultLikRatioTest_3 = ot.HypothesisTest.LikelihoodRatioTest(\n    4, llh_NonStatLL, 5, llh_NonStatLL_3, 0.05\n)\naccepted_2 = resultLikRatioTest_2.getBinaryQualityMeasure()\naccepted_3 = resultLikRatioTest_3.getBinaryQualityMeasure()\nprint(\n    f\"Hypothesis H0 (linear mu(t) model) vs H1 (quadratic mu(t) model):  accepted ? = {accepted_2}\"\n)\nprint(\n    f\"Hypothesis H0 (linear mu(t) model) vs H1 (linear mu(t) and sigma(t) model):  accepted ? = {accepted_3}\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Non stationary GEV modeling with the covariates Time and SOI**\n\nExtreme sea-levels can be unusually extreme during periods when the El Nino effect is\nactive.\nHence, we study a modeling that takes into account the dependence between the extreme\nsea-levels\nat Fremantle and the annual mean value of the Southern Oscillation Index (SOI)\nbesides the temporal dependence.\nThe following figure shows that the annual maximum sea-levels are generally greater\nwhen the value of SOI is high. It might be due to the time trend in the data for the\nsea-levels\nand the SOI (each one increases with time). But it can also be possible that the\nSOI explains\nsome of the variation in annual maximum sea-levels after allowance for the time variation\nin the process.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graph = ot.Graph(\"SOI at Fremantle\", \"SOI\", \"level (m)\", True, \"\")\ncloud = ot.Cloud(data.getMarginal([2, 1]))\ncloud.setColor(\"red\")\ngraph.add(cloud)\nview = otv.View(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To consider this possibility, we study the model:\n\n\\begin{align}:nowrap:\n\n    \\begin{align*}\n      \\mu(t) & = \\beta_1 t + \\beta_2 \\mbox{SOI} + \\beta_3 \\\\\n      \\sigma(t) & = \\beta_4 \\\\\n      \\xi(t) & = \\beta_5\n    \\end{align*}\\end{align}\n\nWe consider two covariates: the time and the SOI. We build the sample of the values of both\ncovariates: $(t_i, \\mbox{SOI}_i)_{1 \\leq i \\leq n}$ where $\\mbox{SOI}_i =\nSOI(t_i)$.\nThe constant covariate is\nautomatically added by the library if not specified in order to allow\nsome of the GEV parameters to remain constant (ie independent of both covariates\n$(t, \\mbox{SOI})$):\nthis is the case for the $\\sigma$ and $\\xi$ parameters.\nThis last constant covariate is associated to the\nthird component of the covariates sample which now gathers the values\n$(t_i, \\mbox{SOI}_i, 1)$ for $1 \\leq i \\leq n$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataCovariates = data.getMarginal([0, 2])\nprint(dataCovariates[0:10])\nresult_Cov = factory.buildCovariates(sample, dataCovariates, [0, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We check here that a third component has effectively been added to the covariates\nsample: see the added third column which is constant equal to 1.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(result_Cov.getCovariates()[0:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We get the optimal parameter $\\vect{\\beta}$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "beta = result_Cov.getOptimalParameter()\nprint(\"beta = \", beta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We get here the function $(\\vect{\\beta}, t, \\mbox{SOI}) \\mapsto \\vect{\\theta}\n(\\vect{\\beta}, t, \\mbox{SOI})$ where $\\vect{\\theta} = (\\mu, \\sigma, \\xi)$. We see that\n$\\mu$ depends on the three\ncovariates $(t, \\mbox{SOI}, 1)$ and that $\\sigma$ and $\\xi$ depends\nonly on the third one\nwhich is the constant one.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(result_Cov.getParameterFunction())\nprint(f\"beta = {beta}\")\nprint(f\"mu(t) = {beta[0]:.4f} *t + {beta[1]:.4f} * SOI(t) + {beta[2]:.4f}\")\nprint(f\"sigma = {beta[3]:.4f}\")\nprint(f\"xi = {beta[4]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We check here the normalizing function that has been used, which comes from\nthe default method (the *MinMax* one).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(result_Cov.getNormalizationFunction())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We test this new model where $\\mu(t,SOI)$ is modeled as a linear combination\nof the three covariates  $(t, \\mbox{SOI}, 1)$ against the model\nwith the linear-trend only $\\mu(t)$. The maximized log-likelihood of this\nnew model is 53.9, compared to 49.9 for the first model. Hence, the\ndeviance statistics is equal to $D = 8.0$, which is large when judged relative to\na $\\chi_1^2$ distribution.\nIt provides evidence that the effect of SOI is influential on annual maximum\nsea-levels at Fremantle, even after the allowance for time-variation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "llh_cov = result_Cov.getLogLikelihood()\nprint(\"Max log-likelihood: \", llh_cov)\nresultLikRatioTest_SOI = ot.HypothesisTest.LikelihoodRatioTest(\n    4, llh_NonStatLL, 5, llh_cov, 0.05\n)\nprint(f\"Dp={resultLikRatioTest_SOI.getStatistic():.2f}\")\naccepted = resultLikRatioTest_SOI.getBinaryQualityMeasure()\nprint(\n    f\"Hypothesis H0 (linear-trend mu(t) model) vs H1 (linear-trend and SOI mu(t,SOI) model):  accepted ? = {accepted}\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We plot here the graphs $t \\mapsto \\mu(t, \\mbox{SOI}_0)$ where\n$\\mbox{SOI}_0$ is a given value (the mean value of the sample if not specified),\nand $\\mbox{SOI} \\mapsto \\mu(t_0, \\mbox{SOI})$ where $t_0$ is a given time.\nCare: there are three covariates $(t, SOI, 1)$ for the reasons mentioned previously.\nThen the reference point must be of dimension 3.\n\nAs the relation is linear (the link function is the Identity function), we get some straight\nlines.\nThe third graph is the dependence on the third covariate which is constant.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "refSOI = dataCovariates.computeMean()[1]\nrefTime = 1940\nrefPoint = [refTime, refSOI, 1]\ngridMu = result_Cov.drawParameterFunction1D(0, refPoint)\nview = otv.View(gridMu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To adapt the labels and get rid of the last graph:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graphCol = gridMu.getGraphCollection()\ngraphMu1 = graphCol[0]\ngraphMu1.setTitle(r\"$t \\mapsto \\mu(t, SOI_0)$, $SOI_0$ = {0:.2f}\".format(refSOI))\ngraphMu1.setXTitle(\"t\")\ngraphMu2 = graphCol[1]\ngraphMu2.setTitle(r\"$SOI \\mapsto \\mu(t_0, SOI)$, $t_0 = $\" + str(refTime))\ngraphMu2.setXTitle(\"SOI\")\nnewGridLayout = ot.GridLayout(1, 2)\nnewGridLayout.setGraph(0, 0, graphMu1)\nnewGridLayout.setGraph(0, 1, graphMu2)\nview = otv.View(newGridLayout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We plot here the graph $(t, SOI) \\mapsto \\mu(t, SOI)$.\nAs the third covariate is constant, the other graphs $(t, 1)\n\\mapsto \\mu(t, \\mbox{SOI}_0, 1)$\nand $(1, SOI) \\mapsto \\mu(t_0, 1, SOI)$ are not interesting\nas we have already obtained them with the previous method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graphCol = result_Cov.drawParameterFunction2D(0, refPoint)\nview = otv.View(graphCol)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We plot here the graphs $t \\mapsto q_p(Z_{t, \\mbox{SOI}_0})$\nand $\\mbox{SOI} \\mapsto q_p(Z_{t_0, \\mbox{SOI}})$ where $Z_{t, \\mbox{SOI}_0}$\nis the process whose excesses of $u$ follow the estimated GPD,\ndepending on the covariates $(t, SOI)$. Then $q_p$\nis the quantile of order $p$.\nBecause of the constant third covariate, the last graph is reduced to a point.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "p = 0.95\ngridQuantile = result_Cov.drawQuantileFunction1D(p, refPoint)\nview = otv.View(gridQuantile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To adapt the labels and get rid of the last graph:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "graphCol = gridQuantile.getGraphCollection()\ngraphQuant1 = graphCol[0]\ngraphQuant1.setTitle(r\"$t \\mapsto q_p(Z(t, SOI_0))$, $SOI_0$ = {0:.2f}\".format(refSOI))\ngraphQuant1.setXTitle(\"t\")\ngraphQuant1.setYTitle(r\"$q_p$\")\ngraphQuant2 = graphCol[1]\ngraphQuant2.setTitle(r\"$SOI \\mapsto q_p(Z(t_0, SOI))$, $t_0 = $\" + str(refTime))\ngraphQuant2.setXTitle(\"SOI\")\ngraphQuant2.setYTitle(r\"$q_p$\")\nnewGridLayout = ot.GridLayout(1, 2)\nnewGridLayout.setGraph(0, 0, graphQuant1)\nnewGridLayout.setGraph(0, 1, graphQuant2)\nview = otv.View(newGridLayout)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "otv.View.ShowAll()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}