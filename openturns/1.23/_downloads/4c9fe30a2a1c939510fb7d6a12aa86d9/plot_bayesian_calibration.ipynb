{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Bayesian calibration of a computer code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example we compute the parameters of a computer model thanks\nto Bayesian estimation.\nWe use the :class:`~openturns.RandomWalkMetropolisHastings` and\n:class:`~openturns.Gibbs` classes\nand simulate a sample of the posterior distribution using\n`metropolis_hastings`.\n\nLet us denote $(y_1, \\dots, y_n)$ the observation sample,\n$(\\vect z_1, \\ldots, \\vect z_n) = (f(x_1|\\vect\\theta), \\ldots, f(x_n|\\vect\\theta))$ the model prediction,\n$p(y |\\vect z)$ the density function of observation $y$\nconditional on model prediction $\\vect z$,\nand $\\vect\\theta \\in \\mathbb{R}^p$ the calibration parameters we wish to estimate.\n\n\nThe posterior distribution is given by Bayes theorem:\n\n\\begin{align}\\pi(\\vect\\theta | \\vect y) \\quad \\propto \\quad L\\left(\\vect y | \\vect\\theta\\right) \\times \\pi(\\vect\\theta)\\end{align}\n\nwhere $\\propto$ means \"proportional to\", regarded as a function of $\\vect\\theta$.\n\nThe posterior distribution is approximated here by the empirical distribution\nof the sample $\\vect\\theta^1, \\ldots, \\vect\\theta^N$ generated by the Metropolis-Hastings algorithm.\nThis means that any quantity characteristic of the posterior distribution\n(mean, variance, quantile, ...) is approximated by its empirical counterpart.\n\nOur model (i.e. the compute code to calibrate) is a standard normal linear regression, where\n\n\\begin{align}y_i = \\theta_1 + x_i \\theta_2 + x_i^2 \\theta_3 + \\varepsilon_i\\end{align}\n\nwhere $\\varepsilon_i \\stackrel{i.i.d.}{\\sim} \\mathcal N(0, 1)$.\n\nThe \"true\" value of $\\theta$ is:\n\n\\begin{align}\\vect \\theta_{true} = (-4.5,4.8,2.2)^T.\\end{align}\n\n\nWe use a normal prior on $\\vect\\theta$:\n\n\\begin{align}\\pi(\\vect\\theta) = \\mathcal N(\\vect{\\mu}_\\vect{\\theta}, \\mat{\\Sigma}_\\vect{\\theta})\\end{align}\n\nwhere\n\n\\begin{align}\\vect{\\mu}_\\vect{\\theta} =\n    \\begin{pmatrix}\n     -3 \\\\\n      4 \\\\\n      1\n    \\end{pmatrix}\\end{align}\n\nis the mean of the prior and\n\n\\begin{align}\\mat{\\Sigma}_\\vect{\\theta} =\n   \\begin{pmatrix}\n     \\sigma_{\\theta_1}^2 & 0 & 0 \\\\\n     0 & \\sigma_{\\theta_2}^2 & 0 \\\\\n     0 & 0 & \\sigma_{\\theta_3}^2\n   \\end{pmatrix}\\end{align}\n\nis the prior covariance matrix with\n\n\\begin{align}\\sigma_{\\theta_1} = 2, \\qquad \\sigma_{\\theta_2} = 1, \\qquad \\sigma_{\\theta_3} = 1.5.\\end{align}\n\nThe following objects need to be defined in order to perform Bayesian calibration:\n\n- The conditional density $p(y|\\vect z)$ must be defined as a probability distribution.\n- The computer model must be implemented thanks to the :class:`~openturns.ParametricFunction` class.\n  This takes a value of $\\vect\\theta$ as input, and outputs the vector of model predictions $\\vect z$,\n  as defined above (the vector of covariates $\\vect x = (x_1, \\ldots, x_n)$ is treated as a known constant).\n  When doing that, we have to keep in mind that $\\vect z$ will be used as the vector of parameters corresponding\n  to the distribution specified for $p(y |\\vect z)$. For instance, if $p(y|\\vect z)$ is normal,\n  this means that $\\vect z$ must be a vector containing the mean and standard deviation of $y$.\n- The prior density $\\pi(\\vect\\theta)$ encoding the set of possible values for the calibration parameters,\n  each value being weighted by its a priori probability, reflecting the beliefs about the possible values\n  of $\\vect\\theta$ before consideration of the experimental data.\n  Again, this is implemented as a probability distribution.\n- Metropolis-Hastings algorithm(s), possibly used in tandem with a Gibbs algorithm\n  in order to sample from the posterior distribution of the calibration parameters.\n\nThis example uses the :class:`~openturns.ParametricFunction` class.\nPlease read its documentation and\n:doc:`/auto_functional_modeling/vectorial_functions/plot_parametric_function`\nfor a detailed example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import openturns as ot\nimport openturns.viewer as viewer\nfrom matplotlib import pylab as plt\n\not.Log.Show(ot.Log.NONE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dimension of the vector of parameters to calibrate\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "paramDim = 3\n# The number of obesrvations\nobsSize = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the observed inputs $x_i$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "xmin = -2.0\nxmax = 3.0\nstep = (xmax - xmin) / (obsSize - 1)\nrg = ot.RegularGrid(xmin, step, obsSize)\nx_obs = rg.getVertices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the parametric model $\\vect z = f(x,\\vect\\theta)$ that associates each\nobservation $x$ and value of $\\vect \\theta$ to the parameters\nof the distribution of the corresponding observation $y$:\nhere $\\vect z=(\\mu, \\sigma)$ where $\\mu$,\nthe first output of the model, is the mean and $\\sigma$,\nthe second output of the model, is the standard deviation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fullModel = ot.SymbolicFunction(\n    [\"x\", \"theta1\", \"theta2\", \"theta3\"], [\"theta1+theta2*x+theta3*x^2\", \"1.0\"]\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To differentiate between the two classes of inputs ($x$ and $\\vect\\theta$),\nwe define a :class:`~openturns.ParametricFunction` from `fullModel`\nand make the first input (the observations $x$) its *parameter*:\n$f_x(\\vect \\theta) := f(x, \\vect \\theta)$.\nWe set $x = 1$ as a placeholder,\nbut $x$ will actually take the values $x_i$ of the observations\nwhen we sample $\\vect\\theta$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "linkFunction = ot.ParametricFunction(fullModel, [0], [1.0])\nprint(linkFunction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the observation noise $\\varepsilon {\\sim} \\mathcal N(0, 1)$ and create a sample from it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ot.RandomGenerator.SetSeed(0)\nnoiseStandardDeviation = 1.0\nnoise = ot.Normal(0, noiseStandardDeviation)\nnoiseSample = noise.getSample(obsSize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the vector of observations $y_i$,\nhere sampled using the \"true\" value of $\\vect \\theta$: $\\vect \\theta_{true}$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "thetaTrue = [-4.5, 4.8, 2.2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y_obs = ot.Sample(obsSize, 1)\nfor i in range(obsSize):\n    linkFunction.setParameter(x_obs[i])\n    y_obs[i, 0] = linkFunction(thetaTrue)[0] + noiseSample[i, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Draw the model predictions vs the observations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "functionnalModel = ot.ParametricFunction(fullModel, [1, 2, 3], thetaTrue)\ngraphModel = functionnalModel.getMarginal(0).draw(xmin, xmax)\nobservations = ot.Cloud(x_obs, y_obs)\ngraphModel.add(observations)\ngraphModel.setLegends([\"Model\", \"Observations\"])\ngraphModel.setLegendPosition(\"upper left\")\nview = viewer.View(graphModel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the distribution of observations $y | \\vect{z}$ conditional on model predictions.\n\nNote that its parameter dimension is the one of $\\vect{z}$, so the model must be adjusted accordingly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "conditional = ot.Normal()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define the mean $\\mu_\\theta$, the covariance matrix $\\Sigma_\\theta$, then the prior distribution $\\pi(\\vect\\theta)$ of the parameter $\\vect\\theta$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "thetaPriorMean = [-3.0, 4.0, 1.0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sigma0 = [2.0, 1.0, 1.5]  # standard deviations\nthetaPriorCovarianceMatrix = ot.CovarianceMatrix(paramDim)\nfor i in range(paramDim):\n    thetaPriorCovarianceMatrix[i, i] = sigma0[i] ** 2\n\nprior = ot.Normal(thetaPriorMean, thetaPriorCovarianceMatrix)\nprior.setDescription([\"theta1\", \"theta2\", \"theta3\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The proposed steps for\n$\\theta_1$, $\\theta_2$ and $\\theta_3$\nwill all follow a uniform distribution.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "proposal = ot.Uniform(-1.0, 1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test the Metropolis-Hastings sampler\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creation of a single component random walk Metropolis-Hastings (RWMH) sampler.\nThis involves a combination of the RWMH and the Gibbs algorithms.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "initialState = thetaPriorMean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create a :class:`~openturns.RandomWalkMetropolisHastings` sampler for each component.\nEach sampler must be aware of the joint prior distribution.\nWe also use the same proposal distribution, but this is not mandatory.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mh_coll = [\n    ot.RandomWalkMetropolisHastings(prior, initialState, proposal, [i])\n    for i in range(paramDim)\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Each sampler must be made aware of the likelihood.\nOtherwise we would sample from the prior!\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for mh in mh_coll:\n    mh.setLikelihood(conditional, y_obs, linkFunction, x_obs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, the :class:`~openturns.Gibbs` algorithm is constructed from all Metropolis-Hastings samplers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sampler = ot.Gibbs(mh_coll)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate a sample from the posterior distribution of the parameters $\\vect \\theta$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sampleSize = 10000\nsample = sampler.getSample(sampleSize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Look at the acceptance rate (basic check of the sampling efficiency:\nvalues close to $0.2$ are usually recommended\nfor Normal posterior distributions).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "[mh.getAcceptanceRate() for mh in sampler.getMetropolisHastingsCollection()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build the distribution of the posterior by kernel smoothing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "kernel = ot.KernelSmoothing()\nposterior = kernel.build(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display prior vs posterior for each parameter.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_bayesian_prior_vs_posterior_pdf(prior, posterior):\n    \"\"\"\n    Plot the prior and posterior distribution of a Bayesian calibration\n\n    Parameters\n    ----------\n    prior : ot.Distribution(dimension)\n        The prior.\n    posterior : ot.Distribution(dimension)\n        The posterior.\n\n    Return\n    ------\n    grid : ot.GridLayout(1, dimension)\n        The prior and posterior PDF for each marginal.\n    \"\"\"\n    palette = ot.Drawable.BuildDefaultPalette(2)\n    paramDim = prior.getDimension()\n    grid = ot.GridLayout(1, paramDim)\n    parameterNames = prior.getDescription()\n    for parameter_index in range(paramDim):\n        graph = ot.Graph(\"\", parameterNames[parameter_index], \"PDF\", True)\n        # Prior\n        curve = prior.getMarginal(parameter_index).drawPDF().getDrawable(0)\n        curve.setLineStyle(\n            ot.ResourceMap.GetAsString(\"CalibrationResult-PriorLineStyle\")\n        )\n        curve.setLegend(\"Prior\")\n        graph.add(curve)\n        # Posterior\n        curve = posterior.getMarginal(parameter_index).drawPDF().getDrawable(0)\n        curve.setLineStyle(\n            ot.ResourceMap.GetAsString(\"CalibrationResult-PosteriorLineStyle\")\n        )\n        curve.setLegend(\"Posterior\")\n        graph.add(curve)\n        #\n        if parameter_index < paramDim - 1:\n            graph.setLegends([\"\"])\n        if parameter_index > 0:\n            graph.setYTitle(\"\")\n        graph.setColors(palette)\n        graph.setLegendPosition(\"upper right\")\n        grid.setGraph(0, parameter_index, graph)\n    grid.setTitle(\"Bayesian calibration\")\n    return grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "sphinx_gallery_thumbnail_number = 2\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "grid = plot_bayesian_prior_vs_posterior_pdf(prior, posterior)\nviewer.View(\n    grid,\n    figure_kw={\"figsize\": (8.0, 3.0)},\n    legend_kw={\"bbox_to_anchor\": (1.0, 1.0), \"loc\": \"upper left\"},\n)\nplt.subplots_adjust(right=0.8, bottom=0.2, wspace=0.3)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}