
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_functional_modeling/vectorial_functions/plot_functions_grad_hess.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_functional_modeling_vectorial_functions_plot_functions_grad_hess.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_functional_modeling_vectorial_functions_plot_functions_grad_hess.py:


=========================================
Get the gradient and hessian as functions
=========================================

.. GENERATED FROM PYTHON SOURCE LINES 8-20

Context
=======

Let :math:`f : \mathbb{R}^\inputDim \mapsto \mathbb{R}^q` a vectorial function defined by
its component functions :math:`f_i: \mathbb{R}^d \mapsto \mathbb{R}`. We want to get its gradient
and hessian as functions. The way to get it depends on the type of the function :math:`f`.

We can do that:

- Case 1: This case covers the :class:`~openturns.SymbolicFunction`,
- Case 2: This case covers all the general case.


.. GENERATED FROM PYTHON SOURCE LINES 20-22

.. code-block:: Python

    import openturns as ot








.. GENERATED FROM PYTHON SOURCE LINES 23-47

Case 1: The :class:`~openturns.SymbolicFunction`
================================================

Here, the function :math:`f` is a :class:`~openturns.SymbolicFunction`. In this
particular case, we have access to the analytical
expression of the gradient and hessian functions.

Let :math:`f : \Rset^2 \rightarrow \Rset^2` be defined by:

.. math::
   f(\vect{x}) = (f_1(x_1, x_2), f_2(x_1, x_2)) = (x_1^2 + 2x_2^2, x_1 + 3x_2^4)

We want to define the gradient function :math:`\Rset^2 \rightarrow \Rset^4` defined by:

.. math::
   :label: gradientFct

   \vect{x} \rightarrow & = (\dfrac{\partial f_1 (\vect{x})}{\partial x_1},
                    \dfrac{\partial f_1 (\vect{x})}{\partial x_2},
                    \dfrac{\partial f_2 (\vect{x})}{\partial x_1},
                    \dfrac{\partial f_2 (\vect{x})}{\partial x_2})\\
               & = (2x_1, 4x_2, 1, 12x_2^3)

To get the gradient as a function, we first get the analytical expressions of the derivatives:

.. GENERATED FROM PYTHON SOURCE LINES 47-53

.. code-block:: Python

    f = ot.SymbolicFunction(["x1", "x2"], ["x1^2 + 2 * x2^2", "x1 + 3 * x2^4"])
    formula_f1_deriv_x1 = f.getGradient().getImplementation().getFormula(0, 0)
    formula_f1_deriv_x2 = f.getGradient().getImplementation().getFormula(1, 0)
    formula_f2_deriv_x1 = f.getGradient().getImplementation().getFormula(0, 1)
    formula_f2_deriv_x2 = f.getGradient().getImplementation().getFormula(1, 1)








.. GENERATED FROM PYTHON SOURCE LINES 54-55

Then we create a new :class:`~openturns.SymbolicFunction` from these analytical expressions:

.. GENERATED FROM PYTHON SOURCE LINES 55-66

.. code-block:: Python

    gradient_AsFunction = ot.SymbolicFunction(
        ["x1", "x2"],
        [
            formula_f1_deriv_x1,
            formula_f1_deriv_x2,
            formula_f2_deriv_x1,
            formula_f2_deriv_x2,
        ],
    )
    print(gradient_AsFunction)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [x1,x2]->[2*x1,4*x2,1,12*x2^3]




.. GENERATED FROM PYTHON SOURCE LINES 67-81

We want to define the hessian function :math:`\Rset^2 \rightarrow \Rset^6` defined by:

.. math::
   :label: hessianFct

   \vect{x} \rightarrow & = (\dfrac{\partial^2 f_1 (\vect{x})}{\partial x_1^2},
                   \dfrac{\partial^2 f_1 (\vect{x})}{ \partial x_1\partial x_2},
                   \dfrac{\partial^2 f_1 (\vect{x})}{\partial x_2^2},
                   \dfrac{\partial f_2 (\vect{x})}{\partial x_1^2},
                   \dfrac{\partial f_2 (\vect{x})}{ \partial x_1\partial x_2}),
                   \dfrac{\partial^2 f_2 (\vect{x})}{\partial x_2^2}\\
               & = (2, 0 , 4, 0, 0, 36x_2^2)

To get the hessian as a function, we first get the analytical expressions of the second derivatives:

.. GENERATED FROM PYTHON SOURCE LINES 81-88

.. code-block:: Python

    formula_f1_hessian_x1x1 = f.getHessian().getImplementation().getFormula(0, 0, 0)
    formula_f1_hessian_x1x2 = f.getHessian().getImplementation().getFormula(1, 0, 0)
    formula_f1_hessian_x2x2 = f.getHessian().getImplementation().getFormula(1, 1, 0)
    formula_f2_hessian_x1x1 = f.getHessian().getImplementation().getFormula(0, 0, 1)
    formula_f2_hessian_x1x2 = f.getHessian().getImplementation().getFormula(1, 0, 1)
    formula_f2_hessian_x2x2 = f.getHessian().getImplementation().getFormula(1, 1, 1)








.. GENERATED FROM PYTHON SOURCE LINES 89-90

Then we create a new :class:`~openturns.SymbolicFunction` from these analytical expressions:

.. GENERATED FROM PYTHON SOURCE LINES 90-103

.. code-block:: Python

    hessian_AsFunction = ot.SymbolicFunction(
        ["x1", "x2"],
        [
            formula_f1_hessian_x1x1,
            formula_f1_hessian_x1x2,
            formula_f1_hessian_x2x2,
            formula_f2_hessian_x1x1,
            formula_f2_hessian_x1x2,
            formula_f2_hessian_x2x2,
        ],
    )
    print(hessian_AsFunction)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [x1,x2]->[2,0,4,0,0,36*x2^2]




.. GENERATED FROM PYTHON SOURCE LINES 104-112

Case 2: The general case
========================

Here, we consider a function :math:`f` such that we do not have access to the analytical
expression of its gradient and hessian functions.

To get the gradient function as a function defined in :eq:`gradientFct`, we have to use a
:class:`~openturns.PythonFunction`. We re-use the previous function for for educational purposes.

.. GENERATED FROM PYTHON SOURCE LINES 112-125

.. code-block:: Python



    def gradient_AsFunction_Python(inPoint):
        f1_deriv_x1 = f.gradient(inPoint)[0, 0]
        f1_deriv_x2 = f.gradient(inPoint)[1, 0]
        f2_deriv_x1 = f.gradient(inPoint)[0, 1]
        f2_deriv_x2 = f.gradient(inPoint)[1, 1]
        return [f1_deriv_x1, f1_deriv_x2, f2_deriv_x1, f2_deriv_x2]


    gradient_AsFunction_OT = ot.PythonFunction(2, 4, gradient_AsFunction_Python)
    print(gradient_AsFunction_OT([1.0, 2.0]))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [2,8,1,96]




.. GENERATED FROM PYTHON SOURCE LINES 126-127

To get the hessian function as a functiond efined in :eq:`hessianFct`, we do the same:

.. GENERATED FROM PYTHON SOURCE LINES 127-148

.. code-block:: Python



    def hessian_AsFunction_Python(inPoint):
        f1_hessian_x1x1 = f.hessian(inPoint)[0, 0, 0]
        f1_hessian_x1x2 = f.hessian(inPoint)[1, 0, 0]
        f1_hessian_x2x2 = f.hessian(inPoint)[1, 1, 0]
        f2_hessian_x1x1 = f.hessian(inPoint)[0, 0, 1]
        f2_hessian_x1x2 = f.hessian(inPoint)[1, 0, 1]
        f2_hessian_x2x2 = f.hessian(inPoint)[1, 1, 1]
        return [
            f1_hessian_x1x1,
            f1_hessian_x1x2,
            f1_hessian_x2x2,
            f2_hessian_x1x1,
            f2_hessian_x1x2,
            f2_hessian_x2x2,
        ]


    hessian_AsFunction_OT = ot.PythonFunction(2, 6, hessian_AsFunction_Python)
    print(hessian_AsFunction_OT([1.0, 2.0]))




.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [2,0,4,0,0,144]





.. _sphx_glr_download_auto_functional_modeling_vectorial_functions_plot_functions_grad_hess.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_functions_grad_hess.ipynb <plot_functions_grad_hess.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_functions_grad_hess.py <plot_functions_grad_hess.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_functions_grad_hess.zip <plot_functions_grad_hess.zip>`
