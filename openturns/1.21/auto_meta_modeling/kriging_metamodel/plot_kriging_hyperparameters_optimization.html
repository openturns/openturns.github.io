
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

    <title>Kriging :configure the optimization solver &#8212; OpenTURNS 1.21.3 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/openturns.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css" />
    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/sphinx_highlight.js"></script>
    <script src="../../_static/js/mysearchtools.js"></script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Kriging : choose a trend vector space" href="plot_kriging_chose_trend.html" />
    <link rel="prev" title="Advanced kriging" href="plot_kriging_advanced.html" />
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300,400,700'
          rel='stylesheet' type='text/css' />
 

  </head><body>
<div class="pageheader">
  <ul>
    <li><a href="http://www.openturns.org/">Home</a></li>
    <li><a href="../../install.html">Get it</a></li>
    <li><a href="../../contents.html">Doc</a></li>
    <li><a href="https://openturns.discourse.group/">Forum</a></li>
    <li><a href="https://gitter.im/openturns/community">Chat</a></li>
    <li><a href="https://github.com/openturns/openturns/wiki/Modules">Modules</a></li>
    <li><a href="https://github.com/openturns">Code</a></li>
    <li><a href="https://github.com/openturns/openturns/issues">Bugs</a></li>
  </ul>
  <a href="../../index.html">
    <h1>
      <img src="../../_static/logo-openturns-wo-bg.png" alt="" width=100px height=100px />
      OpenTURNS
    </h1>
    <h2> An Open source initiative for the Treatment of Uncertainties, Risks'N Statistics</h2>
  </a>
</div>

    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="plot_kriging_chose_trend.html" title="Kriging : choose a trend vector space"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="plot_kriging_advanced.html" title="Advanced kriging"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">OpenTURNS 1.21.3 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../contents.html" >Contents</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../../examples/examples.html" >Examples</a> &#187;</li>
          <li class="nav-item nav-item-3"><a href="../index.html" >Meta modeling</a> &#187;</li>
          <li class="nav-item nav-item-4"><a href="index.html" accesskey="U">Kriging metamodel</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Kriging :configure the optimization solver</a></li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="../../index.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">Kriging :configure the optimization solver</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a></li>
<li><a class="reference internal" href="#definition-of-the-model">Definition of the model</a></li>
<li><a class="reference internal" href="#create-the-design-of-experiments">Create the design of experiments</a></li>
<li><a class="reference internal" href="#create-the-metamodel">Create the metamodel</a></li>
<li><a class="reference internal" href="#get-the-optimizer-algorithm">Get the optimizer algorithm</a></li>
<li><a class="reference internal" href="#configure-the-starting-point-of-the-optimization">Configure the starting point of the optimization</a></li>
<li><a class="reference internal" href="#disabling-the-optimization">Disabling the optimization</a></li>
<li><a class="reference internal" href="#reuse-the-parameters-from-a-previous-optimization">Reuse the parameters from a previous optimization</a></li>
<li><a class="reference internal" href="#configure-the-local-optimization-solver">Configure the local optimization solver</a></li>
<li><a class="reference internal" href="#configure-the-global-optimization-solver">Configure the global optimization solver</a></li>
</ul>
</li>
</ul>

  </div>
  <div>
    <h4>Previous topic</h4>
    <p class="topless"><a href="plot_kriging_advanced.html"
                          title="previous chapter">Advanced kriging</a></p>
  </div>
  <div>
    <h4>Next topic</h4>
    <p class="topless"><a href="plot_kriging_chose_trend.html"
                          title="next chapter">Kriging : choose a trend vector space</a></p>
  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../../_sources/auto_meta_modeling/kriging_metamodel/plot_kriging_hyperparameters_optimization.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-auto-meta-modeling-kriging-metamodel-plot-kriging-hyperparameters-optimization-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code</p>
</div>
<section class="sphx-glr-example-title" id="kriging-configure-the-optimization-solver">
<span id="sphx-glr-auto-meta-modeling-kriging-metamodel-plot-kriging-hyperparameters-optimization-py"></span><h1>Kriging :configure the optimization solver<a class="headerlink" href="#kriging-configure-the-optimization-solver" title="Permalink to this heading">¶</a></h1>
<p>The goal of this example is to show how to fine-tune the optimization solver used to estimate the hyperparameters of the covariance model of the kriging metamodel.</p>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">¶</a></h2>
<p>In a kriging metamodel, there are various types of parameters which are estimated from the data.</p>
<ul class="simple">
<li><p>The parameters <img class="math" src="data:image/svg+xml;base64,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" alt="{\bf \beta}" style="vertical-align: -3px"/> associated with the deterministic trend. These parameters are computed based on linear least squares.</p></li>
<li><p>The parameters of the covariance model.</p></li>
</ul>
<p>The covariance model has two types of parameters.</p>
<ul class="simple">
<li><p>The amplitude parameter <img class="math" src="data:image/svg+xml;base64,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" alt="\sigma^2" style="vertical-align: 0px"/> is estimated from the data.
If the output dimension is equal to one, this parameter is estimated using
the analytic variance estimator which maximizes the likelihood.
Otherwise, if output dimension is greater than one or analytical sigma disabled,
this parameter is estimated from numerical optimization.</p></li>
<li><p>The other parameters <img class="math" src="data:image/svg+xml;base64,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" alt="{\bf \theta}\in\mathbb{R}^d" style="vertical-align: -1px"/> where <img class="math" src="data:image/svg+xml;base64,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" alt="d" style="vertical-align: 0px"/> is
the spatial dimension of the covariance model.
Often, the parameter <img class="math" src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0nMS4wJyBlbmNvZGluZz0nVVRGLTgnPz4KPCEtLSBUaGlzIGZpbGUgd2FzIGdlbmVyYXRlZCBieSBkdmlzdmdtIDMuMS4yIC0tPgo8c3ZnIHZlcnNpb249JzEuMScgeG1sbnM9J2h0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnJyB4bWxuczp4bGluaz0naHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluaycgd2lkdGg9JzUuNzgwMzQxcHQnIGhlaWdodD0nOC4zMDIxOTFwdCcgdmlld0JveD0nMCAtOC4zMDIxOTEgNS43ODAzNDEgOC4zMDIxOTEnPgo8ZGVmcz4KPHBhdGggaWQ9J2cwLTE4JyBkPSdNNS4yOTYxMzktNi4wMTM0NUM1LjI5NjEzOS03LjIzMjg3NyA0LjkxMzU3NC04LjQxNjQzOCAzLjkzMzI1LTguNDE2NDM4QzIuMjU5NTI3LTguNDE2NDM4IC40NzgyMDctNC45MTM1NzQgLjQ3ODIwNy0yLjI4MzQzN0MuNDc4MjA3LTEuNzMzNDk5IC41OTc3NTggLjExOTU1MiAxLjg1MzA1MSAuMTE5NTUyQzMuNDc4OTU0IC4xMTk1NTIgNS4yOTYxMzktMy4yOTk2MjYgNS4yOTYxMzktNi4wMTM0NVpNMS42NzM3MjQtNC4zMjc3NzFDMS44NTMwNTEtNS4wMzMxMjYgMi4xMDQxMS02LjAzNzM2IDIuNTgyMzE2LTYuODg2MTc3QzIuOTc2ODM3LTcuNjAzNDg3IDMuMzk1MjY4LTguMTc3MzM1IDMuOTIxMjk1LTguMTc3MzM1QzQuMzE1ODE2LTguMTc3MzM1IDQuNTc4ODI5LTcuODQyNTkgNC41Nzg4MjktNi42OTQ4OTRDNC41Nzg4MjktNi4yNjQ1MDggNC41NDI5NjQtNS42NjY3NSA0LjE5NjI2NC00LjMyNzc3MUgxLjY3MzcyNFpNNC4xMTI1NzgtMy45NjkxMTZDMy44MTM2OTktMi43OTc1MDkgMy41NjI2NC0yLjA0NDMzNCAzLjEzMjI1NC0xLjI5MTE1OEMyLjc4NTU1NC0uNjgxNDQ1IDIuMzY3MTIzLS4xMTk1NTIgMS44NjUwMDYtLjExOTU1MkMxLjQ5NDM5Ni0uMTE5NTUyIDEuMTk1NTE3LS40MDY0NzYgMS4xOTU1MTctMS41OTAwMzdDMS4xOTU1MTctMi4zNjcxMjMgMS4zODY4LTMuMTgwMDc1IDEuNTc4MDgyLTMuOTY5MTE2SDQuMTEyNTc4WicvPgo8L2RlZnM+CjxnIGlkPSdwYWdlMSc+Cjx1c2UgeD0nMCcgeT0nMCcgeGxpbms6aHJlZj0nI2cwLTE4Jy8+CjwvZz4KPC9zdmc+CjwhLS0gREVQVEg9MCAtLT4=" alt="{\bf \theta}" style="vertical-align: 0px"/> is a scale parameter.
This step involves an optimization algorithm.</p></li>
</ul>
<p>All these parameters are estimated with the <cite>GeneralLinearModelAlgorithm</cite> class.</p>
<p>The estimation of the <img class="math" src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0nMS4wJyBlbmNvZGluZz0nVVRGLTgnPz4KPCEtLSBUaGlzIGZpbGUgd2FzIGdlbmVyYXRlZCBieSBkdmlzdmdtIDMuMS4yIC0tPgo8c3ZnIHZlcnNpb249JzEuMScgeG1sbnM9J2h0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnJyB4bWxuczp4bGluaz0naHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluaycgd2lkdGg9JzUuNzgwMzQxcHQnIGhlaWdodD0nOC4zMDIxOTFwdCcgdmlld0JveD0nMCAtOC4zMDIxOTEgNS43ODAzNDEgOC4zMDIxOTEnPgo8ZGVmcz4KPHBhdGggaWQ9J2cwLTE4JyBkPSdNNS4yOTYxMzktNi4wMTM0NUM1LjI5NjEzOS03LjIzMjg3NyA0LjkxMzU3NC04LjQxNjQzOCAzLjkzMzI1LTguNDE2NDM4QzIuMjU5NTI3LTguNDE2NDM4IC40NzgyMDctNC45MTM1NzQgLjQ3ODIwNy0yLjI4MzQzN0MuNDc4MjA3LTEuNzMzNDk5IC41OTc3NTggLjExOTU1MiAxLjg1MzA1MSAuMTE5NTUyQzMuNDc4OTU0IC4xMTk1NTIgNS4yOTYxMzktMy4yOTk2MjYgNS4yOTYxMzktNi4wMTM0NVpNMS42NzM3MjQtNC4zMjc3NzFDMS44NTMwNTEtNS4wMzMxMjYgMi4xMDQxMS02LjAzNzM2IDIuNTgyMzE2LTYuODg2MTc3QzIuOTc2ODM3LTcuNjAzNDg3IDMuMzk1MjY4LTguMTc3MzM1IDMuOTIxMjk1LTguMTc3MzM1QzQuMzE1ODE2LTguMTc3MzM1IDQuNTc4ODI5LTcuODQyNTkgNC41Nzg4MjktNi42OTQ4OTRDNC41Nzg4MjktNi4yNjQ1MDggNC41NDI5NjQtNS42NjY3NSA0LjE5NjI2NC00LjMyNzc3MUgxLjY3MzcyNFpNNC4xMTI1NzgtMy45NjkxMTZDMy44MTM2OTktMi43OTc1MDkgMy41NjI2NC0yLjA0NDMzNCAzLjEzMjI1NC0xLjI5MTE1OEMyLjc4NTU1NC0uNjgxNDQ1IDIuMzY3MTIzLS4xMTk1NTIgMS44NjUwMDYtLjExOTU1MkMxLjQ5NDM5Ni0uMTE5NTUyIDEuMTk1NTE3LS40MDY0NzYgMS4xOTU1MTctMS41OTAwMzdDMS4xOTU1MTctMi4zNjcxMjMgMS4zODY4LTMuMTgwMDc1IDEuNTc4MDgyLTMuOTY5MTE2SDQuMTEyNTc4WicvPgo8L2RlZnM+CjxnIGlkPSdwYWdlMSc+Cjx1c2UgeD0nMCcgeT0nMCcgeGxpbms6aHJlZj0nI2cwLTE4Jy8+CjwvZz4KPC9zdmc+CjwhLS0gREVQVEg9MCAtLT4=" alt="{\bf \theta}" style="vertical-align: 0px"/> parameters is the step which has the highest CPU cost.
Moreover, the maximization of likelihood may be associated with difficulties e.g. many local maximums or even the non convergence of the optimization algorithm.
In this case, it might be useful to fine tune the optimization algorithm so that the convergence of the optimization algorithm is, hopefully, improved.</p>
<p>Furthermore, there are several situations in which the optimization can be initialized or completely bypassed.
Suppose for example that we have already created an initial kriging metamodel with <img class="math" src="data:image/svg+xml;base64,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" alt="N" style="vertical-align: 0px"/> points and we want to add a single new point.</p>
<ul class="simple">
<li><p>It might be interesting to initialize the optimization algorithm with the optimum found for the previous kriging metamodel:
this may reduce the number of iterations required to maximize the likelihood.</p></li>
<li><p>We may as well completely bypass the optimization step: if the previous covariance model was correctly estimated,
the update of the parameters may or may not significantly improve the estimates.</p></li>
</ul>
<p>This is why the goal of this example is to see how to configure the optimization of the hyperparameters of a kriging metamodel.</p>
</section>
<section id="definition-of-the-model">
<h2>Definition of the model<a class="headerlink" href="#definition-of-the-model" title="Permalink to this heading">¶</a></h2>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">openturns</span> <span class="k">as</span> <span class="nn">ot</span>

<span class="n">ot</span><span class="o">.</span><span class="n">Log</span><span class="o">.</span><span class="n">Show</span><span class="p">(</span><span class="n">ot</span><span class="o">.</span><span class="n">Log</span><span class="o">.</span><span class="n">NONE</span><span class="p">)</span>
</pre></div>
</div>
<p>We define the symbolic function which evaluates the output Y depending on the inputs E, F, L and I.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">SymbolicFunction</span><span class="p">([</span><span class="s2">&quot;E&quot;</span><span class="p">,</span> <span class="s2">&quot;F&quot;</span><span class="p">,</span> <span class="s2">&quot;L&quot;</span><span class="p">,</span> <span class="s2">&quot;I&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;F*L^3/(3*E*I)&quot;</span><span class="p">])</span>
</pre></div>
</div>
<p>Then we define the distribution of the input random vector.</p>
<p>Young’s modulus E</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">E</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">2.27</span><span class="p">,</span> <span class="mf">2.5e7</span><span class="p">,</span> <span class="mf">5.0e7</span><span class="p">)</span>  <span class="c1"># in N/m^2</span>
<span class="n">E</span><span class="o">.</span><span class="n">setDescription</span><span class="p">(</span><span class="s2">&quot;E&quot;</span><span class="p">)</span>
<span class="c1"># Load F</span>
<span class="n">F</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">LogNormal</span><span class="p">()</span>  <span class="c1"># in N</span>
<span class="n">F</span><span class="o">.</span><span class="n">setParameter</span><span class="p">(</span><span class="n">ot</span><span class="o">.</span><span class="n">LogNormalMuSigma</span><span class="p">()([</span><span class="mf">30.0e3</span><span class="p">,</span> <span class="mf">9e3</span><span class="p">,</span> <span class="mf">15.0e3</span><span class="p">]))</span>
<span class="n">F</span><span class="o">.</span><span class="n">setDescription</span><span class="p">(</span><span class="s2">&quot;F&quot;</span><span class="p">)</span>
<span class="c1"># Length L</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="mf">250.0</span><span class="p">,</span> <span class="mf">260.0</span><span class="p">)</span>  <span class="c1"># in cm</span>
<span class="n">L</span><span class="o">.</span><span class="n">setDescription</span><span class="p">(</span><span class="s2">&quot;L&quot;</span><span class="p">)</span>
<span class="c1"># Moment of inertia I</span>
<span class="n">II</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mi">310</span><span class="p">,</span> <span class="mi">450</span><span class="p">)</span>  <span class="c1"># in cm^4</span>
<span class="n">II</span><span class="o">.</span><span class="n">setDescription</span><span class="p">(</span><span class="s2">&quot;I&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally, we define the dependency using a <cite>NormalCopula</cite>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">dim</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># number of inputs</span>
<span class="n">R</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">CorrelationMatrix</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
<span class="n">R</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.2</span>
<span class="n">myCopula</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">NormalCopula</span><span class="p">(</span><span class="n">ot</span><span class="o">.</span><span class="n">NormalCopula</span><span class="o">.</span><span class="n">GetCorrelationFromSpearmanCorrelation</span><span class="p">(</span><span class="n">R</span><span class="p">))</span>
<span class="n">myDistribution</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">ComposedDistribution</span><span class="p">([</span><span class="n">E</span><span class="p">,</span> <span class="n">F</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">II</span><span class="p">],</span> <span class="n">myCopula</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="create-the-design-of-experiments">
<h2>Create the design of experiments<a class="headerlink" href="#create-the-design-of-experiments" title="Permalink to this heading">¶</a></h2>
<p>We consider a simple Monte-Carlo sampling as a design of experiments. This is why we generate an input sample using the <cite>getSample</cite> method of the distribution.
Then we evaluate the output using the <cite>model</cite> function.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">sampleSize_train</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">myDistribution</span><span class="o">.</span><span class="n">getSample</span><span class="p">(</span><span class="n">sampleSize_train</span><span class="p">)</span>
<span class="n">Y_train</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="create-the-metamodel">
<h2>Create the metamodel<a class="headerlink" href="#create-the-metamodel" title="Permalink to this heading">¶</a></h2>
<p>In order to create the kriging metamodel, we first select a constant trend with the <cite>ConstantBasisFactory</cite> class.
Then we use a squared exponential covariance model.
Finally, we use the <cite>KrigingAlgorithm</cite> class to create the kriging metamodel,
taking the training sample, the covariance model and the trend basis as input arguments.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">dimension</span> <span class="o">=</span> <span class="n">myDistribution</span><span class="o">.</span><span class="n">getDimension</span><span class="p">()</span>
<span class="n">basis</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">ConstantBasisFactory</span><span class="p">(</span><span class="n">dimension</span><span class="p">)</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>

<span class="c1"># Trick B, v2</span>
<span class="n">x_range</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">getMax</span><span class="p">()</span> <span class="o">-</span> <span class="n">X_train</span><span class="o">.</span><span class="n">getMin</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;x_range:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">x_range</span><span class="p">)</span>
<span class="n">scale_max_factor</span> <span class="o">=</span> <span class="mf">4.0</span>  <span class="c1"># Must be &gt; 1, tune this to match your problem</span>
<span class="n">scale_min_factor</span> <span class="o">=</span> <span class="mf">0.1</span>  <span class="c1"># Must be &lt; 1, tune this to match your problem</span>
<span class="n">maximum_scale_bounds</span> <span class="o">=</span> <span class="n">scale_max_factor</span> <span class="o">*</span> <span class="n">x_range</span>
<span class="n">minimum_scale_bounds</span> <span class="o">=</span> <span class="n">scale_min_factor</span> <span class="o">*</span> <span class="n">x_range</span>
<span class="n">scaleOptimizationBounds</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">Interval</span><span class="p">(</span><span class="n">minimum_scale_bounds</span><span class="p">,</span> <span class="n">maximum_scale_bounds</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;scaleOptimizationBounds&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">scaleOptimizationBounds</span><span class="p">)</span>

<span class="n">covarianceModel</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">SquaredExponential</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]</span> <span class="o">*</span> <span class="n">dimension</span><span class="p">,</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">])</span>
<span class="n">covarianceModel</span><span class="o">.</span><span class="n">setScale</span><span class="p">(</span><span class="n">maximum_scale_bounds</span><span class="p">)</span>  <span class="c1"># Trick A</span>
<span class="n">algo</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">KrigingAlgorithm</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">covarianceModel</span><span class="p">,</span> <span class="n">basis</span><span class="p">)</span>
<span class="n">algo</span><span class="o">.</span><span class="n">setOptimizationBounds</span><span class="p">(</span><span class="n">scaleOptimizationBounds</span><span class="p">)</span>
<span class="n">algo</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">getResult</span><span class="p">()</span>
<span class="n">krigingMetamodel</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">getMetaModel</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>x_range:
[2.12636e+07,24296.8,7.35174,106.039]
scaleOptimizationBounds
[2.12636e+06, 8.50545e+07]
[2429.68, 97187.2]
[0.735174, 29.407]
[10.6039, 424.154]
</pre></div>
</div>
<p>The <cite>run</cite> method has optimized the hyperparameters of the metamodel.</p>
<p>We can then print the constant trend of the metamodel, which have been estimated using the least squares method.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">result</span><span class="o">.</span><span class="n">getTrendCoefficients</span><span class="p">()</span>
</pre></div>
</div>
<div class="output_subarea output_html rendered_html output_result">
<p>[18.0423]</p>
</div>
<br />
<br /><p>We can also print the hyperparameters of the covariance model, which have been estimated by maximizing the likelihood.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">basic_covariance_model</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">getCovarianceModel</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">basic_covariance_model</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>SquaredExponential(scale=[8.50532e+07,22986,29.407,366.658], amplitude=[14.488])
</pre></div>
</div>
</section>
<section id="get-the-optimizer-algorithm">
<h2>Get the optimizer algorithm<a class="headerlink" href="#get-the-optimizer-algorithm" title="Permalink to this heading">¶</a></h2>
<p>The <cite>getOptimizationAlgorithm</cite> method returns the optimization algorithm used to optimize the <img class="math" src="data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0nMS4wJyBlbmNvZGluZz0nVVRGLTgnPz4KPCEtLSBUaGlzIGZpbGUgd2FzIGdlbmVyYXRlZCBieSBkdmlzdmdtIDMuMS4yIC0tPgo8c3ZnIHZlcnNpb249JzEuMScgeG1sbnM9J2h0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnJyB4bWxuczp4bGluaz0naHR0cDovL3d3dy53My5vcmcvMTk5OS94bGluaycgd2lkdGg9JzUuNzgwMzQxcHQnIGhlaWdodD0nOC4zMDIxOTFwdCcgdmlld0JveD0nMCAtOC4zMDIxOTEgNS43ODAzNDEgOC4zMDIxOTEnPgo8ZGVmcz4KPHBhdGggaWQ9J2cwLTE4JyBkPSdNNS4yOTYxMzktNi4wMTM0NUM1LjI5NjEzOS03LjIzMjg3NyA0LjkxMzU3NC04LjQxNjQzOCAzLjkzMzI1LTguNDE2NDM4QzIuMjU5NTI3LTguNDE2NDM4IC40NzgyMDctNC45MTM1NzQgLjQ3ODIwNy0yLjI4MzQzN0MuNDc4MjA3LTEuNzMzNDk5IC41OTc3NTggLjExOTU1MiAxLjg1MzA1MSAuMTE5NTUyQzMuNDc4OTU0IC4xMTk1NTIgNS4yOTYxMzktMy4yOTk2MjYgNS4yOTYxMzktNi4wMTM0NVpNMS42NzM3MjQtNC4zMjc3NzFDMS44NTMwNTEtNS4wMzMxMjYgMi4xMDQxMS02LjAzNzM2IDIuNTgyMzE2LTYuODg2MTc3QzIuOTc2ODM3LTcuNjAzNDg3IDMuMzk1MjY4LTguMTc3MzM1IDMuOTIxMjk1LTguMTc3MzM1QzQuMzE1ODE2LTguMTc3MzM1IDQuNTc4ODI5LTcuODQyNTkgNC41Nzg4MjktNi42OTQ4OTRDNC41Nzg4MjktNi4yNjQ1MDggNC41NDI5NjQtNS42NjY3NSA0LjE5NjI2NC00LjMyNzc3MUgxLjY3MzcyNFpNNC4xMTI1NzgtMy45NjkxMTZDMy44MTM2OTktMi43OTc1MDkgMy41NjI2NC0yLjA0NDMzNCAzLjEzMjI1NC0xLjI5MTE1OEMyLjc4NTU1NC0uNjgxNDQ1IDIuMzY3MTIzLS4xMTk1NTIgMS44NjUwMDYtLjExOTU1MkMxLjQ5NDM5Ni0uMTE5NTUyIDEuMTk1NTE3LS40MDY0NzYgMS4xOTU1MTctMS41OTAwMzdDMS4xOTU1MTctMi4zNjcxMjMgMS4zODY4LTMuMTgwMDc1IDEuNTc4MDgyLTMuOTY5MTE2SDQuMTEyNTc4WicvPgo8L2RlZnM+CjxnIGlkPSdwYWdlMSc+Cjx1c2UgeD0nMCcgeT0nMCcgeGxpbms6aHJlZj0nI2cwLTE4Jy8+CjwvZz4KPC9zdmc+CjwhLS0gREVQVEg9MCAtLT4=" alt="{\bf \theta}" style="vertical-align: 0px"/> parameters of the <cite>SquaredExponential</cite> covariance model.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">solver</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">getOptimizationAlgorithm</span><span class="p">()</span>
</pre></div>
</div>
<p>Get the default optimizer.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">solverImplementation</span> <span class="o">=</span> <span class="n">solver</span><span class="o">.</span><span class="n">getImplementation</span><span class="p">()</span>
<span class="n">solverImplementation</span><span class="o">.</span><span class="n">getClassName</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>&#39;TNC&#39;
</pre></div>
</div>
<p>The <cite>getOptimizationBounds</cite> method returns the bounds. The dimension of these bounds correspond to the spatial dimension of the covariance model.
In the metamodeling context, this correspond to the input dimension of the model.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">bounds</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">getOptimizationBounds</span><span class="p">()</span>
<span class="n">bounds</span><span class="o">.</span><span class="n">getDimension</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>4
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">lbounds</span> <span class="o">=</span> <span class="n">bounds</span><span class="o">.</span><span class="n">getLowerBound</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;lbounds&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lbounds</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>lbounds
[2.12636e+06,2429.68,0.735174,10.6039]
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">ubounds</span> <span class="o">=</span> <span class="n">bounds</span><span class="o">.</span><span class="n">getUpperBound</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ubounds&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">ubounds</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>ubounds
[8.50545e+07,97187.2,29.407,424.154]
</pre></div>
</div>
<p>The <cite>getOptimizeParameters</cite> method returns <cite>True</cite> if these parameters are to be optimized.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">isOptimize</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">getOptimizeParameters</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">isOptimize</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</section>
<section id="configure-the-starting-point-of-the-optimization">
<h2>Configure the starting point of the optimization<a class="headerlink" href="#configure-the-starting-point-of-the-optimization" title="Permalink to this heading">¶</a></h2>
<p>The starting point of the optimization is based on the parameters of the covariance model.
In the following example, we configure the parameters of the covariance model to the arbitrary values <cite>[12.,34.,56.,78.]</cite>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">covarianceModel</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">SquaredExponential</span><span class="p">([</span><span class="mf">12.0</span><span class="p">,</span> <span class="mf">34.0</span><span class="p">,</span> <span class="mf">56.0</span><span class="p">,</span> <span class="mf">78.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">])</span>
<span class="n">covarianceModel</span><span class="o">.</span><span class="n">setScale</span><span class="p">(</span><span class="n">maximum_scale_bounds</span><span class="p">)</span>  <span class="c1"># Trick A</span>
<span class="n">algo</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">KrigingAlgorithm</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">covarianceModel</span><span class="p">,</span> <span class="n">basis</span><span class="p">)</span>
<span class="n">algo</span><span class="o">.</span><span class="n">setOptimizationBounds</span><span class="p">(</span><span class="n">scaleOptimizationBounds</span><span class="p">)</span>  <span class="c1"># Trick B</span>
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">algo</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">result</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">getResult</span><span class="p">()</span>
<span class="n">new_covariance_model</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">getCovarianceModel</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">new_covariance_model</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>SquaredExponential(scale=[8.50532e+07,22986,29.407,366.658], amplitude=[14.488])
</pre></div>
</div>
<p>In order to see the difference with the default optimization, we print the previous optimized covariance model.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">basic_covariance_model</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>SquaredExponential(scale=[8.50532e+07,22986,29.407,366.658], amplitude=[14.488])
</pre></div>
</div>
<p>We observe that this does not change much the values of the parameters in this case.</p>
</section>
<section id="disabling-the-optimization">
<h2>Disabling the optimization<a class="headerlink" href="#disabling-the-optimization" title="Permalink to this heading">¶</a></h2>
<p>It is sometimes useful to completely disable the optimization of the parameters.
In order to see the effect of this, we first initialize the parameters of the covariance model with the arbitrary values <cite>[12.,34.,56.,78.]</cite>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">covarianceModel</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">SquaredExponential</span><span class="p">([</span><span class="mf">12.0</span><span class="p">,</span> <span class="mf">34.0</span><span class="p">,</span> <span class="mf">56.0</span><span class="p">,</span> <span class="mf">78.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">91.0</span><span class="p">])</span>
<span class="n">algo</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">KrigingAlgorithm</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">covarianceModel</span><span class="p">,</span> <span class="n">basis</span><span class="p">)</span>
<span class="n">algo</span><span class="o">.</span><span class="n">setOptimizationBounds</span><span class="p">(</span><span class="n">scaleOptimizationBounds</span><span class="p">)</span>  <span class="c1"># Trick B</span>
</pre></div>
</div>
<p>The <cite>setOptimizeParameters</cite> method can be used to disable the optimization of the parameters.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">algo</span><span class="o">.</span><span class="n">setOptimizeParameters</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>Then we run the algorithm and get the result.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">algo</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">getResult</span><span class="p">()</span>
</pre></div>
</div>
<p>We observe that the covariance model is unchanged:
the parameters have not been optimized, as required.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">updated_covariance_model</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">getCovarianceModel</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">updated_covariance_model</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>SquaredExponential(scale=[12,34,56,78], amplitude=[91])
</pre></div>
</div>
<p>The trend, however, is still optimized, using linear least squares.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">result</span><span class="o">.</span><span class="n">getTrendCoefficients</span><span class="p">()</span>
</pre></div>
</div>
<div class="output_subarea output_html rendered_html output_result">
<p>[12.0499]</p>
</div>
<br />
<br /></section>
<section id="reuse-the-parameters-from-a-previous-optimization">
<h2>Reuse the parameters from a previous optimization<a class="headerlink" href="#reuse-the-parameters-from-a-previous-optimization" title="Permalink to this heading">¶</a></h2>
<p>In this example, we show how to reuse the optimized parameters of a previous kriging and configure a new one.
Furthermore, we disable the optimization so that the parameters of the covariance model are not updated.
This make the process of adding a new point very fast:
it improves the quality by adding a new point in the design of experiments without paying the price of the update of the covariance model.</p>
<p>Step 1: Run a first kriging algorithm.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">dimension</span> <span class="o">=</span> <span class="n">myDistribution</span><span class="o">.</span><span class="n">getDimension</span><span class="p">()</span>
<span class="n">basis</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">ConstantBasisFactory</span><span class="p">(</span><span class="n">dimension</span><span class="p">)</span><span class="o">.</span><span class="n">build</span><span class="p">()</span>
<span class="n">covarianceModel</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">SquaredExponential</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]</span> <span class="o">*</span> <span class="n">dimension</span><span class="p">,</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">])</span>
<span class="n">covarianceModel</span><span class="o">.</span><span class="n">setScale</span><span class="p">(</span><span class="n">maximum_scale_bounds</span><span class="p">)</span>  <span class="c1"># Trick A</span>
<span class="n">algo</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">KrigingAlgorithm</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">covarianceModel</span><span class="p">,</span> <span class="n">basis</span><span class="p">)</span>
<span class="n">algo</span><span class="o">.</span><span class="n">setOptimizationBounds</span><span class="p">(</span><span class="n">scaleOptimizationBounds</span><span class="p">)</span>  <span class="c1"># Trick B</span>
<span class="n">algo</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">getResult</span><span class="p">()</span>
<span class="n">covarianceModel</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">getCovarianceModel</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">covarianceModel</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>SquaredExponential(scale=[8.50532e+07,22986,29.407,366.658], amplitude=[14.488])
</pre></div>
</div>
<p>Step 2: Create a new point and add it to the previous training design.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">X_new</span> <span class="o">=</span> <span class="n">myDistribution</span><span class="o">.</span><span class="n">getSample</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span>
<span class="n">Y_new</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>
<span class="n">X_train</span><span class="o">.</span><span class="n">getSize</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>30
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">Y_train</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Y_new</span><span class="p">)</span>
<span class="n">Y_train</span><span class="o">.</span><span class="n">getSize</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>30
</pre></div>
</div>
<p>Step 3: Create an updated kriging, using the new point with the old covariance parameters.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">algo</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">KrigingAlgorithm</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">covarianceModel</span><span class="p">,</span> <span class="n">basis</span><span class="p">)</span>
<span class="n">algo</span><span class="o">.</span><span class="n">setOptimizeParameters</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="n">algo</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">algo</span><span class="o">.</span><span class="n">getResult</span><span class="p">()</span>
<span class="n">notUpdatedCovarianceModel</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">getCovarianceModel</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">notUpdatedCovarianceModel</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>SquaredExponential(scale=[8.50532e+07,22986,29.407,366.658], amplitude=[14.488])
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">printCovarianceParameterChange</span><span class="p">(</span><span class="n">covarianceModel1</span><span class="p">,</span> <span class="n">covarianceModel2</span><span class="p">):</span>
    <span class="n">parameters1</span> <span class="o">=</span> <span class="n">covarianceModel1</span><span class="o">.</span><span class="n">getFullParameter</span><span class="p">()</span>
    <span class="n">parameters2</span> <span class="o">=</span> <span class="n">covarianceModel2</span><span class="o">.</span><span class="n">getFullParameter</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">parameters1</span><span class="o">.</span><span class="n">getDimension</span><span class="p">()):</span>
        <span class="n">deltai</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">parameters1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">parameters2</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Change in the parameter #</span><span class="si">%d</span><span class="s2"> = </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">deltai</span><span class="p">))</span>
    <span class="k">return</span>
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">printCovarianceParameterChange</span><span class="p">(</span><span class="n">covarianceModel</span><span class="p">,</span> <span class="n">notUpdatedCovarianceModel</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Change in the parameter #0 = 0.0
Change in the parameter #1 = 0.0
Change in the parameter #2 = 0.0
Change in the parameter #3 = 0.0
Change in the parameter #4 = 0.0
</pre></div>
</div>
<p>We see that the parameters did not change <em>at all</em>: disabling the optimization allows one to keep a constant covariance model.
In a practical algorithm, we may, for example, add a block of 10 new points before updating the parameters of the covariance model.
At this point, we may reuse the previous covariance model so that the optimization starts from a better point, compared to the parameters default values.
This will reduce the cost of the optimization.</p>
</section>
<section id="configure-the-local-optimization-solver">
<h2>Configure the local optimization solver<a class="headerlink" href="#configure-the-local-optimization-solver" title="Permalink to this heading">¶</a></h2>
<p>The following example shows how to set the local optimization solver.
We choose the SLSQP algorithm from NLOPT.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">problem</span> <span class="o">=</span> <span class="n">solver</span><span class="o">.</span><span class="n">getProblem</span><span class="p">()</span>
<span class="n">local_solver</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">NLopt</span><span class="p">(</span><span class="n">problem</span><span class="p">,</span> <span class="s2">&quot;LD_SLSQP&quot;</span><span class="p">)</span>
<span class="n">covarianceModel</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">SquaredExponential</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]</span> <span class="o">*</span> <span class="n">dimension</span><span class="p">,</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">])</span>
<span class="n">covarianceModel</span><span class="o">.</span><span class="n">setScale</span><span class="p">(</span><span class="n">maximum_scale_bounds</span><span class="p">)</span>  <span class="c1"># Trick A</span>
<span class="n">algo</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">KrigingAlgorithm</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">covarianceModel</span><span class="p">,</span> <span class="n">basis</span><span class="p">)</span>
<span class="n">algo</span><span class="o">.</span><span class="n">setOptimizationBounds</span><span class="p">(</span><span class="n">scaleOptimizationBounds</span><span class="p">)</span>  <span class="c1"># Trick B</span>
<span class="n">algo</span><span class="o">.</span><span class="n">setOptimizationAlgorithm</span><span class="p">(</span><span class="n">local_solver</span><span class="p">)</span>
<span class="n">algo</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">finetune_covariance_model</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">getCovarianceModel</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">finetune_covariance_model</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>SquaredExponential(scale=[8.50532e+07,22986,29.407,366.658], amplitude=[14.488])
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">printCovarianceParameterChange</span><span class="p">(</span><span class="n">finetune_covariance_model</span><span class="p">,</span> <span class="n">basic_covariance_model</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Change in the parameter #0 = 0.0
Change in the parameter #1 = 0.0
Change in the parameter #2 = 0.0
Change in the parameter #3 = 0.0
Change in the parameter #4 = 0.0
</pre></div>
</div>
</section>
<section id="configure-the-global-optimization-solver">
<h2>Configure the global optimization solver<a class="headerlink" href="#configure-the-global-optimization-solver" title="Permalink to this heading">¶</a></h2>
<p>The following example checks the robustness of the optimization of the kriging algorithm with respect to the optimization of the likelihood function in the covariance model parameters estimation.
We use a <cite>MultiStart</cite> algorithm in order to avoid to be trapped by a local minimum.
Furthermore, we generate the design of experiments using a <cite>LHSExperiments</cite>, which guarantees that the points will fill the space.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">sampleSize_train</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">myDistribution</span><span class="o">.</span><span class="n">getSample</span><span class="p">(</span><span class="n">sampleSize_train</span><span class="p">)</span>
<span class="n">Y_train</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</pre></div>
</div>
<p>First, we create a multivariate distribution, based on independent <cite>Uniform</cite> marginals which have the bounds required by the covariance model.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">distributions</span> <span class="o">=</span> <span class="p">[</span><span class="n">ot</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="n">lbounds</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ubounds</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">dim</span><span class="p">)]</span>
<span class="n">boundedDistribution</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">ComposedDistribution</span><span class="p">(</span><span class="n">distributions</span><span class="p">)</span>
</pre></div>
</div>
<p>We first generate a Latin Hypercube Sampling (LHS) design made of 25 points in the sample space. This LHS is optimized so as to fill the space.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">K</span> <span class="o">=</span> <span class="mi">25</span>  <span class="c1"># design size</span>
<span class="n">LHS</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">LHSExperiment</span><span class="p">(</span><span class="n">boundedDistribution</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
<span class="n">LHS</span><span class="o">.</span><span class="n">setAlwaysShuffle</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">SA_profile</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">GeometricProfile</span><span class="p">(</span><span class="mf">10.0</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">,</span> <span class="mi">20000</span><span class="p">)</span>
<span class="n">LHS_optimization_algo</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">SimulatedAnnealingLHS</span><span class="p">(</span><span class="n">LHS</span><span class="p">,</span> <span class="n">ot</span><span class="o">.</span><span class="n">SpaceFillingC2</span><span class="p">(),</span> <span class="n">SA_profile</span><span class="p">)</span>
<span class="n">LHS_optimization_algo</span><span class="o">.</span><span class="n">generate</span><span class="p">()</span>
<span class="n">LHS_design</span> <span class="o">=</span> <span class="n">LHS_optimization_algo</span><span class="o">.</span><span class="n">getResult</span><span class="p">()</span>
<span class="n">starting_points</span> <span class="o">=</span> <span class="n">LHS_design</span><span class="o">.</span><span class="n">getOptimalDesign</span><span class="p">()</span>
<span class="n">starting_points</span><span class="o">.</span><span class="n">getSize</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>25
</pre></div>
</div>
<p>We can check that the minimum and maximum in the sample correspond to the bounds of the design of experiment.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">lbounds</span><span class="p">,</span> <span class="n">ubounds</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[2.12636e+06,2429.68,0.735174,10.6039] [8.50545e+07,97187.2,29.407,424.154]
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">starting_points</span><span class="o">.</span><span class="n">getMin</span><span class="p">(),</span> <span class="n">starting_points</span><span class="o">.</span><span class="n">getMax</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>(class=Point name=Unnamed dimension=4 values=[3.58268e+06,4126.37,0.875832,25.501], class=Point name=Unnamed dimension=4 values=[8.21336e+07,95739.1,28.6265,414.801])
</pre></div>
</div>
<p>Then we create a <cite>MultiStart</cite> algorithm based on the LHS starting points.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">solver</span><span class="o">.</span><span class="n">setMaximumIterationNumber</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">multiStartSolver</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">MultiStart</span><span class="p">(</span><span class="n">solver</span><span class="p">,</span> <span class="n">starting_points</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally, we configure the optimization algorithm so as to use the <cite>MultiStart</cite> algorithm.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">algo</span> <span class="o">=</span> <span class="n">ot</span><span class="o">.</span><span class="n">KrigingAlgorithm</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">covarianceModel</span><span class="p">,</span> <span class="n">basis</span><span class="p">)</span>
<span class="n">algo</span><span class="o">.</span><span class="n">setOptimizationBounds</span><span class="p">(</span><span class="n">scaleOptimizationBounds</span><span class="p">)</span>  <span class="c1"># Trick B</span>
<span class="n">algo</span><span class="o">.</span><span class="n">setOptimizationAlgorithm</span><span class="p">(</span><span class="n">multiStartSolver</span><span class="p">)</span>
<span class="n">algo</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">finetune_covariance_model</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">getCovarianceModel</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">finetune_covariance_model</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>SquaredExponential(scale=[8.50532e+07,22986,29.407,366.658], amplitude=[14.488])
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">printCovarianceParameterChange</span><span class="p">(</span><span class="n">finetune_covariance_model</span><span class="p">,</span> <span class="n">basic_covariance_model</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Change in the parameter #0 = 0.0
Change in the parameter #1 = 0.0
Change in the parameter #2 = 0.0
Change in the parameter #3 = 0.0
Change in the parameter #4 = 0.0
</pre></div>
</div>
<p>We see that there are no changes in the estimated parameters. This shows that the first optimization of the parameters worked fine.</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-meta-modeling-kriging-metamodel-plot-kriging-hyperparameters-optimization-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/bf1a4f3bee47286c7b782ba172f895e3/plot_kriging_hyperparameters_optimization.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">plot_kriging_hyperparameters_optimization.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/06b69b36ded61f91f0e533e211435c93/plot_kriging_hyperparameters_optimization.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">plot_kriging_hyperparameters_optimization.py</span></code></a></p>
</div>
</div>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="plot_kriging_chose_trend.html" title="Kriging : choose a trend vector space"
             >next</a> |</li>
        <li class="right" >
          <a href="plot_kriging_advanced.html" title="Advanced kriging"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">OpenTURNS 1.21.3 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../../contents.html" >Contents</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="../../examples/examples.html" >Examples</a> &#187;</li>
          <li class="nav-item nav-item-3"><a href="../index.html" >Meta modeling</a> &#187;</li>
          <li class="nav-item nav-item-4"><a href="index.html" >Kriging metamodel</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Kriging :configure the optimization solver</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2005-2023 Airbus-EDF-IMACS-ONERA-Phimeca.
      Last updated on Dec 04, 2023.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 5.3.0.
    </div>
  </body>
</html>