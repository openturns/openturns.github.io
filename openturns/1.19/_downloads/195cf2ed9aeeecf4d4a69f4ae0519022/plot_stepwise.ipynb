{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Perfom stepwise regression\nIn this example we perform the selection of the most suitable function basis for a linear regression model with the help of the stepwise algorithm.\n\nWe consider te so-called Linthurst data set, which contains measures of aerial biomass (BIO) as well as 5 five physicochemical properties of the soil: salinity (SAL), pH, K, Na, and Zn.\n\nThe data set is taken from the book [rawlings2001]_\nand is provided below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import openturns as ot\nfrom openturns.viewer import View\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndescription = [\"BIO\", \"SAL\", \"pH\", \"K\", \"Na\", \"Zn\"]\ndata = [[676, 33, 5, 1441.67, 35185.5, 16.4524],\n        [516, 35, 4.75, 1299.19, 28170.4, 13.9852],\n        [1052, 32, 4.2, 1154.27, 26455, 15.3276],\n        [868, 30, 4.4, 1045.15, 25072.9, 17.3128],\n        [1008, 33, 5.55, 521.62, 31664.2, 22.3312],\n        [436, 33, 5.05, 1273.02, 25491.7, 12.2778],\n        [544, 36, 4.25, 1346.35, 20877.3, 17.8225],\n        [680, 30, 4.45, 1253.88, 25621.3, 14.3516],\n        [640, 38, 4.75, 1242.65, 27587.3, 13.6826],\n        [492, 30, 4.6, 1281.95, 26511.7, 11.7566],\n        [984, 30, 4.1, 553.69, 7886.5, 9.882],\n        [1400, 37, 3.45, 494.74, 14596, 16.6752],\n        [1276, 33, 3.45, 525.97, 9826.8, 12.373],\n        [1736, 36, 4.1, 571.14, 11978.4, 9.4058],\n        [1004, 30, 3.5, 408.64, 10368.6, 14.9302],\n        [396, 30, 3.25, 646.65, 17307.4, 31.2865],\n        [352, 27, 3.35, 514.03, 12822, 30.1652],\n        [328, 29, 3.2, 350.73, 8582.6, 28.5901],\n        [392, 34, 3.35, 496.29, 12369.5, 19.8795],\n        [236, 36, 3.3, 580.92, 14731.9, 18.5056],\n        [392, 30, 3.25, 535.82, 15060.6, 22.1344],\n        [268, 28, 3.25, 490.34, 11056.3, 28.6101],\n        [252, 31, 3.2, 552.39, 8118.9, 23.1908],\n        [236, 31, 3.2, 661.32, 13009.5, 24.6917],\n        [340, 35, 3.35, 672.15, 15003.7, 22.6758],\n        [2436, 29, 7.1, 528.65, 10225, 0.3729],\n        [2216, 35, 7.35, 563.13, 8024.2, 0.2703],\n        [2096, 35, 7.45, 497.96, 10393, 0.3205],\n        [1660, 30, 7.45, 458.38, 8711.6, 0.2648],\n        [2272, 30, 7.4, 498.25, 10239.6, 0.2105],\n        [824, 26, 4.85, 936.26, 20436, 18.9875],\n        [1196, 29, 4.6, 894.79, 12519.9, 20.9687],\n        [1960, 25, 5.2, 941.36, 18979, 23.9841],\n        [2080, 26, 4.75, 1038.79, 22986.1, 19.9727],\n        [1764, 26, 5.2, 898.05, 11704.5, 21.3864],\n        [412, 25, 4.55, 989.87, 17721, 23.7063],\n        [416, 26, 3.95, 951.28, 16485.2, 30.5589],\n        [504, 26, 3.7, 939.83, 17101.3, 26.8415],\n        [492, 27, 3.75, 925.42, 17849, 27.7292],\n        [636, 27, 4.15, 954.11, 16949.6, 21.5699],\n        [1756, 24, 5.6, 720.72, 11344.6, 19.6531],\n        [1232, 27, 5.35, 782.09, 14752.4, 20.3295],\n        [1400, 26, 5.5, 773.3, 13649.8, 19.588],\n        [1620, 28, 5.5, 829.26, 14533, 20.1328],\n        [1560, 28, 5.4, 856.96, 16892.2, 19.242]]\ninput_description = [\"SAL\", \"pH\", \"K\", \"Na\", \"Zn\"]\noutput_description = [\"BIO\"]\nsample = ot.Sample(np.array(data))\ndimension = sample.getDimension()-1\nn = sample.getSize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Complete linear model\n\nWe consider a linear model with the purpose of predicting the aerial biomass as a function of the soil physicochemical properties,\nand we wish to identify the predictive variables which result in the most simple and precise linear regression model.\n\nWe start by creating a linear model which takes into account all of the physicochemical variables present within the Linthrust data set.\n\nLet us consider the following linear model $\\tilde{Y} = a_0 + \\sum_{i = 1}^{d} a_i X_i + \\epsilon$. If all of the predictive variables\nare considered, the regression can be performed with the help of the `LinearModelAlgorithm` class.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_sample = sample[:, 1:dimension+1]\noutput_sample = sample[:, 0]\nalgo_full = ot.LinearModelAlgorithm(input_sample, output_sample)\nalgo_full.run()\nresult_full = ot.LinearModelResult(algo_full.getResult())\nprint('R-squared = ', result_full.getRSquared())\nprint('Adjusted R-squared = ', result_full.getAdjustedRSquared())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Forward stepwise regression\n\nWe now wish to perform the selection of the most important predictive variables through a stepwise algorithm.\n\nIt is first necessary to define a suitable function basis for the regression. Each variable is associated to a univariate basis\nand an additional basis is used in order to represent the constant term $a_0$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "functions = []\nfunctions.append(ot.SymbolicFunction(input_description, ['1.0']))\nfor i in range(dimension):\n    functions.append(ot.SymbolicFunction(\n        input_description, [input_description[i]]))\nbasis = ot.Basis(functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plese note that this example uses a linear basis with respect to the various predictors for the sake of clarity.\nHowever, this is not a necessity, and more complex and non linear relations between predictors may be considered\n(e.g., polynomial bases).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now perform a forward stepwise regression. We suppose having no information regarding the given data set, and therefore the set of minimal\nindices only contains the constant term (indexed by 0).\n\nThe first regression is performed by relying on the Akaike Information Criterion (AIC), which translates into a penalty term equal to 2.\nIn practice, the algorithm selects the functional basis subset that minimizes the AIC by iteratively adding the single function which provides\nthe largest improvement until convergence is reached.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "minimalIndices = [0]\ndirection = ot.LinearModelStepwiseAlgorithm.FORWARD\npenalty = 2.0\nalgo_forward = ot.LinearModelStepwiseAlgorithm(\n    input_sample, basis, output_sample, minimalIndices, direction)\nalgo_forward.setPenalty(penalty)\nalgo_forward.run()\nresult_forward = algo_forward.getResult()\nprint('Selected basis: ', result_forward.getCoefficientsNames())\nprint('R-squared = ', result_forward.getRSquared())\nprint('Adjusted R-squared = ', result_forward.getAdjustedRSquared())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With this first forward stepwise regression, the results show that the selected optimal basis contains a constant term,\nplus two linear terms depending respectively on the pH value (pH) and on the sodium concentration (Na).\n\nAs can be expected, the R-squared value diminishes if compared to the regression on the entire basis, as the stepwise\nregression results in a lower number of predictive variables.  However, it can also be seen that the adjusted R-squared,\nwhich is a metric that also takes into account the ratio  between the amount of training data and the number of explanatory\nvariables, is improved if compared to the complete model.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Backward stepwise regression\n\nWe now perform a backward stepwise regression, meaning that rather than iteratively adding predictive variables, we will be removing them,\nstarting from the complete model.\nThis regression is performed by relying on the Bayesian Information Criterion (BIC), which translates into a penalty term equal to $log(n)$.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "minimalIndices = [0]\ndirection = ot.LinearModelStepwiseAlgorithm.BACKWARD\npenalty = np.log(n)\nalgo_backward = ot.LinearModelStepwiseAlgorithm(\n    input_sample, basis, output_sample, minimalIndices, direction)\nalgo_backward.setPenalty(penalty)\nalgo_backward.run()\nresult_backward = algo_backward.getResult()\nprint('Selected basis: ', result_backward.getCoefficientsNames())\nprint('R-squared = ', result_backward.getRSquared())\nprint('Adjusted R-squared = ', result_backward.getAdjustedRSquared())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is interesting to point out that although both approaches converge towards a model characterized by 2 predictive variables,\nthe selected variables do not coincide.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Both directions stepwise regression\n\nA third available option consists in performing a stepwise regression in both directions, meaning that at each iteration the predictive variables\ncan be either added or removed. In this case, a set of starting indices must be provided to the algorithm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "minimalIndices = [0]\nstartIndices = [0, 2, 3]\npenalty = np.log(n)\ndirection = ot.LinearModelStepwiseAlgorithm.BOTH\nalgo_both = ot.LinearModelStepwiseAlgorithm(\n    input_sample, basis, output_sample, minimalIndices, direction, startIndices)\nalgo_both.setPenalty(penalty)\nalgo_both.run()\nresult_both = algo_both.getResult()\nprint('Selected basis: ', result_both.getCoefficientsNames())\nprint('R-squared = ', result_both.getRSquared())\nprint('Adjusted R-squared = ', result_both.getAdjustedRSquared())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It is interesting to note that the basis varies depending on the selected set of starting indices, as is shown below.\nAn informed initialization might therefore improve the model selection and the resulting regression\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "minimalIndices = [0]\nstartIndices = [0, 1]\npenalty = np.log(n)\ndirection = ot.LinearModelStepwiseAlgorithm.BOTH\nalgo_both = ot.LinearModelStepwiseAlgorithm(\n    input_sample, basis, output_sample, minimalIndices, direction, startIndices)\nalgo_both.setPenalty(penalty)\nalgo_both.run()\nresult_both = algo_both.getResult()\nprint('Selected basis: ', result_both.getCoefficientsNames())\nprint('R-squared = ', result_both.getRSquared())\nprint('Adjusted R-squared = ', result_both.getAdjustedRSquared())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Graphical analyses\n\nFinally, we can rely on the LinearModelAnalysis class in order to analyse\nthe predictive differences between the obtained models.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "analysis_full = ot.LinearModelAnalysis(result_full)\nanalysis_full.setName('Full model')\nanalysis_forward = ot.LinearModelAnalysis(result_forward)\nanalysis_forward.setName('Forward selection')\nanalysis_backward = ot.LinearModelAnalysis(result_backward)\nanalysis_backward.setName('Backward selection')\nfig = plt.figure(figsize=(12, 8))\nfor k, analysis in enumerate([analysis_full, analysis_forward, analysis_backward]):\n    graph = analysis.drawModelVsFitted()\n    ax = fig.add_subplot(3, 1, k + 1)\n    ax.set_title(analysis.getName(), fontdict={'fontsize': 16})\n    graph.setXTitle('Exact values')\n    ax.xaxis.label.set_size(12)\n    ax.yaxis.label.set_size(14)\n    graph.setTitle('')\n    v = View(graph, figure=fig, axes=[ax])\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For illustrative purposes, we show the Bayesian Information Criterion (BIC) and Akaike Information Criterion (AIC) values\nwhich are computed during the iterations of the forward step-wise regression. Please note that in order to do\nso, we set the penalty parameter to a negligible value (meaning that the basis selection only takes into account the model likelihood,\nand not the number of parameters characterizing the linear model).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "minimalIndices = [0]\npenalty = 1e-10\ndirection = ot.LinearModelStepwiseAlgorithm.FORWARD\n\nBIC = []\nAIC = []\nfor iterations in range(1, 6):\n    algo_forward = ot.LinearModelStepwiseAlgorithm(\n        input_sample, basis, output_sample, minimalIndices, direction)\n    algo_forward.setPenalty(penalty)\n    algo_forward.setMaximumIterationNumber(iterations)\n    algo_forward.run()\n    result_forward = algo_forward.getResult()\n\n    RSS = np.sum(np.array(result_forward.getSampleResiduals())\n                 ** 2)  # Residual sum of squares\n    LL = n*np.log(RSS/n)  # Log-likelihood\n    BIC.append(LL + iterations*np.log(n))  # Bayesian Information Criterion\n    AIC.append(LL + iterations*2)  # Akaike Information Criterion\n    print('Selected basis: ', result_forward.getCoefficientsNames())\n\n\nplt.figure()\nplt.plot(np.arange(1, 6), BIC, label='BIC')\nplt.plot(np.arange(1, 6), AIC, label='AIC')\nplt.xticks(np.arange(1, 6))\nplt.xlabel('Basis size', fontsize=14)\nplt.ylabel('Information criterion', fontsize=14)\nplt.legend(fontsize=14)\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The graphic above shows that the optimal linear model in terms of compromise between prediction likelihood and model complexity\nshould take into account the influence of 2 regession variables as well as the constant term. This is coherent with the results previously\nobtained\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}