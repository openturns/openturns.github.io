{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Benchmark on a given set of problems\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we show how to make a loop over the problems in the benchmark.\nWe also show how to run various reliability algorithms on a given problem so that\nwe can score the methods using number of digits or performance.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import openturns as ot\nimport numpy as np\nimport otbenchmark as otb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Browse the reliability problems\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We present the BBRC test cases using the otbenchmark module.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "benchmarkProblemList = otb.ReliabilityBenchmarkProblemList()\nnumberOfProblems = len(benchmarkProblemList)\nnumberOfProblems"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i in range(numberOfProblems):\n    problem = benchmarkProblemList[i]\n    name = problem.getName()\n    pf = problem.getProbability()\n    event = problem.getEvent()\n    antecedent = event.getAntecedent()\n    distribution = antecedent.getDistribution()\n    dimension = distribution.getDimension()\n    print(\"#\", i, \":\", name, \" : pf = \", pf, \", dimension=\", dimension)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "maximumEvaluationNumber = 1000\nmaximumAbsoluteError = 1.0e-3\nmaximumRelativeError = 1.0e-3\nmaximumResidualError = 1.0e-3\nmaximumConstraintError = 1.0e-3\nnearestPointAlgorithm = ot.AbdoRackwitz()\nnearestPointAlgorithm.setMaximumCallsNumber(maximumEvaluationNumber)\nnearestPointAlgorithm.setMaximumAbsoluteError(maximumAbsoluteError)\nnearestPointAlgorithm.setMaximumRelativeError(maximumRelativeError)\nnearestPointAlgorithm.setMaximumResidualError(maximumResidualError)\nnearestPointAlgorithm.setMaximumConstraintError(maximumConstraintError)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The FORM method\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = otb.ReliabilityProblem8()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "metaAlgorithm = otb.ReliabilityBenchmarkMetaAlgorithm(problem)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "benchmarkResult = metaAlgorithm.runFORM(nearestPointAlgorithm)\nbenchmarkResult.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The SORM method\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "benchmarkResult = metaAlgorithm.runSORM(nearestPointAlgorithm)\nbenchmarkResult.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The LHS method\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "benchmarkResult = metaAlgorithm.runLHS(maximumOuterSampling=10000)\nbenchmarkResult.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The MonteCarloSampling method\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "benchmarkResult = metaAlgorithm.runMonteCarlo(maximumOuterSampling=10000)\nbenchmarkResult.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The FORM - Importance Sampling method\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "benchmarkResult = metaAlgorithm.runFORMImportanceSampling(nearestPointAlgorithm)\nbenchmarkResult.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Subset method\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "benchmarkResult = metaAlgorithm.runSubsetSampling()\nbenchmarkResult.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following function computes the number of correct base-10 digits\nin the computed result compared to the exact result.\nThe `CompareMethods` function takes as a parameter a problem\nand it returns the probabilities estimated by each method.\nIn addition, it returns the performance of these methods.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def PrintResults(name, benchmarkResult):\n    print(\"------------------------------------------------------------------\")\n    print(name)\n    numberOfDigitsPerEvaluation = (\n        benchmarkResult.numberOfCorrectDigits\n        / benchmarkResult.numberOfFunctionEvaluations\n    )\n    print(\"Estimated probability:\", benchmarkResult.computedProbability)\n    print(\"Number of function calls:\", benchmarkResult.numberOfFunctionEvaluations)\n    print(\"Number of correct digits=%.1f\" % (benchmarkResult.numberOfCorrectDigits))\n    print(\n        \"Performance=%.2e (correct digits/evaluation)\" % (numberOfDigitsPerEvaluation)\n    )\n    return [name, benchmarkResult.numberOfCorrectDigits, numberOfDigitsPerEvaluation]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def CompareMethods(problem, nearestPointAlgorithm, maximumOuterSampling=10000):\n    \"\"\"\n    Runs various algorithms on a given problem.\n    \"\"\"\n    summaryList = []\n    pfReference = problem.getProbability()\n    print(\"Exact probability:\", pfReference)\n    metaAlgorithm = otb.ReliabilityBenchmarkMetaAlgorithm(problem)\n    # SubsetSampling\n    benchmarkResult = metaAlgorithm.runSubsetSampling()\n    summaryList.append(PrintResults(\"SubsetSampling\", benchmarkResult))\n    # FORM\n    benchmarkResult = metaAlgorithm.runFORM(nearestPointAlgorithm)\n    summaryList.append(PrintResults(\"FORM\", benchmarkResult))\n    # SORM\n    benchmarkResult = metaAlgorithm.runSORM(nearestPointAlgorithm)\n    summaryList.append(PrintResults(\"SORM\", benchmarkResult))\n    # FORM - ImportanceSampling\n    benchmarkResult = metaAlgorithm.runFORMImportanceSampling(\n        nearestPointAlgorithm, maximumOuterSampling=maximumOuterSampling\n    )\n    summaryList.append(PrintResults(\"FORM-IS\", benchmarkResult))\n    # MonteCarloSampling\n    benchmarkResult = metaAlgorithm.runMonteCarlo(\n        maximumOuterSampling=maximumOuterSampling\n    )\n    summaryList.append(PrintResults(\"MonteCarloSampling\", benchmarkResult))\n    # LHS\n    benchmarkResult = metaAlgorithm.runLHS()\n    summaryList.append(PrintResults(\"LHS\", benchmarkResult))\n    # Gather results\n    numberOfMethods = len(summaryList)\n    correctDigitsList = []\n    performanceList = []\n    algorithmNames = []\n    for i in range(numberOfMethods):\n        [name, numberOfCorrectDigits, numberOfDigitsPerEvaluation] = summaryList[i]\n        algorithmNames.append(name)\n        correctDigitsList.append(numberOfCorrectDigits)\n        performanceList.append(numberOfDigitsPerEvaluation)\n    print(\"------------------------------------------------------------------------\")\n    print(\"Scoring by number of correct digits\")\n    indices = np.argsort(correctDigitsList)\n    rank = list(indices)\n    for i in range(numberOfMethods):\n        j = rank[i]\n        print(\"%d : %s (%.1f)\" % (j, algorithmNames[j], correctDigitsList[j]))\n    print(\"------------------------------------------------------------------------\")\n    print(\"Scoring by performance (digits/evaluation)\")\n    indices = np.argsort(performanceList)\n    rank = list(indices)\n    for i in range(len(indices)):\n        j = rank[i]\n        print(\"%d : %s (%.1e)\" % (j, algorithmNames[j], performanceList[j]))\n    return correctDigitsList, performanceList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = otb.ReliabilityProblem8()\n_ = CompareMethods(problem, nearestPointAlgorithm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remarks\n\n* We note that the FORM and SORM methods are faster, but, they do not converge to the exact proba.\n* We also notice the effectiveness of the FORM-ImportanceSampling method (inexpensive method, and converges).\n* The convergence of the MonteCarlo method requires a large number of simulations.\n* SubsetSampling converges even if the probability is very low.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}