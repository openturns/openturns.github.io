{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Benchmark the reliability solvers on the problems\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this example, we show how to run all the methods on all the problems and get the computed probability.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import openturns as ot\nimport numpy as np\nimport otbenchmark as otb\nimport pandas as pd\nfrom tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ot.Log.Show(ot.Log.NONE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We import the list of reliability problems.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "benchmarkProblemList = otb.ReliabilityBenchmarkProblemList()\nnumberOfProblems = len(benchmarkProblemList)\nnumberOfProblems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each problem in the benchmark, print the problem name and the exact failure probability.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i in range(numberOfProblems):\n    problem = benchmarkProblemList[i]\n    name = problem.getName()\n    pf = problem.getProbability()\n    print(\"#\", i, \" : \", name, \", exact PF : \", pf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run several algorithms on a single problem\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We want to run several algorithms on a single problem.\nWe set the parameters of the algorithms and run them on a single problem.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "maximumEvaluationNumber = 1000\nmaximumAbsoluteError = 1.0e-3\nmaximumRelativeError = 1.0e-3\nmaximumResidualError = 1.0e-3\nmaximumConstraintError = 1.0e-3\nnearestPointAlgorithm = ot.AbdoRackwitz()\nnearestPointAlgorithm.setMaximumCallsNumber(maximumEvaluationNumber)\nnearestPointAlgorithm.setMaximumAbsoluteError(maximumAbsoluteError)\nnearestPointAlgorithm.setMaximumRelativeError(maximumRelativeError)\nnearestPointAlgorithm.setMaximumResidualError(maximumResidualError)\nnearestPointAlgorithm.setMaximumConstraintError(maximumConstraintError)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "i = 3\nproblem = benchmarkProblemList[i]\nmetaAlgorithm = otb.ReliabilityBenchmarkMetaAlgorithm(problem)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We try the FORM algorithm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "benchmarkFORM = metaAlgorithm.runFORM(nearestPointAlgorithm)\ns1 = benchmarkFORM.summary()\nprint(s1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then the SORM algorithm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "benchmarkSORM = metaAlgorithm.runSORM(nearestPointAlgorithm)\ns2 = benchmarkSORM.summary()\nprint(s2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "benchmarkMC = metaAlgorithm.runMonteCarlo(\n    maximumOuterSampling=1000000, coefficientOfVariation=0.1, blockSize=1,\n)\ns3 = benchmarkMC.summary()\nprint(s3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "benchmarkFORMIS = metaAlgorithm.runFORMImportanceSampling(\n    nearestPointAlgorithm,\n    maximumOuterSampling=1000,\n    coefficientOfVariation=0.1,\n    blockSize=1,\n)\ns4 = benchmarkFORMIS.summary()\nprint(s4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "benchmarkSS = metaAlgorithm.runSubsetSampling(\n    maximumOuterSampling=5000, coefficientOfVariation=0.1, blockSize=1,\n)\ns5 = benchmarkSS.summary()\nprint(s5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run all algorithms on all problems and produce a single result table\n\nFor several algorithms and all the reliability problems, we want to estimate the failure probability and compare them.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create a list of problem names.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem_names = []\nfor i in range(numberOfProblems):\n    problem = benchmarkProblemList[i]\n    name = problem.getName()\n    problem_names.append(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "metrics = [\n    \"Exact\",\n    \"FORM\",\n    \"SORM\",\n    \"Monte Carlo\",\n    \"FORM-IS\",\n    \"Subset\",\n]\nresults = np.zeros((numberOfProblems, len(metrics)))\nmaximumOuterSampling = 10 ** 2\nblockSize = 10 ** 2\ncoefficientOfVariation = 0.0\n\nfor i in tqdm(range(numberOfProblems)):\n    problem = benchmarkProblemList[i]\n    results[i][0] = problem.getProbability()\n    metaAlgorithm = otb.ReliabilityBenchmarkMetaAlgorithm(problem)\n    benchmarkResult = metaAlgorithm.runFORM(nearestPointAlgorithm)\n    results[i][1] = benchmarkResult.computedProbability\n    benchmarkResult = metaAlgorithm.runSORM(nearestPointAlgorithm)\n    results[i][2] = benchmarkResult.computedProbability\n    benchmarkResult = metaAlgorithm.runMonteCarlo(\n        maximumOuterSampling=maximumOuterSampling,\n        coefficientOfVariation=coefficientOfVariation,\n        blockSize=blockSize,\n    )\n    results[i][3] = benchmarkResult.computedProbability\n    benchmarkResult = metaAlgorithm.runFORMImportanceSampling(\n        nearestPointAlgorithm,\n        maximumOuterSampling=maximumOuterSampling,\n        coefficientOfVariation=coefficientOfVariation,\n        blockSize=blockSize,\n    )\n    results[i][4] = benchmarkResult.computedProbability\n    benchmarkResult = metaAlgorithm.runSubsetSampling(\n        maximumOuterSampling=maximumOuterSampling,\n        coefficientOfVariation=coefficientOfVariation,\n        blockSize=blockSize,\n    )\n    results[i][5] = benchmarkResult.computedProbability\n\ndf = pd.DataFrame(results, index=problem_names, columns=metrics)\ndf.to_csv(\"reliability_benchmark_table-output.csv\")\ndf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run several algorithms on all problems and get detailed statistics\n\nRun several algorithms on all reliability benchmark problems: print statistics on each problem.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def FormatRow(benchmarkResult):\n    \"\"\"Format a single row of the benchmark table\"\"\"\n    result = [\n        benchmarkResult.exactProbability,\n        benchmarkResult.computedProbability,\n        benchmarkResult.absoluteError,\n        benchmarkResult.numberOfCorrectDigits,\n        benchmarkResult.numberOfFunctionEvaluations,\n        benchmarkResult.numberOfDigitsPerEvaluation,\n    ]\n    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "method_names = [\"Monte-Carlo\", \"FORM\", \"SORM\", \"FORM-IS\", \"SUBSET\"]\n\nmaximumOuterSampling = 10 ** 2\nblockSize = 10 ** 2\ncoefficientOfVariation = 0.0\n\nresult = dict()\nfor i in range(numberOfProblems):\n    problem = benchmarkProblemList[i]\n    name = problem_names[i]\n    exact_pf_name = \"%10s\" % (\"Exact PF \" + name[0:10])\n    metrics = [\n        exact_pf_name,\n        \"Estimated PF\",\n        \"Absolute Error\",\n        \"Correct Digits\",\n        \"Function Calls\",\n        \"Digits / Evaluation\",\n    ]\n    results = np.zeros((len(method_names), len(metrics)))\n    metaAlgorithm = otb.ReliabilityBenchmarkMetaAlgorithm(problem)\n    # Monte-Carlo\n    benchmarkResult = metaAlgorithm.runMonteCarlo(\n        maximumOuterSampling=maximumOuterSampling,\n        coefficientOfVariation=coefficientOfVariation,\n        blockSize=blockSize,\n    )\n    results[0, :] = FormatRow(benchmarkResult)\n    # FORM\n    benchmarkResult = metaAlgorithm.runFORM(nearestPointAlgorithm)\n    results[1, :] = FormatRow(benchmarkResult)\n    # SORM\n    benchmarkResult = metaAlgorithm.runSORM(nearestPointAlgorithm)\n    results[2, :] = FormatRow(benchmarkResult)\n    # FORM-IS\n    benchmarkResult = metaAlgorithm.runFORMImportanceSampling(\n        nearestPointAlgorithm,\n        maximumOuterSampling=maximumOuterSampling,\n        coefficientOfVariation=coefficientOfVariation,\n        blockSize=blockSize,\n    )\n    results[3, :] = FormatRow(benchmarkResult)\n    # Subset\n    benchmarkResult = metaAlgorithm.runSubsetSampling(\n        maximumOuterSampling=maximumOuterSampling,\n        coefficientOfVariation=coefficientOfVariation,\n        blockSize=blockSize,\n    )\n    results[4, :] = FormatRow(benchmarkResult)\n    # Gather statistics and print them\n    df = pd.DataFrame(results, index=method_names, columns=metrics,)\n    # Format the columns for readability\n    s = df.style.format(\n        {\n            exact_pf_name: lambda x: \"{:.3e}\".format(x),\n            \"Estimated PF\": lambda x: \"{:.3e}\".format(x),\n            \"Absolute Error\": lambda x: \"{:.3e}\".format(x),\n            \"Correct Digits\": lambda x: \"{:.1f}\".format(x),\n            \"Function Calls\": lambda x: \"{:d}\".format(int(x)),\n            \"Digits / Evaluation\": lambda x: \"{:.1f}\".format(x),\n        }\n    )\n    result[name] = s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result[\"RP33\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "result[\"RP35\"]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}