.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_plot_using_otagrum.py>`     to download the full example code
    .. rst-class:: sphx-glr-example-title

    .. _sphx_glr_auto_examples_plot_using_otagrum.py:


Using otagrum
=============


.. code-block:: default

    import openturns as ot
    import pyAgrum as gum
    from matplotlib import pylab as plt
    import otagrum








Creating the CBN structure
We begin by creating the CBN that will be used throughout this example.

To do so, we need a NamedDAG structure...


.. code-block:: default

    dag = gum.DAG()









.. code-block:: default

    mapping = {}
    mapping['A'] = dag.addNode() # Add node A
    mapping['B'] = dag.addNode() # Add node B
    mapping['C'] = dag.addNode() # Add node C
    mapping['D'] = dag.addNode() # Add node D









.. code-block:: default

    dag.addArc(mapping['A'], mapping['C']) # Arc A -> C
    dag.addArc(mapping['B'], mapping['C']) # Arc B -> C
    dag.addArc(mapping['C'], mapping['D']) # Arc C -> D










.. code-block:: default

    dag






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    <pyAgrum.pyAgrum.DAG; proxy of <Swig Object of type 'gum::DAG *' at 0x7fd398341390> >




.. code-block:: default

    structure = otagrum.NamedDAG(dag, list(mapping.keys()))










.. code-block:: default

    structure







.. raw:: html

    <p>[A,B,C,D]<br>
    [B->C,A->C,C->D]</p>
    <br />
    <br />

Parameters of the CBN ... and a collection of marginals and local conditional copulas.


.. code-block:: default

    m_list = [ot.Uniform(0.0, 1.0) for i in range(structure.getSize())] # Local marginals
    lcc_list = [] # Local Conditional Copulas
    for i in range( structure.getSize() ):
        dim_lcc = structure.getParents(i).getSize() + 1
        R = ot.CorrelationMatrix(dim_lcc)
        for j in range(dim_lcc):
            for k in range(j):
                R[j, k] = 0.6
        lcc_list.append( ot.Normal([0.0]*dim_lcc, [1.0]*dim_lcc, R).getCopula() )









Now that we have a NamedDAG structure and a collection of local conditional copulas, we can construct a CBN.


.. code-block:: default

    cbn = otagrum.ContinuousBayesianNetwork(structure, m_list, lcc_list)








Having a CBN, we can now sample from it.


.. code-block:: default

    ot.RandomGenerator.SetSeed(10) # Set random seed
    sample = cbn.getSample(1000)
    train = sample[:-100]
    test = sample[-100:]









Learning the structure with continuous PC:
Now that we have data, we can use it to learn the structure with the continuous PC algorithm.


.. code-block:: default

    learner = otagrum.ContinuousPC(sample, maxConditioningSetSize=5, alpha=0.1)








We first learn the skeleton, that is the undirected structure.


.. code-block:: default

    skeleton = learner.learnSkeleton()










.. code-block:: default

    skeleton





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    (gum::UndiGraph@0x555a17b6a3f0) {0,1,2,3} , {2--3,0--2,1--2}



Then we look for the v-structures, leading to a Partially Directed Acyclic Graph (PDAG) 


.. code-block:: default

    pdag = learner.learnPDAG()










.. code-block:: default

    pdag





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    (gum::MixedGraph@0x555a175a2cb0) {0,1,2,3} , {1->2,0->2} , {2--3}



Finally, the remaining edges are oriented by propagating constraints


.. code-block:: default

    ndag = learner.learnDAG()










.. code-block:: default

    ndag






.. raw:: html

    <p>[A,B,C,D]<br>
    [B->C,A->C,C->D]</p>
    <br />
    <br />

The true structure has been recovered.
Learning with continuous MIIC
Otagrum provides another learning algorithm to learn the structure: the continuous MIIC algorithm.


.. code-block:: default

    learner = otagrum.ContinuousMIIC(sample)








This algorithm relies on the computing of mutual information which is done through the copula function. Hence, a copula model for the data is needed. The continuous MIIC algorithm can make use of Gaussian copulas (parametric) or Bernstein copulas (non-parametric) to compute mutual information. Moreover, due to finite sampling size, the mutual information estimators need to be corrected. Two kind of correction are provided: NoCorr (no correction) or Naive (a fixed correction is substracted from the raw mutual information estimators). Those behaviours can be changed as follows: 

learner.setCMode(otagrum.CorrectedMutualInformation.CModeTypes_Bernstein) # By default


.. code-block:: default

    learner.setCMode(otagrum.CorrectedMutualInformation.CModeTypes_Gaussian) # To use Gaussian copulas
    learner.setKMode(otagrum.CorrectedMutualInformation.KModeTypes_Naive) # By default
    #learner.setKMode(otagrum.CorrectedMutualInformation.KModeTypes_NoCorr) # To use the raw estimators
    learner.setAlpha(0.01) # Set the correction value for the Naive behaviour, it is set to 0.01 by default








As with PC algorithm we can learn the skeleton, PDAG and DAG using


.. code-block:: default

    skeleton = learner.learnSkeleton()










.. code-block:: default

    skeleton






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    (gum::UndiGraph@0x555a17c16090) {0,1,2,3} , {2--3,0--2,1--2}




.. code-block:: default

    pdag = learner.learnPDAG()










.. code-block:: default

    pdag






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    (gum::MixedGraph@0x555a17b45c80) {0,1,2,3} , {2->3,1->2,0->2} , {}




.. code-block:: default

    dag = learner.learnDAG()










.. code-block:: default

    dag






.. raw:: html

    <p>[A,B,C,D]<br>
    [B->C,A->C,C->D]</p>
    <br />
    <br />

Learning parameters
Bernstein copulas are used to learn the local conditional copulas associated to each node


.. code-block:: default

    m_list = []
    lcc_list = []
    for i in range(train.getDimension()):
        m_list.append(ot.UniformFactory().build(train.getMarginal(i)))
        indices = [i] + [int(n) for n in ndag.getParents(i)]
        dim_lcc = len(indices)
        if dim_lcc == 1:
            bernsteinCopula = ot.IndependentCopula(1)
        elif dim_lcc > 1:
            K = otagrum.ContinuousTTest.GetK(len(train), dim_lcc)
            bernsteinCopula = ot.EmpiricalBernsteinCopula(train.getMarginal(indices), K, False)
        lcc_list.append(bernsteinCopula)








We can now create the learned CBN


.. code-block:: default

    lcbn = otagrum.ContinuousBayesianNetwork(ndag, m_list, lcc_list) # Learned CBN








And compare the mean loglikelihood between the true and the learned models


.. code-block:: default

    def compute_mean_LL(cbn, test):
        ll = 0
        for t in test:
            ll += cbn.computeLogPDF(t)
        ll /= len(test)
        return ll










.. code-block:: default

    true_LL = compute_mean_LL(cbn, test)
    print(true_LL)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    0.31620906143480526





.. code-block:: default

    exp_LL = compute_mean_LL(lcbn, test)
    print(exp_LL)




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    0.15241697100666615





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  0.252 seconds)


.. _sphx_glr_download_auto_examples_plot_using_otagrum.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_using_otagrum.py <plot_using_otagrum.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_using_otagrum.ipynb <plot_using_otagrum.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
